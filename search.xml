<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[National Data Science Bowl(Classification)]]></title>
      <url>/classification/2021/01/24/bowl/</url>
      <content type="text"><![CDATA[대회 소개플랑크톤 이미지로 121 개의 플랑크톤 종 구분하는 대회National Data Science BowlPredict ocean health, one plankton at a timehttps://www.kaggle.com/c/datasciencebowlDescriptionPlankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health.The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the health of the world’s oceans, one plankton at a time.import numpy as np import pandas as pd import zipfileimport globfrom PIL import Imageimport osimport matplotlib.pyplot as pltimport seaborn as snsimport randomimport cv2import tensorflow as tffrom tensorflow.keras import *from tensorflow.keras.layers import *from tensorflow.keras.applications import EfficientNetB1from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStoppingfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldimport matplotlib.pyplot as pltimport seaborn as snsfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))/kaggle/input/datasciencebowl/plankton_identification.pdf/kaggle/input/datasciencebowl/train.zip/kaggle/input/datasciencebowl/sampleSubmission.csv.zip/kaggle/input/datasciencebowl/test.zip데이터 불러오기  train, test 데이터가 zip으로 압축되어 있으므로 먼저 압축을 풀어주자.with zipfile.ZipFile('/kaggle/input/datasciencebowl/train.zip','r') as z:    z.extractall('train')    with zipfile.ZipFile('/kaggle/input/datasciencebowl/test.zip','r') as z:    z.extractall('test')Train Dataset  Train 데이터프레임을 만들어 주기train = glob.glob('train/*/*/*')train_df = pd.DataFrame({'path' : train})train_df['image'] = train_df['path'].apply(lambda x : x.split('/')[-1])train_df['label'] = train_df['path'].apply(lambda x : x.split('/')[-2])train_df                  path      image      label                  0      train/train/pteropod_theco_dev_seq/36451.jpg      36451.jpg      pteropod_theco_dev_seq              1      train/train/pteropod_theco_dev_seq/44793.jpg      44793.jpg      pteropod_theco_dev_seq              2      train/train/pteropod_theco_dev_seq/157712.jpg      157712.jpg      pteropod_theco_dev_seq              3      train/train/pteropod_theco_dev_seq/4992.jpg      4992.jpg      pteropod_theco_dev_seq              4      train/train/pteropod_theco_dev_seq/144126.jpg      144126.jpg      pteropod_theco_dev_seq              ...      ...      ...      ...              30331      train/train/hydromedusae_shapeB/27912.jpg      27912.jpg      hydromedusae_shapeB              30332      train/train/hydromedusae_shapeB/49210.jpg      49210.jpg      hydromedusae_shapeB              30333      train/train/hydromedusae_shapeB/114615.jpg      114615.jpg      hydromedusae_shapeB              30334      train/train/hydromedusae_shapeB/81391.jpg      81391.jpg      hydromedusae_shapeB              30335      train/train/hydromedusae_shapeB/95843.jpg      95843.jpg      hydromedusae_shapeB      30336 rows × 3 columnsTest Dataset  Test 데이터 프레임을 만들어 주기test = glob.glob('./test/*/*')test_df = pd.DataFrame({'path' : test})test_df['image'] = test_df['path'].apply(lambda x : x.split('/')[-1])test_df                  path      image                  0      ./test/test/105188.jpg      105188.jpg              1      ./test/test/27651.jpg      27651.jpg              2      ./test/test/11940.jpg      11940.jpg              3      ./test/test/87538.jpg      87538.jpg              4      ./test/test/95396.jpg      95396.jpg              ...      ...      ...              130395      ./test/test/34122.jpg      34122.jpg              130396      ./test/test/88776.jpg      88776.jpg              130397      ./test/test/78514.jpg      78514.jpg              130398      ./test/test/1830.jpg      1830.jpg              130399      ./test/test/8917.jpg      8917.jpg      130400 rows × 2 columns데이터 확인하기  이미지 모양 및 크기 확인  class 개수 및 분포 확인 이미지 확인하기 fig , axes = plt.subplots(4,3)fig.set_size_inches(15,10)for index in range(0,12):    idx = random.randrange(len(train_df))    img = Image.fromarray(cv2.imread(train_df.iloc[idx,0], cv2.IMREAD_COLOR)).resize((256,256))    axes[index//3 , index%3].imshow(img)    axes[index//3 , index%3].set_title(train_df['label'][idx])    axes[index//3,  index%3].axis('off')샘플 이미지 사이즈 확인하기sample_image = Image.open(train[1000])print("Image size: ",sample_image.size)sample_image.resize((128,128))Image size:  (131, 251)Train Dataset 이미지들의 평균 너비 및 높이w = []h = []idx=0for i in train:    img = Image.open(i)    w.append(img.size[0])    h.append(img.size[1])    img.close()    idx+=1    print("Train Images' mean width :",np.mean(w),"\nTrain Images' mean height :",np.mean(h))Train Images' mean width : 73.50728507383967 Train Images' mean height : 66.66182093881856Label의 분포 print("The unique label numbers :",train_df['label'].nunique())a,b = plt.subplots(1,1,figsize=(20,12))plot = sns.countplot(train_df['label'])plt.setp(plot.get_xticklabels(), rotation=90)plt.show()The unique label numbers : 121  각 label 별로 데이터의 분포가 다르다.Test Dataset 전처리idg_test = ImageDataGenerator()test_generator = idg_test.flow_from_dataframe(test_df,                                                 x_col = 'path',                                                y_col = None,                                                 class_mode = None,                                                shuffle=False,                                                batch_size = 64,                                                target_size = (150,150),)Found 130400 validated image filenames.교차 검증으로 모델 학습하기k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)result=0index=0for train_index, valid_index in k_fold.split(train_df, train_df['label']):        X_train = train_df.iloc[train_index]    X_val = train_df.iloc[valid_index]        idg_train = ImageDataGenerator(horizontal_flip=True)    idg_val = ImageDataGenerator()        train_generator = idg_train.flow_from_dataframe(X_train,                                                 x_col = 'path',                                                y_col = 'label',                                                 batch_size = 64,                                                target_size = (150,150))    val_generator = idg_val.flow_from_dataframe(X_val,                                                 x_col = 'path',                                                y_col = 'label',                                                 batch_size = 64,                                                target_size = (150,150))            model = Sequential()    model.add(EfficientNetB1(include_top = False,weights ='imagenet', pooling = 'avg'))    model.add(Dense(121, activation = 'softmax'))    model.compile(optimizer = tf.keras.optimizers.Adam() , loss = 'categorical_crossentropy', metrics = ['acc'],)    es = EarlyStopping(patience = 10, verbose = True)    ckpt = ModelCheckpoint('best.h5', save_best_only = True, verbose =True, monitor = 'val_loss')    rl = ReduceLROnPlateau(monitor = 'val_loss',patience = 5, verbose = True)    model.fit(train_generator,               validation_data = val_generator,              callbacks = [ckpt, rl, es],               epochs =25)    model.load_weights('best.h5')        result += model.predict(test_generator, verbose=True) / 5        index+=1    print(f"\n\n{index} 번째 교차 검증 완료 ... ... ...\n\n")Epoch 12/25380/380 [==============================] - ETA: 0s - loss: 0.0739 - acc: 0.9796Epoch 00012: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0739 - acc: 0.9796 - val_loss: 0.9785 - val_acc: 0.7618Epoch 13/25380/380 [==============================] - ETA: 0s - loss: 0.0572 - acc: 0.9852Epoch 00013: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0572 - acc: 0.9852 - val_loss: 1.0230 - val_acc: 0.7636Epoch 14/25380/380 [==============================] - ETA: 0s - loss: 0.0483 - acc: 0.9866Epoch 00014: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0483 - acc: 0.9866 - val_loss: 1.0412 - val_acc: 0.7620Epoch 15/25380/380 [==============================] - ETA: 0s - loss: 0.0404 - acc: 0.9894Epoch 00015: val_loss did not improve from 0.93774Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.380/380 [==============================] - 112s 295ms/step - loss: 0.0404 - acc: 0.9894 - val_loss: 1.0788 - val_acc: 0.7603Epoch 16/25380/380 [==============================] - ETA: 0s - loss: 0.0322 - acc: 0.9927Epoch 00016: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0322 - acc: 0.9927 - val_loss: 1.0791 - val_acc: 0.7623Epoch 17/25380/380 [==============================] - ETA: 0s - loss: 0.0290 - acc: 0.9941Epoch 00017: val_loss did not improve from 0.93774380/380 [==============================] - 112s 296ms/step - loss: 0.0290 - acc: 0.9941 - val_loss: 1.0779 - val_acc: 0.7625Epoch 18/25380/380 [==============================] - ETA: 0s - loss: 0.0285 - acc: 0.9938Epoch 00018: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0285 - acc: 0.9938 - val_loss: 1.0824 - val_acc: 0.7640Epoch 19/25380/380 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9948Epoch 00019: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0260 - acc: 0.9948 - val_loss: 1.0838 - val_acc: 0.7651Epoch 20/25380/380 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9952Epoch 00020: val_loss did not improve from 0.93774Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.380/380 [==============================] - 112s 296ms/step - loss: 0.0253 - acc: 0.9952 - val_loss: 1.0899 - val_acc: 0.7640Epoch 00020: early stopping2038/2038 [==============================] - 107s 53ms/step5 번째 교차 검증 완료 ... ... ...제출  sampleSubmission.csv를 열어서 column의 순서를 확인해보면, 알파벳 순서로 들어가 있지 않다.  result 칼럼의 순서와도 맞지 않으니, 이런 겨웅 직접 제출 파일을 만드는 것이 더 쉽다.with zipfile.ZipFile('/kaggle/input/datasciencebowl/sampleSubmission.csv.zip','r') as z:    z.extractall('sub')sub = pd.read_csv("sub/sampleSubmission.csv")sub                  image      acantharia_protist_big_center      acantharia_protist_halo      acantharia_protist      amphipods      appendicularian_fritillaridae      appendicularian_s_shape      appendicularian_slight_curve      appendicularian_straight      artifacts_edge      ...      trichodesmium_tuft      trochophore_larvae      tunicate_doliolid_nurse      tunicate_doliolid      tunicate_partial      tunicate_salp_chains      tunicate_salp      unknown_blobs_and_smudges      unknown_sticks      unknown_unclassified                  0      1.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              1      10.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              2      100.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              3      1000.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              4      10000.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              130395      99994.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130396      99995.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130397      99996.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130398      99997.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130399      99999.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      130400 rows × 122 columnscol_names = list(sub.columns[1:])col_names.sort()col_names['acantharia_protist', 'acantharia_protist_big_center', 'acantharia_protist_halo', 'amphipods', 'appendicularian_fritillaridae', 'appendicularian_s_shape', 'appendicularian_slight_curve', 'appendicularian_straight', 'artifacts', 'artifacts_edge', 'chaetognath_non_sagitta', 'chaetognath_other', 'chaetognath_sagitta', 'chordate_type1', 'copepod_calanoid', 'copepod_calanoid_eggs', 'copepod_calanoid_eucalanus', 'copepod_calanoid_flatheads', 'copepod_calanoid_frillyAntennae', 'copepod_calanoid_large', 'copepod_calanoid_large_side_antennatucked', 'copepod_calanoid_octomoms', 'copepod_calanoid_small_longantennae', 'copepod_cyclopoid_copilia',    ...  'pteropod_butterfly', 'pteropod_theco_dev_seq', 'pteropod_triangle', 'radiolarian_chain', 'radiolarian_colony', 'shrimp-like_other', 'shrimp_caridean', 'shrimp_sergestidae', 'shrimp_zoea', 'siphonophore_calycophoran_abylidae', 'siphonophore_calycophoran_rocketship_adult', 'siphonophore_calycophoran_rocketship_young', 'siphonophore_calycophoran_sphaeronectes', 'siphonophore_calycophoran_sphaeronectes_stem', 'siphonophore_calycophoran_sphaeronectes_young', 'siphonophore_other_parts', 'siphonophore_partial', 'siphonophore_physonect', 'siphonophore_physonect_young', 'stomatopod', 'tornaria_acorn_worm_larvae', 'trichodesmium_bowtie', 'trichodesmium_multiple', 'trichodesmium_puff', 'trichodesmium_tuft', 'trochophore_larvae', 'tunicate_doliolid', 'tunicate_doliolid_nurse', 'tunicate_partial', 'tunicate_salp', 'tunicate_salp_chains', 'unknown_blobs_and_smudges', 'unknown_sticks', 'unknown_unclassified']my_sub = pd.concat([test_df,pd.DataFrame(result, columns=col_names)], axis=1).drop(['path'], axis=1)my_sub                  image      acantharia_protist      acantharia_protist_big_center      acantharia_protist_halo      amphipods      appendicularian_fritillaridae      appendicularian_s_shape      appendicularian_slight_curve      appendicularian_straight      artifacts      ...      trichodesmium_tuft      trochophore_larvae      tunicate_doliolid      tunicate_doliolid_nurse      tunicate_partial      tunicate_salp      tunicate_salp_chains      unknown_blobs_and_smudges      unknown_sticks      unknown_unclassified                  0      105188.jpg      3.407894e-05      5.234519e-07      4.727002e-07      2.523708e-05      6.414000e-07      1.429394e-04      7.389794e-05      2.510147e-05      8.997078e-08      ...      2.137576e-05      2.115729e-06      1.604188e-06      8.190606e-06      1.233781e-06      2.818242e-06      1.334955e-06      0.000023      7.377646e-05      0.000072              1      27651.jpg      2.613361e-06      2.237140e-07      1.978252e-06      6.420553e-07      1.818773e-06      1.416835e-05      8.689414e-06      2.637850e-07      2.187041e-08      ...      2.810447e-07      9.286011e-07      8.374320e-06      2.925759e-06      2.006822e-07      2.315101e-07      7.389501e-07      0.000014      2.626070e-06      0.000423              2      11940.jpg      6.104551e-06      2.521627e-06      2.513692e-07      4.636616e-05      2.085026e-06      1.489576e-05      8.225787e-05      1.137621e-05      3.021312e-07      ...      1.476521e-05      7.298316e-07      1.122896e-06      1.197497e-04      4.583527e-07      1.900000e-05      4.955790e-07      0.000482      3.988420e-05      0.002802              3      87538.jpg      1.197881e-05      4.275779e-06      1.933437e-05      3.641628e-03      1.369998e-03      2.135313e-04      4.967969e-04      7.344080e-04      8.830065e-06      ...      1.323840e-01      8.188009e-06      6.234657e-04      2.646256e-04      5.426758e-05      1.694718e-03      1.481177e-05      0.000878      4.522099e-02      0.106003              4      95396.jpg      1.079931e-07      1.477846e-06      4.025902e-07      9.353130e-05      6.802826e-07      3.575570e-06      4.541085e-07      1.869251e-07      2.157424e-07      ...      3.714577e-06      4.525396e-07      1.575890e-07      7.061017e-07      1.216337e-06      4.131224e-07      3.218001e-07      0.000011      8.910469e-07      0.000015              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              130395      34122.jpg      9.404398e-06      4.410570e-06      1.469415e-05      4.892956e-04      4.235670e-04      7.620276e-04      7.704680e-05      4.103683e-05      1.476081e-05      ...      1.900832e-04      1.661727e-05      9.112542e-02      4.120418e-04      3.442946e-04      3.249257e-04      1.036034e-05      0.033025      1.140596e-05      0.008211              130396      88776.jpg      1.188530e-04      3.108746e-06      7.981415e-06      2.125595e-07      5.005016e-08      5.221577e-07      6.974371e-07      1.526524e-06      1.525192e-05      ...      7.819511e-01      6.797829e-07      2.360160e-07      1.453742e-07      5.104931e-06      3.599948e-06      5.606108e-07      0.000010      1.053775e-04      0.000017              130397      78514.jpg      3.513707e-06      1.072021e-07      6.263767e-07      2.130197e-07      8.390234e-07      3.240724e-05      1.973183e-04      1.500386e-04      1.360382e-08      ...      1.262665e-07      2.861952e-08      2.094702e-04      1.114861e-06      8.004471e-07      1.075251e-05      1.032177e-07      0.000038      1.695668e-06      0.000293              130398      1830.jpg      2.687246e-08      6.054729e-10      1.001864e-07      2.510690e-08      9.267259e-09      2.659469e-08      6.407049e-08      6.902572e-08      9.994848e-01      ...      1.325866e-07      2.992527e-09      6.681008e-08      9.846344e-09      9.125281e-08      4.204970e-09      2.354498e-08      0.000091      7.961059e-08      0.000001              130399      8917.jpg      1.647273e-07      1.519561e-07      9.120576e-08      1.517367e-03      1.538393e-07      2.816295e-07      6.235562e-07      5.191119e-06      2.437675e-06      ...      7.615223e-07      3.302962e-06      1.213374e-02      8.384104e-01      9.281476e-05      2.865229e-03      1.414542e-05      0.000016      7.298431e-06      0.003105      130400 rows × 122 columnsmy_sub.to_csv('bowl_sub.csv',index=False)  Evaluation : Log Loss 0.65988순위 : 1,049 중 35등]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Invasive Species Monitoring(Classification)]]></title>
      <url>/classification/2021/01/21/invasive/</url>
      <content type="text"><![CDATA[대회 소개식물 이미지로 침입종을 분류하는 이진 분류 대회Invasive Species MonitoringIdentify images of invasive hydrangeahttps://www.kaggle.com/c/invasive-species-monitoringDescriptionTangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale.Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems.In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable.import numpy as np import pandas as pd import globimport zipfilefrom PIL import Imageimport cv2from tensorflow.keras import *from tensorflow.keras.layers import *from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau from tensorflow.keras.applications.efficientnet import EfficientNetB1from sklearn.model_selection import train_test_splitfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import StratifiedKFoldimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))Data Path 불러오기train_path = glob.glob("/kaggle/input/invasivemonitoring/train/train/*")test_path = glob.glob("/kaggle/input/invasivemonitoring/test/test/*")len(train_path), len(test_path)(2295, 1531)  레이블 csv 불러오기with zipfile.ZipFile('/kaggle/input/invasive-species-monitoring/train_labels.csv.zip', 'r') as z :     z.extractall('label')    label = pd.read_csv("./label/train_labels.csv")label                  name      invasive                  0      1      0              1      2      0              2      3      1              3      4      0              4      5      1              ...      ...      ...              2290      2291      1              2291      2292      1              2292      2293      1              2293      2294      1              2294      2295      1      2295 rows × 2 columns데이터 확인  실제 이미지 확인Image.open('/kaggle/input/invasivemonitoring/test/test/708.jpg').resize((256,256))  이미지의 크기 확인idx=0for i in train_path:    print(np.array(Image.open(i)).shape)    idx+=1    if idx&gt;10:        break(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)(866, 1154, 3)  모든 이미지의 shape = (866, 1154, 3)  target_size를 256보다는 좀 더 키워보자.          target_size가 커지면 batch_size는 줄여야 한다.      Dataframe 만들기Train Datasettrain_df = pd.DataFrame({"path" : train_path})train_df['name'] = train_df['path'].apply(lambda x : x.split("/")[-1].split(".")[0])label['name'] = label['name'].astype('str')train_df = pd.merge(train_df, label, on = 'name')train_df['invasive'] = train_df['invasive'].astype('str')train_df.head()                  path      name      invasive                  0      /kaggle/input/invasivemonitoring/train/train/1...      1269      1              1      /kaggle/input/invasivemonitoring/train/train/6...      623      1              2      /kaggle/input/invasivemonitoring/train/train/2...      2193      1              3      /kaggle/input/invasivemonitoring/train/train/2...      2008      0              4      /kaggle/input/invasivemonitoring/train/train/2...      2081      1      Test Datasettest_df = pd.DataFrame({"path" : test_path})test_df['name'] = test_df['path'].apply(lambda x : x.split("/")[-1].split(".")[0])test_df                  path      name                  0      /kaggle/input/invasivemonitoring/test/test/126...      1269              1      /kaggle/input/invasivemonitoring/test/test/623...      623              2      /kaggle/input/invasivemonitoring/test/test/764...      764              3      /kaggle/input/invasivemonitoring/test/test/107...      1075              4      /kaggle/input/invasivemonitoring/test/test/771...      771              ...      ...      ...              1526      /kaggle/input/invasivemonitoring/test/test/25.jpg      25              1527      /kaggle/input/invasivemonitoring/test/test/120...      1201              1528      /kaggle/input/invasivemonitoring/test/test/147...      147              1529      /kaggle/input/invasivemonitoring/test/test/921...      921              1530      /kaggle/input/invasivemonitoring/test/test/728...      728      1531 rows × 2 columnsData preprocessing  Data Augmentation          먼저 test set부터 적용해준다.      교차 검증을 할 것이기 때문에 train셋은 반복문 안에서 처리한다.      idg_test = ImageDataGenerator()test_generator = idg_test.flow_from_dataframe(test_df,                                        x_col = 'path',                                        y_col = None,                                        class_mode=None,                                        shuffle = False,                                        target_size = (512,512)) # 기본 배치사이즈 32 Found 1531 validated image filenames.Data Augmentation 확인하기  ImageDataGenerator의 옵션들이 어떤 식으로 적용되는지 확인하는 것은 매우 중요하다.  rotation을 적용했을 때, target이 중간 부분에 있는 이 대회에선 모델 학습에 도움이 될 수 있다.  rescale 옵션은 직접 모델 구축하는게 아닌 이상 대부분의 모델에서 구현이 되어있다. 따로 추가하지 않아도 된다.  다른 옵션들도 확인해보자.batch_size = 4sample_df = train_df[:batch_size]idg_sample = ImageDataGenerator(rotation_range = 45)sample_generator = idg_sample.flow_from_dataframe(sample_df,                                                   batch_size = batch_size,                                                 x_col = 'path',                                                 y_col = 'invasive',                                                 target_size = (150,150))Found 4 validated image filenames belonging to 2 classes.Augmentation 적용 결과 확인하기images = []for i in enumerate(range(5)):    img, label = sample_generator.next()    n_img = len(label)        image_array = cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB)        for idx in range(n_img-1):        image_2 = cv2.cvtColor(img[idx+1], cv2.COLOR_BGR2RGB)        image_array = np.hstack((image_array,image_2))    images.append(image_array)        img = images[0]for idx in range(len(images)-1):    img  = np.vstack((img, images[idx+1]))Image.fromarray((img).astype(np.uint8))K-fold Cross Validation(교차검증)으로 모델 학습하기  K : 5  train : valid = 1836 : 459 비율로 나눠지게 된다.k_fold =StratifiedKFold(n_splits = 5, shuffle = True, random_state =42)result = 0index=0for train_index, valid_index in k_fold.split(train_df, train_df['invasive']):    X_train = train_df.iloc[train_index]    X_val = train_df.iloc[valid_index]            # Data Preprocessing    idg_train = ImageDataGenerator(horizontal_flip=True, rotation_range=45)     idg_val = ImageDataGenerator()         train_generator = idg_train.flow_from_dataframe(X_train,                                                x_col = 'path',                                                y_col = 'invasive',                                                target_size=(512,512),                                                batch_size = 16,)                                                      valid_generator = idg_val.flow_from_dataframe(X_val,                                              x_col = 'path',                                              y_col = 'invasive',                                              target_size=(512,512),                                              batch_size = 16,)        # callbacks    early_stop = EarlyStopping(patience=7,                          verbose = True)    model_ckpt = ModelCheckpoint(filepath= 'best.h5',                             monitor = 'val_loss',                            save_best_only = True,                            verbose = True)    rl = ReduceLROnPlateau(verbose=True,patience=4,)              # model    model = Sequential()    model.add(EfficientNetB1(include_top = False, weights = 'imagenet', pooling = 'avg'))    model.add(Dense(2, activation = 'softmax'))    model.compile(metrics = ['acc'], optimizer = 'adam', loss='categorical_crossentropy')     model.fit(train_generator,             validation_data= valid_generator,             callbacks=[early_stop, model_ckpt, rl],             epochs = 15)        # load_weights    model.load_weights('best.h5')        result += model.predict(test_generator, verbose = True,) /5    index+=1    print(f'{index} ... cross-validation complete ... ')Epoch 14/15115/115 [==============================] - ETA: 0s - loss: 0.0336 - acc: 0.9875Epoch 00014: val_loss did not improve from 0.02688115/115 [==============================] - 202s 2s/step - loss: 0.0336 - acc: 0.9875 - val_loss: 0.0465 - val_acc: 0.9847Epoch 15/15115/115 [==============================] - ETA: 0s - loss: 0.0293 - acc: 0.9902Epoch 00015: val_loss did not improve from 0.02688115/115 [==============================] - 205s 2s/step - loss: 0.0293 - acc: 0.9902 - val_loss: 0.0311 - val_acc: 0.989148/48 [==============================] - 47s 974ms/step5 ... cross-validation complete ... 결과값 확인resultarray([[3.7590151e-05, 9.9996245e-01],       [9.9689949e-01, 3.1004685e-03],       [5.0094517e-10, 1.0000000e+00],       ...,       [1.6892184e-08, 1.0000000e+00],       [1.9720024e-10, 1.0000000e+00],       [9.9781048e-01, 2.1895403e-03]], dtype=float32) 제출   result의 index값이 sample_submission.csv 이미지의 순서와 다르기 때문에 직접 제출 파일을 만드는 것이 쉽다.with zipfile.ZipFile('/kaggle/input/invasive-species-monitoring/sample_submission.csv.zip','r') as z:    z.extractall('sub')sub = pd.read_csv("sub/sample_submission.csv")sub                  name      invasive                  0      1      0.5              1      2      0.5              2      3      0.5              3      4      0.5              4      5      0.5              ...      ...      ...              1526      1527      0.5              1527      1528      0.5              1528      1529      0.5              1529      1530      0.5              1530      1531      0.5      1531 rows × 2 columns직접 제출 csv 만들기test_df['invasive'] = result[:,1]sub_df = test_df.drop(['path'], axis=1)sub_df                  name      invasive                  0      1269      0.999962              1      623      0.003100              2      764      1.000000              3      1075      0.999977              4      771      1.000000              ...      ...      ...              1526      25      1.000000              1527      1201      0.025586              1528      147      1.000000              1529      921      1.000000              1530      728      0.002190      1531 rows × 2 columns# Final score which evaluated on area under the ROC curve is 0.99714.sub_df.to_csv("sub.csv",index=False) ]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Transfer Learning </tag>
        
          <tag> Computer vVsion </tag>
        
          <tag> Fine Tuning </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[State Farm Distracted Driver Detection(Classification)]]></title>
      <url>/classification/2021/01/14/statefarm/</url>
      <content type="text"><![CDATA[대회 소개운전 중인 운전자의 이미지로 운전자의 행동 분류하는 대회State Farm Distracted Driver DetectionCan computer vision spot distracted drivers?https://www.kaggle.com/c/state-farm-distracted-driver-detection/overviewDescriptionWe’ve all been there: a light turns green and the car in front of you doesn’t budge. Or, a previously unremarkable vehicle suddenly slows and starts swerving from side-to-side.When you pass the offending driver, what do you expect to see? You certainly aren’t surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year.State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver’s behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?What to doThe 10 classes to predict are:c0: normal drivingc1: texting - rightc2: talking on the phone - rightc3: texting - leftc4: talking on the phone - leftc5: operating the radioc6: drinkingc7: reaching behindc8: hair and makeupc9: talking to passengerImport Libraryimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import osimport numpy as np import pandas as pdimport osimport globfrom PIL import Imagefrom tensorflow.keras import *from tensorflow.keras.layers import *from tensorflow.keras.applications import EfficientNetB1from tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau from sklearn.model_selection import StratifiedKFoldpd.options.display.max_colwidth =999이미지 리스트 불러오기total = pd.read_csv('/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv')total                  subject      classname      img                  0      p002      c0      img_44733.jpg              1      p002      c0      img_72999.jpg              2      p002      c0      img_25094.jpg              3      p002      c0      img_69092.jpg              4      p002      c0      img_92629.jpg              ...      ...      ...      ...              22419      p081      c9      img_56936.jpg              22420      p081      c9      img_46218.jpg              22421      p081      c9      img_25946.jpg              22422      p081      c9      img_67850.jpg              22423      p081      c9      img_9684.jpg      22424 rows × 3 columnsTrain Dataset 만들기train = glob.glob('../input/state-farm-distracted-driver-detection/imgs/train/*/*')train_df = pd.DataFrame({'path' : train})train_df['img'] = train_df['path'].apply(lambda x : x.split('/')[-1])train_df['classname'] = train_df['path'].apply(lambda x : x.split('/')[-2])train_df                  path      img      classname                  0      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_68208.jpg      img_68208.jpg      c5              1      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_77583.jpg      img_77583.jpg      c5              2      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_49189.jpg      img_49189.jpg      c5              3      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_6690.jpg      img_6690.jpg      c5              4      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_95740.jpg      img_95740.jpg      c5              ...      ...      ...      ...              22419      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_6087.jpg      img_6087.jpg      c0              22420      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_36959.jpg      img_36959.jpg      c0              22421      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_19429.jpg      img_19429.jpg      c0              22422      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_99342.jpg      img_99342.jpg      c0              22423      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_48589.jpg      img_48589.jpg      c0      22424 rows × 3 columnstrain_df['classname'].nunique() # 클래스 개수10데이터 확인Image.open(train[10])Test Dataset 만들기test = glob.glob('../input/state-farm-distracted-driver-detection/imgs/test/*')test_df = pd.DataFrame({'path' : test})test_df['img'] = test_df['path'].apply(lambda x : x.split('/')[-1])test_df                  path      img                  0      ../input/state-farm-distracted-driver-detection/imgs/test/img_96590.jpg      img_96590.jpg              1      ../input/state-farm-distracted-driver-detection/imgs/test/img_32366.jpg      img_32366.jpg              2      ../input/state-farm-distracted-driver-detection/imgs/test/img_99675.jpg      img_99675.jpg              3      ../input/state-farm-distracted-driver-detection/imgs/test/img_85937.jpg      img_85937.jpg              4      ../input/state-farm-distracted-driver-detection/imgs/test/img_73903.jpg      img_73903.jpg              ...      ...      ...              79721      ../input/state-farm-distracted-driver-detection/imgs/test/img_109.jpg      img_109.jpg              79722      ../input/state-farm-distracted-driver-detection/imgs/test/img_53257.jpg      img_53257.jpg              79723      ../input/state-farm-distracted-driver-detection/imgs/test/img_90376.jpg      img_90376.jpg              79724      ../input/state-farm-distracted-driver-detection/imgs/test/img_28000.jpg      img_28000.jpg              79725      ../input/state-farm-distracted-driver-detection/imgs/test/img_93083.jpg      img_93083.jpg      79726 rows × 2 columnsidg_test = ImageDataGenerator()test_generator = idg_test.flow_from_dataframe(test_df,                                             x_col = 'path',                                             y_col = None,                                             class_mode =None,                                             shuffle = False,                                             target_size = (256,256))Found 79726 validated image filenames.K-fold Cross Validation(교차검증)으로 모델 학습하기kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)모든 데이터셋을 학습시켜 보다 안정적인 모델 성능 뽑아내기  이 대회에서 교차검증 없이 모델을 학습하면 특정 subject(운전자)에 과적합되는 문제가 발생한다.  모델이 classname을 예측할 때 subject에 크게 영향을 받을 수 있다는 것이다.  train_test_split에서 stratify옵션을 주어 학습을 해도, 데이터의 일부분만을 추출하는 것이기 때문에 Test 데이터의 분포가 Valid 데이터셋의 분포와 다를 수 있다.  결국 일정 부분의 학습 데이터를 잃는 것이기에, 전체 데이터셋을 모두 훈련에 사용하고 싶을 때 교차 검증을 진행한다.  k-fold Cross Validation 특징          전체 데이터를 훈련 및 평가할 수 있음.      앙상블의 효과를 얻을 수 있다. k-fold 학습마다 독립적인 모델을 학습한다라고 생각하면 쉽다.      같은 작업을 여러번 반복하는 것이기에 시간은 오래 걸린다.      result = 0index=0for train_index, valid_index in kfold.split(train_df, train_df['classname']):    X_train = train_df.iloc[train_index]    X_val = train_df.iloc[valid_index]            idg_train = ImageDataGenerator()    idg_val = ImageDataGenerator()        train_generator = idg_train.flow_from_dataframe(X_train,                                                    x_col = 'path',                                                   y_col = 'classname',                                                   batch_size = 16,                                                   target_size = (256,256))            val_generator = idg_val.flow_from_dataframe(X_val,                                                x_col = 'path',                                               y_col = 'classname',                                               batch_size = 16,                                               target_size = (256,256))                    early_stop = EarlyStopping(patience=5,                              verbose = True)    model_ckpt = ModelCheckpoint(filepath= 'best.h5',                                 monitor = 'val_loss',                                save_best_only = True,                                verbose = True)    rl = ReduceLROnPlateau(verbose=True,patience=4,)      model = Sequential()    model.add(EfficientNetB1(include_top = False, weights = 'imagenet',pooling = 'avg'))    model.add(Dense(10, activation = 'softmax'))    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])        model.fit(train_generator,             validation_data = val_generator,             epochs=  20,             callbacks = [early_stop, model_ckpt, rl])        model.load_weights('best.h5')            result += model.predict(test_generator, verbose = True) /5    index +=1    print('fin : ',index)제출  sample_submission.csv 에서 각 이미지의 순서와 result의 이미지 순서가 다르기 때문에 새로 Dataframe을 만드는 것이 편하다.result_df = pd.DataFrame(result, columns=['c0', 'c1', 'c2', 'c3','c4', 'c5', 'c6', 'c7','c8', 'c9'])result_df                                                          c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      4.898581e-03      6.054179e-03      5.146383e-03      5.325366e-04      7.062429e-04      0.012463      0.642532      2.142958e-02      3.042254e-01      0.002012              1      4.047657e-08      4.258116e-08      7.385115e-08      7.353977e-08      5.030099e-08      0.999987      0.000007      2.678771e-08      2.975033e-07      0.000006              2      4.653947e-01      9.502587e-03      3.015550e-02      7.141132e-03      8.581814e-03      0.001376      0.228859      1.078819e-03      2.433791e-01      0.004531              3      8.255903e-05      1.916165e-05      1.645608e-05      2.139845e-03      2.219971e-04      0.990963      0.000140      5.377778e-06      3.694886e-05      0.006375              4      3.502117e-07      1.061771e-04      4.572770e-05      2.403757e-06      1.187768e-05      0.000053      0.000408      9.991777e-01      1.797632e-04      0.000014              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      7.563246e-03      1.897852e-02      2.062093e-01      1.308421e-03      5.482526e-03      0.002233      0.118231      1.006306e-01      5.321654e-01      0.007198              79722      2.018641e-03      3.363243e-04      4.372783e-04      4.747344e-01      4.001005e-01      0.004480      0.000733      3.581994e-04      8.031728e-04      0.115998              79723      8.465707e-02      5.784687e-01      3.608754e-03      3.662695e-02      1.371328e-02      0.011587      0.017968      7.717087e-02      3.644365e-02      0.139756              79724      6.226664e-08      4.983243e-08      7.004503e-09      2.172765e-08      6.979324e-08      0.999966      0.000004      6.643240e-09      2.172691e-06      0.000028              79725      5.966864e-02      2.801355e-02      7.123795e-03      6.038536e-04      2.516853e-03      0.004967      0.027987      1.176820e-03      8.555837e-01      0.012359      79726 rows × 10 columnssub = pd.read_csv('../input/state-farm-distracted-driver-detection/sample_submission.csv')sub                  img      c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      img_1.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              1      img_10.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              2      img_100.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              3      img_1000.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              4      img_100000.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      img_99994.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79722      img_99995.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79723      img_99996.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79724      img_99998.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79725      img_99999.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      79726 rows × 11 columnsmy_sub = pd.concat([test_df, result_df], axis = 1).drop(['path'], axis =1)my_sub                  img      c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      img_96590.jpg      4.898581e-03      6.054179e-03      5.146383e-03      5.325366e-04      7.062429e-04      0.012463      0.642532      2.142958e-02      3.042254e-01      0.002012              1      img_32366.jpg      4.047657e-08      4.258116e-08      7.385115e-08      7.353977e-08      5.030099e-08      0.999987      0.000007      2.678771e-08      2.975033e-07      0.000006              2      img_99675.jpg      4.653947e-01      9.502587e-03      3.015550e-02      7.141132e-03      8.581814e-03      0.001376      0.228859      1.078819e-03      2.433791e-01      0.004531              3      img_85937.jpg      8.255903e-05      1.916165e-05      1.645608e-05      2.139845e-03      2.219971e-04      0.990963      0.000140      5.377778e-06      3.694886e-05      0.006375              4      img_73903.jpg      3.502117e-07      1.061771e-04      4.572770e-05      2.403757e-06      1.187768e-05      0.000053      0.000408      9.991777e-01      1.797632e-04      0.000014              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      img_109.jpg      7.563246e-03      1.897852e-02      2.062093e-01      1.308421e-03      5.482526e-03      0.002233      0.118231      1.006306e-01      5.321654e-01      0.007198              79722      img_53257.jpg      2.018641e-03      3.363243e-04      4.372783e-04      4.747344e-01      4.001005e-01      0.004480      0.000733      3.581994e-04      8.031728e-04      0.115998              79723      img_90376.jpg      8.465707e-02      5.784687e-01      3.608754e-03      3.662695e-02      1.371328e-02      0.011587      0.017968      7.717087e-02      3.644365e-02      0.139756              79724      img_28000.jpg      6.226664e-08      4.983243e-08      7.004503e-09      2.172765e-08      6.979324e-08      0.999966      0.000004      6.643240e-09      2.172691e-06      0.000028              79725      img_93083.jpg      5.966864e-02      2.801355e-02      7.123795e-03      6.038536e-04      2.516853e-03      0.004967      0.027987      1.176820e-03      8.555837e-01      0.012359      79726 rows × 11 columnsmy_sub.to_csv('sub.csv', index=False) # 0.18165  Final score which evaluated with multi-class logarithmic loss is 0.18165.]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> CNN </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Plant Seedlings Classification(Classification)]]></title>
      <url>/classification/2021/01/06/plantseedlings/</url>
      <content type="text"><![CDATA[대회 소개묘목과 잡초 이미지를 분류하는 대회Plant Seedlings ClassificationDetermine the species of a seedling from an imagehttps://www.kaggle.com/c/plant-seedlings-classificationDescriptionCan you differentiate a weed from a crop seedling?The ability to do so effectively can mean better crop yields and better stewardship of the environment.The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.We’re hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.Baseline 코드import numpy as npimport pandas as pd import osimport globfrom PIL import Imagefrom sklearn.model_selection import train_test_splitfrom keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras import * from tensorflow.keras.layers import * from tensorflow.keras.applications.efficientnet import EfficientNetB1 # 거의 B1 모델 사용 / B2로 갈수록 무거워짐from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint데이터 불러오기path = glob.glob("/kaggle/input/plant-seedlings-classification/train/*/*")train = pd.DataFrame({"path" : path})train['label'] = train['path'].apply(lambda x : x.split("/")[-2])train                  path      label                  0      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              1      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              2      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              3      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              4      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              ...      ...      ...              4745      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4746      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4747      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4748      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4749      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse      4750 rows × 2 columns데이터 확인Image.open(path[100])Train, valid dataset 만들기  Train, Valid Dataset 나누기x_train, x_valid = train_test_split(train,                                     test_size = 0.2,                                     stratify = train['label'],                                    random_state=42)  ImgaeDataGenerator로 데이터 전처리하기idg = ImageDataGenerator()idg2 = ImageDataGenerator()train_generator = idg.flow_from_dataframe(x_train,                                         x_col = 'path',                                         y_col = 'label',                                         target_size = (256,256),                                          batch_size= 32) valid_generator = idg2.flow_from_dataframe(x_valid,                                         x_col = 'path',                                         y_col = 'label',                                         target_size = (256,256),                                         batch_size= 32)Found 3800 validated image filenames belonging to 12 classes.Found 950 validated image filenames belonging to 12 classes.Model 학습# # 모델 선언 # model = Sequential()# # 이미지 학습하는 층을 쌓기# model.add(Conv2D(16, (3,3), activation = 'relu', input_shape = (256,256,3))) # model.add(Conv2D(16, (3,3), activation = 'relu')) # model.add(MaxPooling2D()) # model.add(Conv2D(32, (3,3), activation = 'relu')) # model.add(Conv2D(32, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(64, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(128, (3,3), activation = 'relu')) # model.add(Conv2D(128, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(256, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(512, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(1024, (1,1), activation = 'relu')) # # 차원 축소# model.add(GlobalAveragePooling2D()) # model.add(Dense(12, activation = 'softmax'))# 조기 종료 옵션es = EarlyStopping(patience=3,                    verbose=True)mc = ModelCheckpoint("best.h5",                     save_best_only = True,                     verbose=True                    )# transfer learningmodel = Sequential() # 항상 모델 선언 다시 하고 학습할 것.model.add(EfficientNetB1(include_top=False, weights = 'imagenet', pooling = 'avg')) model.add(Dense(12, activation='softmax')) # 최적화, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')model.fit(train_generator,         validation_data  = valid_generator,         epochs = 100,          callbacks = [es, mc],           ) model.load_weights('best.h5')  모델 Summarymodel.summary()Model: "sequential"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================efficientnetb1 (Functional)  (None, 1280)              6575239   _________________________________________________________________dense (Dense)                (None, 12)                15372     =================================================================Total params: 6,590,611Trainable params: 6,528,556Non-trainable params: 62,055_________________________________________________________________Test Dataset 만들기  Train, Valid dataset 만들듯이 ImageDataGenerator를 사용해 전처리하면 된다.test_path = glob.glob("/kaggle/input/plant-seedlings-classification/test/*")test = pd.DataFrame({"path" : test_path})test                  path                  0      /kaggle/input/plant-seedlings-classification/t...              1      /kaggle/input/plant-seedlings-classification/t...              2      /kaggle/input/plant-seedlings-classification/t...              3      /kaggle/input/plant-seedlings-classification/t...              4      /kaggle/input/plant-seedlings-classification/t...              ...      ...              789      /kaggle/input/plant-seedlings-classification/t...              790      /kaggle/input/plant-seedlings-classification/t...              791      /kaggle/input/plant-seedlings-classification/t...              792      /kaggle/input/plant-seedlings-classification/t...              793      /kaggle/input/plant-seedlings-classification/t...      794 rows × 1 columnstest_generator =idg.flow_from_dataframe(test,                                        x_col = 'path',                                        y_col = None,                                        class_mode=None,                                        shuffle = False,                                        target_size = (256,256)                                       )Found 794 validated image filenames.모델로 예측하기result = model.predict(test_generator, verbose = 1)result25/25 [==============================] - 9s 376ms/steparray([[3.1431966e-02, 1.3168590e-06, 1.8106741e-06, ..., 3.4729105e-06,        2.8560698e-05, 6.0173708e-05],       [1.1053331e-04, 1.5426338e-04, 2.5204610e-04, ..., 2.1230977e-05,        1.5883794e-05, 8.8891864e-01],       [1.6521607e-06, 6.2318693e-05, 2.8655048e-05, ..., 4.6870503e-01,        1.3315211e-04, 4.2114169e-03],       ...,       [2.1159883e-06, 2.1564020e-01, 7.7060443e-01, ..., 4.1908701e-03,        9.3793590e-03, 1.1505652e-05],       [6.5076878e-05, 3.2120446e-05, 3.2295244e-05, ..., 4.1470761e-05,        5.1161887e-06, 9.9024123e-01],       [1.2335427e-06, 6.4034253e-01, 4.4173703e-05, ..., 3.2450294e-05,        3.5616100e-01, 4.7782612e-07]], dtype=float32)제출  가장 높은 확률의 class(string)를 예측값으로 추출해야 한다.  train_generator.class_indices로 각 클래스에 해당하는 index를 가져온다.class_dict = train_generator.class_indicesclass_dict{'Black-grass': 0, 'Charlock': 1, 'Cleavers': 2, 'Common Chickweed': 3, 'Common wheat': 4, 'Fat Hen': 5, 'Loose Silky-bent': 6, 'Maize': 7, 'Scentless Mayweed': 8, 'Shepherds Purse': 9, 'Small-flowered Cranesbill': 10, 'Sugar beet': 11}class_list =[]for i in class_dict:    class_list.append(i)class_list ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']sub = pd.read_csv('/kaggle/input/plant-seedlings-classification/sample_submission.csv')sub['species']= [class_list[i] for i in np.argmax(result,1)]sub['file'] = test['path'].apply(lambda x : x.split("/")[-1])sub                  file      species                  0      fd87b36ae.png      Loose Silky-bent              1      0e8492cb1.png      Sugar beet              2      8d6acbe9b.png      Common Chickweed              3      54b3afd58.png      Cleavers              4      6049234e6.png      Fat Hen              ...      ...      ...              789      4c7838de4.png      Common Chickweed              790      fda39e16f.png      Loose Silky-bent              791      da4ed3a28.png      Charlock              792      a83820a2c.png      Sugar beet              793      e4a76885b.png      Maize      794 rows × 2 columnssub.to_csv('plant.csv',index=False)]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Dogs vs. Cats(Classification)]]></title>
      <url>/classification/2021/01/01/dogvscat/</url>
      <content type="text"><![CDATA[대회 소개개와 고양이 이미지를 분류하는 대회Dogs vs. Cats Redux: Kernels EditionDistinguish images of dogs from catshttps://www.kaggle.com/c/dogs-vs-cats-redux-kernels-editionDescriptionIn 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician’s cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty.Much has changed at Kaggle as well. Our online coding environment Kernels didn’t exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning’s future challenges.데이터 준비 및 확인이미지 분류 대회에서 …  아이디어 얻기 위해 데이터 하나 하나 뜯어보며 이미지 형태를 파악해야 한다.  머신러닝 대회에서 독립변수의 개수가 같아야 하는 것처럼 딥러닝에서도 Input 크기가 같아야 한다. 따라서 크기 조정 등의 최소한의 전처리가 필요하다.  이미지 크기 조정 시 이미 작은 이미지는 상관 없는데, 큰 이미지를 작은 이미지로 축소할 때 정보의 손실이 있을 수 있다.(나중에 더 깊게 다룰 예정)이미지 분류 베이스 라인 잡기  Data Preprocessing          glob 이용하여 데이터 불러오기      이미지 경로, 이미지 클래스를 칼럼으로 갖는 데이터 프레임 만들기      train_test_split으로 train, valid 데이터 만들기                  stratify 옵션으로 label 비율 맞춰서 데이터셋 나누기                    keras의 ImageDataGenerator 라이브러리를 사용하여 이미지 데이터 전처리하기        Modeling          모델 선언 및 학습 층 쌓기      최적화 함수, 손실 함수 정하기        train          모델로 전처리된 이미지 학습하기       데이터 확인 import numpy as np import pandas as pd import os!unzip '/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip'!unzip '/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip'from PIL import ImageImage.open('train/cat.100.jpg').resize((256,256))이미지 경로, 클래스를 칼럼으로 갖는 데이터프레임 만들기import globtrain = pd.DataFrame({"path" : glob.glob("train/*")})train['label'] = train['path'].apply(lambda x : x.split("/")[-1].split(".")[0])train                  path      label                  0      train/dog.10300.jpg      dog              1      train/cat.7540.jpg      cat              2      train/dog.5684.jpg      dog              3      train/cat.10367.jpg      cat              4      train/cat.537.jpg      cat              ...      ...      ...              24995      train/cat.2300.jpg      cat              24996      train/dog.7893.jpg      dog              24997      train/dog.8167.jpg      dog              24998      train/dog.6154.jpg      dog              24999      train/dog.4016.jpg      dog      25000 rows × 2 columnsTrain, Valid 셋 만들기from sklearn.model_selection import train_test_splitX_train, X_val = train_test_split(train,                                 test_size = 0.2,                                  stratify = train['label'],                                 random_state = 42)keras의 ImageDataGenerator 사용하여 데이터 전처리Data Augmentation  대회별로 도움되는 변형 옵션을 찾아야 한다.      데이터 변형을 한다고 해서 이미지 수가 두배가 되는게 아니다.    에폭당 50%확률로 변형이 적용되는데, 이는 ImageDataGenerator()에서 50%확률로 default 값이 지정되어 있기 때문이다.          한 epoch당 원본 이미지 50% + 변형 이미지 50%가 학습되는 것이므로, Data Augmentation을 할 때는 최소 여러 번의 epoch를 돌게 만들자.      어떤 변형 옵션이 도움이 되는지 확인할 때는 이미지 사이즈를 줄이고 배치 사이즈를 늘려 빨리 실험해보자.      평가할 때는 원본 이미지에 대해서 predict가 이루어져야 하기 때문에 idg_val = ImageDataGenerator()를 따로 지정해주는 것이다.      ImageDataGenerator()를 커스터마이징하여 값을 바꿀 수도 있다.      flow_from_dataframe 옵션  target_size          이미지 사이즈를 키우게 되면 정보 손실이 덜 하므로 그만큼 점수가 좋아진다.      원본 이미지의 사이즈가 클 경우 target_size를 이에 비슷하게 맞추는 것이 이상적이지만, 그만큼 학습 속도가 느려지게 된다. 또한 메모리 문제로 인해 target_size를 늘리면 batch_size를 줄여야 한다.      원본 이미지가 작은 경우 사이즈를 키웠다고 해서 더 성능이 좋아지는 것은 아니다.        batch_size          batch_size 역시 모델 성능(점수)에 영향을 준다.      from keras.preprocessing.image import ImageDataGeneratoridg_train = ImageDataGenerator(horizontal_flip=True)idg_valid = ImageDataGenerator()idg_test = ImageDataGenerator()train_generator = idg_train.flow_from_dataframe(X_train, # df 이름                                          x_col = 'path', # 이미지 path                                          y_col = 'label',# 클래스                                          batch_size = 32, # 기본값 32 -&gt; 처음엔 100으로 설정하여 학습 속도를 빠르게 하기도 함.                                          target_size = (256,256)) # 기본값 256                                          valid_generator = idg_valid.flow_from_dataframe(X_val,                                               x_col = 'path',                                               y_col = 'label',                                                batch_size = 32,                                                target_size = (256,256))Found 20000 validated image filenames belonging to 2 classes.Found 5000 validated image filenames belonging to 2 classes.모델링모델 선언 : Sequential()로 모델을 먼저 선언한다.  이미지 학습하는 층 쌓기: 기본적인 모델 구조를 설정한다. Conv2D함수 안에 n개의 feature map을 추출한다.          첫 layer에서는 input_shape을 설정해 주자. (256,256,3)      activation 함수를 적용하여 하나의 층이 비선형적인 학습을 할 수 있도록 한다.      하나의 Conv2D 층을 하나의 분류기라고 생각하자. 머신러닝에서 하나의 랜덤 포레스트 모델로 생각하면 쉽다.여러 개의 랜덤포레스트를 앙상블하면 학습에 도움이 되는 것처럼 여러 Conv2D층을 쌓으면 여러 특징들을 추출해낼 수 있기 때문에 학습에 도움이 된다.      Conv2D 층만 쌓게 되면 중요하지 않은 배경들도 똑같이 고려하게 되므로 학습이 어려워진다.합성곱층 통과 후 가장 특징적인 값만 가져오는   MaxPool2D층을 추가해주자.        차원 축소하기          Conv2D 채널을 지나서 나오는 피처맵들은 3차원 이미지 데이터들이다. Flatten()으로 3차원 데이터를 1차원으로 펼쳐준다.      Flatten() : 이미지 내 지역에 대한 고려 없이 1차원으로 펼처준다. 지역적인 정보가 날아갈 수 있다.      GlobalAveragePooling2D() : 지역을 고려하여 2D형태로 펼쳐준다. Flatten대신 많이 사용한다.        Output layer 설정하기 : 마지막 층은 학습하는 것이 아니라 각 클래스에 대한 확률값으로 값이 출력되어야 한다.          다중 분류시 softmax를 사용한다.        최적화함수, 손실함수 정의하기          모델의 정확도를 확인할 수 있도록 metrics를 설정한다.      최적화함수는 Adam, 손실함수는 categorical_crossentropy로 설정한다.      adam’s default Learning Rate =  0.001        모델 학습시키기          epochs : 한번의 epochs을 반복할 때마다 전체 데이터를 한번 학습하게 된다. 학습을 여러번 시킬수록 점수 향상에 도움이 되지만 과대적합 문제가 발생할 수 있다.      모델 summary : 모델 구조를 출력한다.from keras import * from keras.layers import * # 모델 선언 model = Sequential()# 이미지 학습하는 층을 쌓기model.add(Conv2D(16, (3,3), activation = 'relu', input_shape = (256,256,3)))model.add(MaxPool2D())model.add(Conv2D(32, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(64, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(128, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(256, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(512, (3,3), activation = 'relu'))model.add(MaxPool2D())# 차원 축소#model.add(Flatten())model.add(GlobalAveragePooling2D())# model.add(Dropout(0.5))# model.add(Dense(512, activation = 'relu'))# model.add(Dense(256, activation = 'relu'))model.add(Dense(2, activation = 'softmax'))# 최적화, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')model.fit(train_generator,         validation_data=valid_generator,        epochs= 10         )Epoch 1/10625/625 [==============================] - 93s 149ms/step - loss: 1.3097 - acc: 0.5896 - val_loss: 0.6291 - val_acc: 0.6382Epoch 2/10625/625 [==============================] - 93s 149ms/step - loss: 0.5652 - acc: 0.7053 - val_loss: 0.5327 - val_acc: 0.7402Epoch 3/10625/625 [==============================] - 95s 153ms/step - loss: 0.4760 - acc: 0.7775 - val_loss: 0.4140 - val_acc: 0.8166Epoch 4/10625/625 [==============================] - 93s 149ms/step - loss: 0.4070 - acc: 0.8147 - val_loss: 0.3493 - val_acc: 0.8484Epoch 5/10625/625 [==============================] - 93s 148ms/step - loss: 0.3471 - acc: 0.8486 - val_loss: 0.4356 - val_acc: 0.8134Epoch 6/10625/625 [==============================] - 93s 149ms/step - loss: 0.3118 - acc: 0.8680 - val_loss: 0.3170 - val_acc: 0.8708Epoch 7/10625/625 [==============================] - 93s 148ms/step - loss: 0.2792 - acc: 0.8821 - val_loss: 0.2858 - val_acc: 0.8822Epoch 8/10625/625 [==============================] - 94s 151ms/step - loss: 0.2515 - acc: 0.8939 - val_loss: 0.2577 - val_acc: 0.8920Epoch 9/10625/625 [==============================] - 93s 149ms/step - loss: 0.2362 - acc: 0.9018 - val_loss: 0.2665 - val_acc: 0.8920Epoch 10/10625/625 [==============================] - 94s 150ms/step - loss: 0.2168 - acc: 0.9097 - val_loss: 0.2565 - val_acc: 0.8932  모델 Summarymodel.summary()Model: "sequential_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_6 (Conv2D)            (None, 254, 254, 16)      448       _________________________________________________________________max_pooling2d_6 (MaxPooling2 (None, 127, 127, 16)      0         _________________________________________________________________conv2d_7 (Conv2D)            (None, 125, 125, 32)      4640      _________________________________________________________________max_pooling2d_7 (MaxPooling2 (None, 62, 62, 32)        0         _________________________________________________________________conv2d_8 (Conv2D)            (None, 60, 60, 64)        18496     _________________________________________________________________max_pooling2d_8 (MaxPooling2 (None, 30, 30, 64)        0         _________________________________________________________________conv2d_9 (Conv2D)            (None, 28, 28, 128)       73856     _________________________________________________________________max_pooling2d_9 (MaxPooling2 (None, 14, 14, 128)       0         _________________________________________________________________conv2d_10 (Conv2D)           (None, 12, 12, 256)       295168    _________________________________________________________________max_pooling2d_10 (MaxPooling (None, 6, 6, 256)         0         _________________________________________________________________conv2d_11 (Conv2D)           (None, 4, 4, 512)         1180160   _________________________________________________________________max_pooling2d_11 (MaxPooling (None, 2, 2, 512)         0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 1,573,794Trainable params: 1,573,794Non-trainable params: 0_________________________________________________________________전이학습(Transfer Learning)Imagenet 등의 빅데이터로 학습된 모델을 가져와 쓰기Pretrained Model : EfficientNet      작년에 나온 모델로, 이미지 분류 대회에서 압도적으로 쓰인다. B0 ~ B7까지 버전이 있고, B2 이상으로 갈수록 모델이 많이 무거워진다.    pretrained model 사용시 알아야할 3가지 필수 옵션          include_top : 마지막 출력층(top)을 포함할 것인지 여부 결정. False로 설정한 후 문제 상황에 맞게 분류기를 바꿀 수 있다.      weights : imagenet으로 설정 시, Imagenet을 통해 학습된 가중치를 사용하게 된다.                  학습된 가중치를 사용했을 때 장점                          대부분의 분류 문제에서 별다른 하이퍼파라미터 튜닝 없이 정확도가 8~90%가 나온다.              사진들끼리 공유하고 있는 특징(배경, 직선, 명암, 색체 등)이 있기 때문에 imagenet의 클래스에 없는 이미지에 대해서도 곧잘 분류한다.                                          pooling : avg 설정 시 분류층 직전에 Global average pooling을 적용한다.                  이미지 분류 문제에서는 대부분 classifier 전에 gap를 사용한다. 앞서 모델링파트에서 소개했듯이 지역적 정보를 가져오기 때문이다.          이미지 분류가 아닌 detection, segmentation, 음성 데이터 처리, 혹은 text mining 등의 경우 Flatten을 사용 시 모델의 성능이 잘 나오기도 한다. 딥러닝에서 정답은 없다. 많이 시도해봐야 한다.                      freeze          freeze시킴으로써 층을 선택적으로 학습을 시킬 수 있다.      데이터가 적으면 pretrained model의 feature extract 층은 얼리고 classifer 층만 학습시키기도 한다.      Callbacks 옵션여러가지 기법들로 모델 성능 높이기  EarlyStopping          조기 종료 옵션이다.                  patience : default로 val_loss를 monitor한다. 하이퍼파라미터로 설정한 양의 정수만큼 val_loss가 떨어지는 것을 참고 지켜보게 된다.                          만약 val_loss가 0.4 -&gt; 0.45 -&gt; 0.42로 높아졌을 때, 두 경우 모두에서 patience가 누적된다. 두 값 모두 0.4보다 높기 때문이다. 즉, 최적의 순간보다 val_loss가 n번 연속 좋아지지 않으면 멈춘다.                                            ModelCheckpoint          모델 최적의 가중치를 저장하는 옵션이다.      EarlyStopping을 사용했을 경우, 10번째가 최적일 시 13번째까지 학습하고 마지막 13번째의 가중치가 남아있게 된다. 체크포인트를 통해 10번째의 가중치를 가져올 수 있게 된다.      model.load_weights() 옵션으로 저장된 모델을 불러오는 것을 잊지 말자.        ReduceLROnPlateau          Learning rate가 컸을 때, 최적점 주변을 멤돌거나 지나쳐서 성능이 오히려 안좋아질 수 있다.      Learning rate(학습 보폭)를 조정하여 최적점에 도달하는데 도움을 줄 수 있다.      학습이 멈추기 전 작동해야하므로 항상 EarlyStopping의 patience 값 보단 작게 해야 한다.      factor 하이퍼파라미터의 기본값 : 0.1      min_lr 하이퍼파라미터의 기본값 : 0      from tensorflow.keras import * from tensorflow.keras.layers import * from tensorflow.keras.applications.efficientnet import EfficientNetB1 from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau# transfer learningmodel = Sequential() # 항상 모델 선언 다시 하고 학습할 것model.add(EfficientNetB1(include_top=False, weights = 'imagenet', pooling = 'avg')) # classifiermodel.add(Dense(2, activation='softmax')) # 최적화 함수, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')# callbacks 설정하기es = EarlyStopping(patience=3,                     verbose=True)mc = ModelCheckpoint("best.h5", #h5:가중치 저장 확장자                    save_best_only = True,                     verbose=True)rlp = ReduceLROnPlateau(patience = 2,                       verbose=True)# 모델 학습하기model.fit(train_generator,         validation_data  = valid_generator,         epochs = 100,         callbacks = [es, mc, rlp],) model.load_weights('best.h5')Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h527025408/27018416 [==============================] - 0s 0us/stepEpoch 1/100625/625 [==============================] - ETA: 0s - loss: 0.0914 - acc: 0.9660Epoch 00001: val_loss improved from inf to 0.06357, saving model to best.h5625/625 [==============================] - 276s 442ms/step - loss: 0.0914 - acc: 0.9660 - val_loss: 0.0636 - val_acc: 0.9768Epoch 2/100625/625 [==============================] - ETA: 0s - loss: 0.0578 - acc: 0.9794Epoch 00002: val_loss did not improve from 0.06357625/625 [==============================] - 272s 435ms/step - loss: 0.0578 - acc: 0.9794 - val_loss: 0.1416 - val_acc: 0.9416Epoch 3/100625/625 [==============================] - ETA: 0s - loss: 0.0433 - acc: 0.9841Epoch 00003: val_loss did not improve from 0.06357Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.625/625 [==============================] - 273s 437ms/step - loss: 0.0433 - acc: 0.9841 - val_loss: 0.0679 - val_acc: 0.9750Epoch 4/100598/625 [===========================&gt;..] - ETA: 10s - loss: 0.0200 - acc: 0.9933제출하기  Test셋 예측 후 제출하기          Train셋과 마찬가지로 test_generator 만들어준다.      Test셋에는 레이블이 없으므로, y_col = None, class_mode = None 옵션 넣어줘야 한다.      학습 시에는 데이터가 섞여도 되지만, predict할 때는 index가 섞일 수 있으니 shuffle = False 옵션을 넣어준다.      test = pd.DataFrame({"path" : glob.glob("test/*")})test_generator = idg_test.flow_from_dataframe(test,                                               x_col = 'path',                                               y_col = None,                                              class_mode=None,                                              shuffle = False,                                              target_size = (256,256))result = model.predict(test_generator, verbose = 1)result   verbose = 1 : 모델 예측 진행 상황을 볼 수 있다.  result의 값을 출력해보면 클래스가 2개이므로 확률값 2개가 나온다.train_generator.class_indices   train_generator.class_indices 로 각 클래스의 변환값을 알 수 있다.  보통 0(False), 1(True)값이므로 이진 분류 대회에서는 1의 확률값을 제출한다.sub = pd.read_csv("/kaggle/input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv")sub['label'] = result[:,1] sub['id'] = test['path']sub['id'] = sub['id'].apply(lambda x : x.split("/")[1].split(".")[0])sub  lable이 id와 매핑되도록 바꿔줘야한다.현재 모델은 test['path']의 순서대로 예측값을 계산했는데, sub['id']는 index가 0부터이다.sub.to_csv("sub.csv",index=False) ]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Dacon Computer Vision(Classification)]]></title>
      <url>/classification/2020/12/23/ComputerVision/</url>
      <content type="text"><![CDATA[대회 소개딥러닝 모델을 이용하여 글자 속 숨겨진 숫자를 정확하게 분류해내는 대회import tensorflow as tfimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalizationfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateaufrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import train_test_split데이터 불러오기train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)train.head()                  digit      letter      0      1      2      3      4      5      6      7      8      9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      ...      744      745      746      747      748      749      750      751      752      753      754      755      756      757      758      759      760      761      762      763      764      765      766      767      768      769      770      771      772      773      774      775      776      777      778      779      780      781      782      783              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1      5      L      1      1      1      4      3      0      0      4      4      3      0      4      3      3      3      4      4      0      0      1      1      3      4      0      4      2      0      4      0      1      3      1      0      4      1      1      3      1      ...      4      3      4      1      3      0      0      1      3      3      3      0      3      2      2      1      0      1      0      0      3      0      0      4      2      0      3      4      1      1      2      1      0      1      2      4      4      4      3      4              2      0      B      0      4      0      0      4      1      1      1      4      2      0      3      4      0      0      2      3      4      0      3      4      3      0      2      2      1      4      2      3      3      4      1      2      4      2      0      3      2      ...      4      2      3      0      0      0      0      4      3      2      2      4      2      1      1      1      3      3      1      2      4      4      4      2      2      4      4      0      4      2      0      3      0      1      4      1      4      2      1      2              3      4      L      1      1      2      2      1      1      1      0      2      1      3      2      2      2      4      1      1      4      1      0      1      3      4      2      2      2      4      1      1      2      0      3      0      2      3      4      0      1      ...      3      0      4      0      3      0      2      0      1      4      2      3      4      4      4      0      2      0      4      4      1      3      0      3      2      0      2      3      0      2      3      3      3      0      2      0      3      0      2      2              4      9      D      1      2      0      2      0      4      0      3      4      3      1      0      3      2      2      0      3      4      1      0      4      1      2      2      3      2      2      0      2      0      3      0      3      2      4      0      0      4      ...      0      3      0      1      4      1      3      1      2      1      1      1      2      2      2      4      3      4      3      0      4      1      2      4      1      4      0      1      0      4      3      3      2      0      1      4      0      0      1      1              5      6      A      3      0      2      4      0      3      0      4      2      4      2      1      4      1      1      4      4      0      2      3      4      4      3      3      3      3      4      1      0      3      0      3      0      0      0      1      1      2      ...      2      1      3      2      1      4      2      3      2      2      1      0      4      2      2      1      2      1      0      3      2      2      2      2      1      4      2      1      2      1      4      4      3      2      1      3      4      3      1      2      5 rows × 786 columnstest.head()                  letter      0      1      2      3      4      5      6      7      8      9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      ...      744      745      746      747      748      749      750      751      752      753      754      755      756      757      758      759      760      761      762      763      764      765      766      767      768      769      770      771      772      773      774      775      776      777      778      779      780      781      782      783              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2049      L      0      4      0      2      4      2      3      1      0      0      1      0      1      3      4      4      0      0      2      4      4      1      3      3      2      2      4      1      0      1      2      2      1      2      2      1      4      0      4      ...      1      3      1      1      3      3      4      1      3      1      2      4      1      2      0      3      1      2      4      0      2      1      2      4      1      1      3      2      1      0      2      0      4      2      2      4      3      4      1      4              2050      C      4      1      4      0      1      1      0      2      2      1      0      3      0      1      1      4      1      2      0      2      2      0      4      3      4      0      2      4      4      2      1      2      4      0      4      2      0      2      3      ...      3      4      2      6      2      2      0      1      2      4      1      1      3      3      2      3      4      2      2      4      3      1      3      3      3      1      3      4      4      2      0      3      2      4      2      4      2      2      1      2              2051      S      0      4      0      1      3      2      3      0      2      1      2      0      1      0      3      0      1      4      3      0      0      3      0      4      1      0      3      2      0      4      1      2      0      0      1      3      0      2      1      ...      0      4      4      3      4      1      4      2      3      4      1      2      0      2      2      3      3      1      1      4      1      2      4      0      0      0      0      2      3      2      1      3      2      0      3      2      3      0      1      4              2052      K      2      1      3      3      3      4      3      0      0      2      3      2      3      4      4      4      0      1      4      2      2      0      1      4      3      1      3      0      2      3      2      4      3      1      1      4      0      0      3      ...      0      4      1      1      2      3      2      3      3      0      0      1      3      3      0      2      0      0      2      3      2      2      3      1      1      2      4      0      1      2      3      0      3      2      4      1      0      4      4      4              2053      W      1      0      1      1      2      2      1      4      1      1      4      3      4      1      2      1      4      3      3      4      0      4      4      2      0      0      0      0      3      4      0      1      4      2      2      2      1      4      4      ...      4      1      3      2      1      2      1      4      4      1      2      3      2      4      2      1      4      3      4      3      0      1      0      1      1      2      1      1      0      2      4      3      1      4      0      2      1      2      3      4      5 rows × 785 columns  digit을 맞춰야 하는 대회이다.데이터 살피기train.shape, test.shape, sub.shape ((2048, 786), (20480, 785), (20480, 1))  데이터 shape 확인하기print('digit :', train['digit'].unique(),'\n','num_digit :',train['digit'].nunique()) print('\n','letter : ', train['letter'].unique(),'\n','num_letter :', train['letter'].nunique()) digit : [5 0 4 9 6 8 1 3 2 7]  num_digit : 10 letter :  ['L' 'B' 'D' 'A' 'C' 'Q' 'M' 'F' 'J' 'H' 'N' 'X' 'I' 'R' 'V' 'Y' 'T' 'S' 'U' 'P' 'K' 'O' 'Z' 'G' 'E' 'W']  num_letter : 26  0~9 숫자 10개  알파벳 26개train.dtypes digit      int64letter    object0          int641          int642          int64           ...  779        int64780        int64781        int64782        int64783        int64Length: 786, dtype: object  숫자 맞춰야 하기 때문에 digit을 category형으로 바꿔야 한다.train['digit'].value_counts() 2    2335    2256    2124    2073    2051    2029    1977    1940    1918    182Name: digit, dtype: int64  목적변수의 분포를 파악한다.  클래스간 어느정도 균형을 이루고 있음을 알수 있다.데이터 시각화fig , axes = plt.subplots(2,5)fig.set_size_inches(20,8)for index in range(0,10):    img = np.array(train.iloc[index*3, 2:]).reshape(28, 28).astype(np.float)    axes[index//5 , index%5].imshow(img)    axes[index//5 , index%5].set_title('Letter : {}\nDigit : {}'.format(train['letter'][index*3+1],train['digit'][index*3+1]))    axes[index//5,  index%5].axis('off')  글자 속에 숫자가 숨겨져 있다.이미지 자세히 살펴보기train_img = train.iloc[:,2:].values.reshape(-1, 28, 28, 1).astype(np.float) # 이미지 데이터만 추출plt.subplot(1,2,2)data = np.where(train_img&gt;=140, train_img, 0)plt.imshow(data[3].reshape(28,28),cmap = 'gray')plt.title('Letter : {} , Digit : {}'.format(train['letter'][4], train['digit'][4]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)  D 속에 숨어있는 숫자 9  문자에 겹치는 숫자 부분만 표현된듯 하다.이미지에 Conv2D 적용하기원본 이미지plt.imshow(np.array(train_img[3].reshape(28,28)))plt.title('letter: {} | digit: {}'.format( train['letter'][4],train['digit'][4]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)Conv2D 적용 후conv2d = Conv2D(64, (3,3), input_shape = (28,28,1))conv2d_activation  = Conv2D(64,(3,3), activation = 'relu', input_shape = (28,28,1))fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d(train_img)[3,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')activation 적용 후fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d_activation(train_img)[3,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')MaxPooling2D 적용 후fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(MaxPooling2D(2, 2)(conv2d(train_img))[3, :, :, i], cmap='gray')    axes[i//8, i%8].axis('off')  Letter 부분이 지워지지 않고 그대로 학습됨.  Letter 부분 픽셀을 제거하고 돌려보면 더 좋은 성능이 나올 것 같다.Letter와 숫자가 겹치는 Pixel 값만 나타내보기fig, axes = plt.subplots(2, 10)fig.set_size_inches(20, 5)print('Letter : {}\nDigit : {}'.format(train['letter'][2], train['digit'][2]))for i in range(20):  data = np.where(train_img&gt;=i*5 +100, train_img, 0)  axes[i//10, i%10].imshow(data[1].reshape(28,28))  axes[i//10, i%10].set_title('pixel size : {}'.format(i*5 +100),fontsize = 12)  axes[i//10, i%10].axis('off')Letter : BDigit : 0  Pixel값 140정도  학습시 도움이 되지 않았다.          많은 정보가 손실되는 것 같다.      fig, axes = plt.subplots(2, 2)axes[0,0].imshow(np.array(train_img[3].reshape(28,28))) # 일정 픽셀 이상의 값만 추출해서 모델 적용axes[1,0].imshow(data[3].reshape(28,28))axes[0,0].set_title(train['letter'][4])axes[1,0].set_title(train['digit'][4])axes[0,0].axis('off')axes[1,0].axis('off')axes[0,1].imshow(np.array(train_img[4].reshape(28,28)))axes[1,1].imshow(data[4].reshape(28,28))axes[0,1].set_title(train['letter'][5])axes[1,1].set_title(train['digit'][5])axes[0,1].axis('off')axes[1,1].axis('off')(-0.5, 27.5, 27.5, -0.5)Pixel값이 140 이상인 경우 Conv2D 적용plt.imshow(np.array(data[4].reshape(28,28)))plt.title('letter: {} | digit: {}'.format( train['letter'][5],train['digit'][5]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d(data)[4,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')Data Augmentation  train셋이 2048개로 class 개수에 비해 적다.  데이터를 늘려보자.train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)X = train.drop(['digit','letter'], axis = 1) # imagey = train['digit'] # Label# 정규화X = X / 255.0X = X.values.reshape(-1,28,28,1)y = pd.get_dummies(y.values)train_img = train.iloc[:,2:].values.reshape(-1, 28, 28, 1).astype(np.float) # 이미지 데이터만 추출data = np.where(train_img&gt;=140, train_img, 0)ImageDataGenerator로 Data Augmentation 후 모델 학습(모델 직접 구축)def cnn_model():  # 파일 읽기  train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.  test = pd.read_csv('test.csv', index_col = 0)  sub = pd.read_csv('submission.csv',index_col = 0)  X = train.drop(['digit','letter'], axis = 1) # image  y = train['digit'] # Label  # 정규화  X = X / 255.0  X = (X.values).reshape(-1,28,28,1)  y = pd.get_dummies(y.values)  # train, valid 나누기  X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size = 0.1, random_state=2)  # 모델 생성  model = Sequential([                        Conv2D(512, (3,3), activation = 'relu', input_shape = (28,28,1)),                         #MaxPooling2D(),                        Conv2D(256, (3,3), activation = 'relu'),                         #MaxPooling2D(),                                              Conv2D(128, (3,3), activation = 'relu'),                           Conv2D(128, (3,3), activation = 'relu'),                         MaxPooling2D(),                                                Conv2D(64, (3,3), activation = 'relu'),                         Conv2D(64, (3,3), activation = 'relu'),                         Flatten(),                        Dropout(0.5),                        Dense(512, activation='relu'),                        Dense(10, activation = 'softmax')                      ])  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acc')  # Model Checkpoint  check_path = 'check.ckpt'  check_point = ModelCheckpoint(      filepath = check_path,      monitor = 'val_acc',      verbose = 1,      save_best_only = True,      save_weights_only = True)    # Data Augmentation  datagen = ImageDataGenerator(        featurewise_center=False,  # set input mean to 0 over the dataset        samplewise_center=False,  # set each sample mean to 0        featurewise_std_normalization=False,  # divide inputs by std of the dataset        samplewise_std_normalization=False,  # divide each input by its std        zca_whitening=False,  # apply ZCA whitening        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)        zoom_range = 0.1, # Randomly zoom image         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)        horizontal_flip=False,  # randomly flip images        vertical_flip=False)  # randomly flip images  datagen.fit(X_train)  batch_size = 16  history = model.fit_generator(datagen.flow(X_train,Y_train.values,                                              batch_size=16),                                              epochs = 100,                                              validation_data = (X_val,Y_val.values),                                              verbose = 1,                                               steps_per_epoch=X_train.shape[0] // batch_size  + 1,                                              validation_steps = X_val.shape[0] // batch_size +1,                                              callbacks = [check_point]                                              )                      model.load_weights(check_path)    return modelif __name__ == '__main__':    model = cnn_model()    #model.save("mymodel.h5")114/116 [============================&gt;.] - ETA: 0s - loss: 0.2794 - acc: 0.9045Epoch 00095: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2776 - acc: 0.9056 - val_loss: 0.3073 - val_acc: 0.8976Epoch 96/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2397 - acc: 0.9181Epoch 00096: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2440 - acc: 0.9164 - val_loss: 0.3861 - val_acc: 0.8683Epoch 97/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2676 - acc: 0.9042Epoch 00097: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2677 - acc: 0.9040 - val_loss: 0.2738 - val_acc: 0.8976Epoch 98/100114/116 [============================&gt;.] - ETA: 0s - loss: 0.2470 - acc: 0.9122Epoch 00098: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2465 - acc: 0.9121 - val_loss: 0.3565 - val_acc: 0.8780Epoch 99/100116/116 [==============================] - ETA: 0s - loss: 0.2602 - acc: 0.9126Epoch 00099: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2602 - acc: 0.9126 - val_loss: 0.4582 - val_acc: 0.8634Epoch 100/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2358 - acc: 0.9187Epoch 00100: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2408 - acc: 0.9164 - val_loss: 0.5688 - val_acc: 0.8341최종 제출 코드 Train, Valid dataset 만들기# 데이터 불러오기train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)# 기본 전처리x = np.array(train.iloc[:,2:]).reshape(-1,28,28,1).astype(np.float)  # reshape(batch_size, width, height, channel)y = pd.get_dummies(train['digit']).values  # target variable One Hot Encoding# train, valid 셋 나누기x_train, x_valid , y_train, y_valid = train_test_split(x, y, test_size = 0.1, random_state=1,                                                       stratify=y)x_train = x_train / 255.0x_valid = x_valid / 255.0 모델 구축 및 학습# model model = Sequential([                      Conv2D(256, (3,3), activation = 'relu', input_shape = (28,28,1)),                       MaxPooling2D(),                      Conv2D(256, (3,3), activation = 'relu'),                       #MaxPooling2D(),                                          Conv2D(128, (3,3), activation = 'relu'),                        Conv2D(128, (3,3), activation = 'relu'),                       #MaxPooling2D(),                      Flatten(),                      Dropout(0.5),                      Dense(512, activation='relu'),                      Dense(10, activation = 'softmax')                    ])# model compilemodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acc')# model checkpointcheckpath = 'ck.ckpt'checkpoint = ModelCheckpoint(filepath = checkpath,                             monitor = 'val_loss',                             verbose =1,                             save_best_only = True,                             save_weights_only = True)# model fithistory = model.fit(x_train, y_train,                    validation_data = (x_valid, y_valid),                    batch_size = 32,                    epochs = 40,                    callbacks = [checkpoint])# model load weightsmodel.load_weights(checkpath)학습 결과 시각화Loss / Val_loss 시각화# val_loss vs loss 시각화plt.figure(figsize=(12, 9))plt.plot(np.arange(1, 41), history.history['loss'])plt.plot(np.arange(1, 41), history.history['val_loss'])plt.title('Loss / Val Loss', fontsize=20)plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend(['loss', 'val_loss'], fontsize=15)plt.show()Accuracy / Val_Accuracy 시각화plt.figure(figsize=(12, 9))plt.plot(np.arange(1, 41), history.history['acc'])plt.plot(np.arange(1, 41), history.history['val_acc'])plt.title('acc / val acc', fontsize=20)plt.xlabel('Epochs')plt.ylabel('acc')plt.legend(['acc', 'val_acc'], fontsize=15)plt.show()Test Dataset 전처리x_test = test.drop(['letter'], axis=1).valuesx_test = x_test.reshape(-1, 28, 28, 1)x_test = x_test/255Test Dataset 예측 및 제출sub['digit'] = np.argmax(model.predict(x_test), axis=1)sub.to_csv('submission.csv')sub.head()                  digit              id                        2049      6              2050      8              2051      2              2052      0              2053      3      예측 결과 시각화fig , axes = plt.subplots(2,5)fig.set_size_inches(20,8)for index in range(0,10):    img = np.array(x_test[index*3]).reshape(28, 28).astype(np.float)    axes[index//5 , index%5].imshow(img)    axes[index//5 , index%5].set_title('Letter : {}\nDigit : {}'.format(test['letter'].values[index*3],sub['digit'].values[index*3]))    axes[index//5,  index%5].axis('off')  Letter는 주어진 값이고, 모델은 Digit을 예측한다.]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
