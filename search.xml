<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[Cut-Mix in Classification]]></title>
      <url>/classification/2021/05/25/invasive-pytorch/</url>
      <content type="text"><![CDATA[Cut Mix 방법 소개식물 이미지로 침입종을 분류하는 이진 분류 대회이미지 분류에서 Cut-Mix을 적용하여 모델 성능을 높일 수 있습니다.Invasive Species MonitoringIdentify images of invasive hydrangeahttps://www.kaggle.com/c/invasive-species-monitoring평가 지표 : Area under ROC Curve패키지 불러오기필요한 라이브러라들을 불러옵니다.import numpy as npimport pandas as pd import globfrom torch.utils.data import DataLoader, Datasetfrom sklearn.model_selection import StratifiedKFoldfrom torch.optim.lr_scheduler import ReduceLROnPlateaufrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scorefrom PIL import Imageimport matplotlib.pyplot as pltimport timeimport copyimport cv2import randomfrom tqdm import tqdmimport torch.nn as nnimport torchfrom torchvision import models, transformsimport torch.nn.functional as Fimport warningswarnings.filterwarnings('ignore')import os하이퍼 파라미터하이퍼파라미터 값들이 있는 딕셔너리를 정해줍니다.args = {"TRAIN_LABEL_CSV" : './train_labels.csv',        "TRAIN_PATH" : "../input/invasivespecies/train",        "TEST_PATH" : "../input/invasivespecies/test",        "RESIZE" : 224,#(224,224),        "LEARNING_RATE" : 0.001,        "WEIGHT_DECAY" : 0.003,        "BATCH_SIZE" : 32,        "FEATURE_EXTRACTING" : True,        "NUM_EPOCHS" : 50,        "MEAN" : (0.485, 0.456, 0.406),        "STD" : (0.229, 0.224, 0.225),        "BETA" : 1.0,        "MODEL" : "efficient",        "MODEL_PATH" : ".",        "NUM_FOLDS" : 5,        "DEVICE" : torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')}데이터 불러오기데이터를 간단히 살펴보면, invasive(1) 데이터는 1,448개, non-invasive(0) 데이터는 847개 입니다. 또한 예측에 사용될 Test 데이터는 전체 1,531개 입니다. 학습 데이터 전체를 사용하기 위해, 5-Fold Stratify로 모델을 훈련합니다.def get_data(args):    train = pd.read_csv(args["TRAIN_LABEL_CSV"])    train_list = glob.glob(args["TRAIN_PATH"] + "/*")    test_list = glob.glob(args["TEST_PATH"] + "/*")    path_df = pd.DataFrame({"path" : train_list})    path_df["name"] = path_df["path"].apply(lambda x : x.split("/")[-1].split(".")[0])    path_df["name"] = path_df["name"].astype(np.int)    train_df = pd.merge(train, path_df, on='name', how='left')        print(f"Train 데이터 수 : {len(train_list)}\nTest 데이터 수 : {len(test_list)}\n")    print(f"Target 데이터 수 : \n{train_df['invasive'].value_counts()}\n")        return train_df, train_list, test_listtrain_df, train_list, test_list = get_data(args)train_df.head()Train 데이터 수 : 2295Test 데이터 수 : 1531Target 데이터 수 : 1    14480     847Name: invasive, dtype: int64                  name      invasive      path                  0      1      0      ../input/invasivespecies/train/1.jpg              1      2      0      ../input/invasivespecies/train/2.jpg              2      3      1      ../input/invasivespecies/train/3.jpg              3      4      0      ../input/invasivespecies/train/4.jpg              4      5      1      ../input/invasivespecies/train/5.jpg      Invasive(1)/Non-invasive(0) 이미지 확인침입종 수국(1)과 침입종이 아닌 식물(0)의 이미지입니다.데이터 로더데이터 로더 클래스를 정의합니다.class InvasiveDataset(Dataset):    def __init__(self, dataframe, transform=None):        super().__init__()        self.df = dataframe        self.file_list = dataframe["path"].values        self.transform = transform    def __getitem__(self, index):                path = self.file_list[index]        image_id = path.split("/")[-1].split(".")[0]        image = Image.open(path)        label = self.df[self.df['name']==int(image_id)]['invasive'].values # label in Series. Change to array.        label = label.squeeze()        if self.transform:            image = self.transform(image)                return image, label            def __len__(self):        return len(self.file_list)Transforms 함수 정의Train 데이터에 적용된 augmentation : RandomResizedCrop &amp; RandomHorizontalFlip     Valid 데이터에 적용된 augmentation : Resize &amp; CenterCrop위 augmentation 적용 결과는 아래에서 확인할 수 있습니다.def get_train_transform(args):    return transforms.Compose([        transforms.RandomResizedCrop(args["RESIZE"], scale=(0.5, 1.0)),        transforms.RandomHorizontalFlip(),        transforms.ToTensor(),        transforms.Normalize(mean=args["MEAN"], std=args["STD"])    ])def get_valid_transform(args):    return transforms.Compose([        transforms.Resize(256),        transforms.CenterCrop(args["RESIZE"]),        transforms.ToTensor(),        transforms.Normalize(mean=args["MEAN"], std=args["STD"])    ])데이터 로더 적용 함수데이터 로더를 적용하여 데이터셋을 train, valid로 나누고, 각각 배치 형태로 만들어 줍니다.def get_data_loader(train_df, valid_df=None):        if valid_df is not None: # k-fold        train_set = InvasiveDataset(train_df, transform=get_train_transform(args))        train_loader = DataLoader(train_set, batch_size=args["BATCH_SIZE"], shuffle=True, drop_last=True, num_workers=1)        valid_set = InvasiveDataset(valid_df, transform=get_valid_transform(args))        valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False, num_workers=1)         else:        train_, valid_ = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df["invasive"])        train_set = InvasiveDataset(train_, transform=get_train_transform(args))        train_loader = DataLoader(train_set, batch_size=args["BATCH_SIZE"], shuffle=True, drop_last=True, num_workers=1)        valid_set = InvasiveDataset(valid_, transform=get_valid_transform(args))        valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False, num_workers=1)        return train_loader, valid_loadertrain_loader, valid_loader = get_data_loader(train_df)1. Augmentation 확인하기(Train)Train 데이터에 적용된 augmentation을 시각화했습니다.def plot_train_transform(train_list, args, num_images=3, fig_size=(20,15)):    preprocess1 = transforms.Compose([        transforms.RandomResizedCrop(args["RESIZE"], scale=(0.5, 1.0)),    ])    preprocess2 = transforms.Compose([            transforms.RandomHorizontalFlip(p=1),    ])        fig , axes = plt.subplots(num_images,4)    fig.set_size_inches(fig_size)    random_path = random.sample(train_list,num_images)    for idx in range(num_images):        img = Image.open(random_path[idx])        img_origin = img.copy()        img_aug1 = preprocess1(img)        img_aug2 = preprocess2(img)        img_aug3 = preprocess2(img_aug1)        imgs = [img_origin, img_aug1, img_aug2, img_aug3]        preprocess_name = ["Original Image", f"RandomResizedCrop(Resize:{args['RESIZE']})", "RandomHorizontalFlip"]        for i in range(len(imgs)):            axes[idx, i].imshow(imgs[i])            if i ==len(imgs)-1:                axes[idx, i].set_title(f"{preprocess_name[1]}+{preprocess_name[2]}\nSize : {imgs[i].size}")            else:                axes[idx, i].set_title(f"{preprocess_name[i]}\nSize : {imgs[i].size}")            axes[idx, i].axis('off')            plot_train_transform(train_list, args, num_images=3, fig_size=(15,12))Train셋에 적용된 augmentation을 시각화하였습니다.     먼저 RandomResizedCrop을 통해 이미지를 scale범위 내의 비율로 랜덤하게 크롭한 후, size(224,224)로 크기를 조정했습니다. 그 다음 RandomHorizontalFlip으로 랜덤하게 좌우반전을 주었습니다.2. Augmentation 확인하기(Valid)Valid 데이터에 적용된 augmentation을 시각화했습니다.def plot_valid_transform(train_list, args, num_images=3, fig_size=(20,15)):        preprocess1= transforms.Compose([        transforms.Resize(256),    ])    preprocess2 = transforms.Compose([            transforms.CenterCrop(args["RESIZE"]),    ])        fig , axes = plt.subplots(num_images,4)    fig.set_size_inches(fig_size)    random_path = random.sample(train_list,num_images)    for idx in range(num_images):        img = Image.open(random_path[idx])        img_origin = img.copy()        img_aug1 = preprocess1(img)        img_aug2 = preprocess2(img)        img_aug3 = preprocess2(img_aug1)        imgs = [img_origin, img_aug1, img_aug2, img_aug3]        preprocess_name = ["Original Image", "Resize(256)", f"CenterCrop(Resize:{args['RESIZE']})"]        for i in range(len(imgs)):            axes[idx, i].imshow(imgs[i])            if i == len(imgs)-1:                axes[idx, i].set_title(f"{preprocess_name[1]}+{preprocess_name[2]}\nSize : {imgs[i].size}")            else:                axes[idx, i].set_title(f"{preprocess_name[i]}\nSize : {imgs[i].size}")            axes[idx, i].axis('off')plot_valid_transform(train_list, args, num_images=3, fig_size=(15,12))Valid 데이터에는 Resize 와 CenterCrop이 적용되었습니다.   Resize에서는 가로세로 비율을 유지하며 이미지 크기를 조정하였습니다. Invasive 대회의 데이터셋의 이미지들은 주로 중앙쪽에 주요 정보(침입종 식물)가 분포되어 있었습니다.   이러한 점을 이용하여 CenterCrop으로 중간 부분 위주로 이미지를 잘라내어 모델이 주요한 정보에 더욱 집중하여 학습할 수 있도록 하였습니다.학습 준비모델, Optimizer, Criterion모델은 resnet18을 사용했습니다. 이진 분류이므로, num_classes를 2로 설정합니다.    Criterion은 CrossEntropyLoss, Optimizer은 Adam을 사용합니다.def set_parameter_requires_grad(model, feature_extracting):    if feature_extracting:        for param in model.parameters():            param.requires_grad = Falsedef get_model(args):    model = models.resnet18(pretrained=True)    set_parameter_requires_grad(model, args["FEATURE_EXTRACTING"])    model.fc = nn.Linear(512, 2)    model = model.to(args["DEVICE"])    criterion = nn.CrossEntropyLoss()    optimizer = torch.optim.Adam(params=model.parameters(), lr=args["LEARNING_RATE"], weight_decay=args["WEIGHT_DECAY"])        return model, criterion, optimizer한개의 배치에 대해 모델 성능 확인전체 데이터를 학습하기 전, 우선 하나의 배치에 대해서 모델의 학습이 잘 이루어지는 확인합니다. 전체 데이터를 학습하게 되면, 모델이 잘 훈련되고 있는지 확인하기까지 시간이 많이 소요되므로 먼저 일부 데이터에 대해서만 학습을 진행해봅니다.아래 출력되는 loss값을 보면, 모델 학습이 잘 이루어진 것을 확인할 수 있습니다.def check_batch(model, train_loader,criterion, optimizer, args):    image, label = next(iter(train_loader))    for i in range(30):        data = image.to(args["DEVICE"])        targets = label.to(args["DEVICE"])        outputs = model(data)        loss = criterion(outputs, targets)        optimizer.zero_grad()        loss.backward()        optimizer.step()        print(loss)    return model, criterion, optimizer = get_model(args)check_batch(model, train_loader, criterion, optimizer, args)tensor(0.7094, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.6460, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.5871, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.5325, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.4823, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.4363, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.3945, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.3565, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.3224, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.2917, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.2642, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.2396, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.2177, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1982, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1808, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1653, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1514, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1391, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1280, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1181, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1092, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.1013, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0941, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0876, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0818, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0765, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0717, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0674, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0635, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)tensor(0.0599, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)Cut MixCut Mix 설명Cut mix은 주어진 이미지에 대해서 랜덤하게 패치를 만들어내어 다른 이미지에 합성하는 augmentation 기법 중 하나입니다. Cut mix의 예시는 아래 4.결과에서 확인할 수 있습니다.rand_bbox 함수는 패치를 생성하는 코드입니다. 주어진 이미지 크기 내에서, 랜덤하게 패치를 만들어내고 이에 대한 좌표를 반환하는 함수입니다.패치의 폭과 넓이는, 주어진 이미지의 폭과 넓이에 np.sqrt(1-lam)을 곱하여 얻게 됩니다. 여기서 lam은 베타분포에서 랜덤하게 얻은 값입니다. 이렇게 얻어낸 패치 부분을, 랜덤하게 섞은 X(input)값들의 패치 부분으로 교체합니다. 이때 합성된 이미지의 레이블은, 합성된 전체 이미지에서 각각의 이미지가 차지하는 면적 비율만큼(lam)을 더한 값이 됩니다.y = λ x invasive + (1-λ) x non-invasive자세한 코드 설명은 아래에 있습니다. 이러한 cut mix 기법은 모델이 본 적이 없는 데이터에 대해서도 robust하게 예측할 수 있도록 도움을 줄 수 있습니다.def rand_bbox(size, lam): # size : [B, C, W, H]    W = size[2] # 이미지의 width    H = size[3] # 이미지의 height    cut_rat = np.sqrt(1. - lam)  # 패치 크기의 비율 정하기    cut_w = np.int(W * cut_rat)  # 패치의 너비    cut_h = np.int(H * cut_rat)  # 패치의 높이    # uniform    # 기존 이미지의 크기에서 랜덤하게 값을 가져옵니다.(중간 좌표 추출)    cx = np.random.randint(W)    cy = np.random.randint(H)    # 패치 부분에 대한 좌표값을 추출합니다.    bbx1 = np.clip(cx - cut_w // 2, 0, W)    bby1 = np.clip(cy - cut_h // 2, 0, H)    bbx2 = np.clip(cx + cut_w // 2, 0, W)    bby2 = np.clip(cy + cut_h // 2, 0, H)    return bbx1, bby1, bbx2, bby2Cut Mix 적용 과정1. 배치 내의 데이터 셔플데이터 로더에서 배치별로 데이터가 불러와지면, torch.randperm함수를 사용하여 인덱스를 랜덤하게 셔플합니다. 처음 람다값은 베타 분포에서 랜덤하게 가져옵니다.X,y = next(iter(train_loader))X = X.to(args["DEVICE"])y = y.to(args["DEVICE"])lam = np.random.beta(1.0, 1.0)  # 베타 분포에서 lam 값을 가져옵나다.rand_index = torch.randperm(X.size()[0]).to(args["DEVICE"]) # batch_size 내의 인덱스가 랜덤하게 셔플됩니다.shuffled_y = y[rand_index] # 타겟 레이블을 랜덤하게 셔플합니다.print(lam)print(rand_index)0.2246911819269543tensor([ 3, 26, 11, 24,  5, 28, 17, 31, 21, 27,  9, 13, 20, 12, 15,  1, 14, 16,        30,  2, 25,  8, 19, 29, 18, 22,  6,  7, 10, 23,  4,  0],       device='cuda:0')2. 패치 부분 교체하기기존 이미지의 패치 부분을, rand_index를 통해 셔플된 이미지들의 패치로 채워 넣습니다.bbx1, bby1, bbx2, bby2 = rand_bbox(X.size(), lam)X[:,:,bbx1:bbx2, bby1:bby2] = X[rand_index,:,bbx1:bbx2, bby1:bby2]3. 람다 조정하기픽셀 비율과 정확히 일치하도록 람다를 조정합니다.  lam은 이후 loss값을 계산하는데 사용됩니다.lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (X.size()[-1] * X.size()[-2]))print(lam)0.496093754. 결과섞인 이미지를 볼 수 있습니다.plt.imshow(X[0].permute(1, 2, 0).cpu())&lt;matplotlib.image.AxesImage at 0x7fc575f0ad10&gt;Cut Mix 결과 이미지def cutmix_plot(train_loader):    fig , axes = plt.subplots(1,3)    fig.set_size_inches(15,12)        for i in range(3):        for inputs, targets in train_loader:            inputs = inputs            targets = targets            break        lam = np.random.beta(1.0, 1.0)         rand_index = torch.randperm(inputs.size()[0])        bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)        inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))        axes[i].imshow(inputs[1].permute(1, 2, 0).cpu())        axes[i].set_title(f'λ : {np.round(lam,3)}')        axes[i].axis('off')    returncutmix_plot(train_loader)모델 학습Validation 함수대회의 평가 지표가 area under the ROC curve이므로, validation 함수에서 AUC가 계산되도록 하였습니다.def validation(model, valid_loader, criterion):    accuracy = 0    valid_loss = 0    y_score = []    for i, (X, y) in enumerate(valid_loader):        if torch.cuda.is_available():            X = X.to('cuda')            y = y.to('cuda')        outputs = model(X)        loss = criterion(outputs, y)        valid_loss += loss.item()        outputs_ = torch.argmax(outputs, dim=1)                accuracy += (outputs_ == y).float().sum()                # For roc_auc_score        outputs = F.softmax(outputs, dim=-1)        y_score.append(outputs.detach().cpu().numpy())        y_score = np.array([i for sub in y_score for i in sub])    return valid_loss, accuracy, y_scoreTrain 함수cut mix를 적용한 모델 훈련 코드입니다.  스케줄러는 ReduceLROnPlateau을 사용하였습니다.   val_loss가 가장 낮은 모델만 저장되도록 하였습니다.def train_model(model, train_loader, valid_loader, criterion, optimizer, args, fold_num=1):    steps = 0    total_step = len(train_loader)    train_losses, validation_losses = [], []    best_val = np.inf        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=True)        if torch.cuda.is_available():        model = model.to(args["DEVICE"])    for epoch in range(args['NUM_EPOCHS']):        running_loss = 0        for i, (X, y) in enumerate(train_loader):                        if torch.cuda.is_available():                X = X.to(args["DEVICE"])                y = y.to(args["DEVICE"])                            if args["BETA"] &gt; 0 and np.random.random()&gt;0.5: # cutmix 작동될 확률                      lam = np.random.beta(args["BETA"], args["BETA"])                rand_index = torch.randperm(X.size()[0]).to(args["DEVICE"])                target_a = y                target_b = y[rand_index]                            bbx1, bby1, bbx2, bby2 = rand_bbox(X.size(), lam)                X[:, :, bbx1:bbx2, bby1:bby2] = X[rand_index, :, bbx1:bbx2, bby1:bby2]                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (X.size()[-1] * X.size()[-2]))                outputs = model(X)                loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)                            else:                outputs = model(X)                loss = criterion(outputs, y)                                              steps += 1            optimizer.zero_grad()            loss.backward()            optimizer.step()            running_loss += loss.item()                        if steps % total_step == 0:                model.eval()                with torch.no_grad():                    valid_loss, accuracy, y_score = validation(model, valid_loader, criterion)                print("Epoch: {}/{}.. ".format(epoch + 1, args['NUM_EPOCHS']) +                      "Training Loss: {:.5f}.. ".format(running_loss / total_step) +                      "Valid Loss: {:.5f}.. ".format(valid_loss / len(valid_loader)) +                      "Valid Accuracy: {:.5f}.. ".format(accuracy / len(valid_loader.dataset)) + # 전체 데이터 수에 대해 나눠준다                      "Area Under Curve: {:.5f}..".format(roc_auc_score(valid_loader.dataset.df['invasive'].values, y_score[:, 1])))                                 # Save Model                if (valid_loss / len(valid_loader)) &lt; best_val:                    best_val = (valid_loss / len(valid_loader))                    torch.save(model.state_dict(), f"{args['MODEL_PATH']}/{fold_num}_best_checkpoint_{str(epoch + 1).zfill(3)}epoch.tar")                    try:                        os.remove(f_pth)                    except:                        pass                    f_pth = f"{args['MODEL_PATH']}/{fold_num}_best_checkpoint_{str(epoch + 1).zfill(3)}epoch.tar"                                train_losses.append(running_loss / len(train_loader))                validation_losses.append(valid_loss / len(valid_loader))                steps = 0                running_loss = 0                model.train()                        scheduler.step(valid_loss / len(valid_loader))    return Fold 없이 단일 모델 학습150개 데이터에 대해서 코드에 이상이 없는지 빠르게 실험해봤습니다.train, train_list, test_list = get_data(args)train_loader, valid_loader = get_data_loader(train[:150])model, criterion, optimizer = get_model(args)train_model(model, train_loader, valid_loader, criterion, optimizer, args)Train 데이터 수 : 2295Test 데이터 수 : 1531Target 데이터 수 : 1    14480     847Name: invasive, dtype: int64Epoch: 1/50.. Training Loss: 0.73771.. Valid Loss: 0.65644.. Valid Accuracy: 0.66667.. Area Under Curve: 0.62000Epoch: 2/50.. Training Loss: 0.67826.. Valid Loss: 0.57433.. Valid Accuracy: 0.66667.. Area Under Curve: 0.76000Epoch: 3/50.. Training Loss: 0.59255.. Valid Loss: 0.54534.. Valid Accuracy: 0.80000.. Area Under Curve: 0.88000Epoch: 4/50.. Training Loss: 0.54118.. Valid Loss: 0.50904.. Valid Accuracy: 0.66667.. Area Under Curve: 0.86000Epoch: 5/50.. Training Loss: 0.55861.. Valid Loss: 0.51992.. Valid Accuracy: 0.66667.. Area Under Curve: 0.90000Epoch: 6/50.. Training Loss: 0.52161.. Valid Loss: 0.44656.. Valid Accuracy: 0.73333.. Area Under Curve: 0.94000Epoch: 7/50.. Training Loss: 0.49751.. Valid Loss: 0.40690.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 8/50.. Training Loss: 0.45763.. Valid Loss: 0.37955.. Valid Accuracy: 0.80000.. Area Under Curve: 1.00000Epoch: 9/50.. Training Loss: 0.45117.. Valid Loss: 0.39089.. Valid Accuracy: 0.73333.. Area Under Curve: 1.00000Epoch: 10/50.. Training Loss: 0.46032.. Valid Loss: 0.33357.. Valid Accuracy: 0.86667.. Area Under Curve: 1.00000Epoch: 11/50.. Training Loss: 0.53030.. Valid Loss: 0.31587.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 12/50.. Training Loss: 0.41064.. Valid Loss: 0.31270.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 13/50.. Training Loss: 0.42074.. Valid Loss: 0.30051.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 14/50.. Training Loss: 0.47569.. Valid Loss: 0.30176.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 15/50.. Training Loss: 0.37441.. Valid Loss: 0.27918.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 16/50.. Training Loss: 0.42745.. Valid Loss: 0.27515.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 17/50.. Training Loss: 0.36221.. Valid Loss: 0.26101.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 18/50.. Training Loss: 0.40636.. Valid Loss: 0.27356.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 19/50.. Training Loss: 0.37077.. Valid Loss: 0.24716.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 20/50.. Training Loss: 0.39795.. Valid Loss: 0.23801.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 21/50.. Training Loss: 0.33757.. Valid Loss: 0.23665.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 22/50.. Training Loss: 0.30681.. Valid Loss: 0.23004.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 23/50.. Training Loss: 0.57297.. Valid Loss: 0.22571.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 24/50.. Training Loss: 0.43501.. Valid Loss: 0.23294.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 25/50.. Training Loss: 0.29009.. Valid Loss: 0.22755.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 26/50.. Training Loss: 0.35224.. Valid Loss: 0.22339.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 27/50.. Training Loss: 0.30319.. Valid Loss: 0.20292.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 28/50.. Training Loss: 0.38560.. Valid Loss: 0.21128.. Valid Accuracy: 0.93333.. Area Under Curve: 1.00000Epoch: 29/50.. Training Loss: 0.29471.. Valid Loss: 0.20262.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 30/50.. Training Loss: 0.32983.. Valid Loss: 0.19320.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 31/50.. Training Loss: 0.31507.. Valid Loss: 0.18878.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 32/50.. Training Loss: 0.29588.. Valid Loss: 0.18774.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 33/50.. Training Loss: 0.38119.. Valid Loss: 0.17978.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 34/50.. Training Loss: 0.38593.. Valid Loss: 0.17570.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 35/50.. Training Loss: 0.40947.. Valid Loss: 0.18061.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 36/50.. Training Loss: 0.28186.. Valid Loss: 0.18099.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 37/50.. Training Loss: 0.45454.. Valid Loss: 0.17288.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 38/50.. Training Loss: 0.29359.. Valid Loss: 0.17260.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 39/50.. Training Loss: 0.28112.. Valid Loss: 0.17446.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 40/50.. Training Loss: 0.29126.. Valid Loss: 0.16450.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 41/50.. Training Loss: 0.47272.. Valid Loss: 0.16572.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 42/50.. Training Loss: 0.29763.. Valid Loss: 0.16253.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 43/50.. Training Loss: 0.23737.. Valid Loss: 0.15957.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 44/50.. Training Loss: 0.31854.. Valid Loss: 0.15708.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 45/50.. Training Loss: 0.28495.. Valid Loss: 0.15177.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 46/50.. Training Loss: 0.33081.. Valid Loss: 0.15521.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 47/50.. Training Loss: 0.36897.. Valid Loss: 0.14969.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 48/50.. Training Loss: 0.38459.. Valid Loss: 0.15459.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 49/50.. Training Loss: 0.24727.. Valid Loss: 0.15773.. Valid Accuracy: 1.00000.. Area Under Curve: 1.00000Epoch: 50/50.. Training Loss: 0.24382.. Valid Loss: 0.15321.. Valid Accuracy: 1.00000.. Area Under Curve: 1.000005-Fold 학습5번의 교차 검증 및 학습을 진행합니다. 각 교차 검증에서 가장 낮은 val_loss의 모델을 저장합니다. 이후 test 데이터 예측 시, 5개의 모델의 예측 결과를 voting을 통해 앙상블할 수 있습니다.def fold_train(args, train_df):    folds = StratifiedKFold(n_splits=args["NUM_FOLDS"], shuffle=True, random_state=42)    X = train_df    for i, (train_index, valid_index) in enumerate(folds.split(X,X["invasive"])):        fold_num = i+1        X_train = X.iloc[train_index]        X_val = X.iloc[valid_index]        model, criterion, optimizer = get_model(args)        train_loader, valid_loader = get_data_loader(X_train, X_val)        print("=" * 100)        print(f"{fold_num}/{args['NUM_FOLDS']} Cross Validation Training Starts ...\n")        train_model(model, train_loader, valid_loader, criterion, optimizer, args, fold_num=fold_num)        print(f"\n{fold_num}/{args['NUM_FOLDS']} Cross Validation Training Ends ...\n")    return fold_train(args, train_df)====================================================================================================1/5 Cross Validation Training Starts ...Epoch: 1/50.. Training Loss: 0.54776.. Valid Loss: 0.40315.. Valid Accuracy: 0.81481.. Area Under Curve: 0.89644Epoch: 2/50.. Training Loss: 0.41436.. Valid Loss: 0.35228.. Valid Accuracy: 0.84314.. Area Under Curve: 0.92516Epoch: 3/50.. Training Loss: 0.40087.. Valid Loss: 0.32396.. Valid Accuracy: 0.84967.. Area Under Curve: 0.93788Epoch: 4/50.. Training Loss: 0.37837.. Valid Loss: 0.31565.. Valid Accuracy: 0.87800.. Area Under Curve: 0.94291Epoch: 5/50.. Training Loss: 0.36564.. Valid Loss: 0.29177.. Valid Accuracy: 0.87146.. Area Under Curve: 0.94726Epoch: 6/50.. Training Loss: 0.32669.. Valid Loss: 0.29859.. Valid Accuracy: 0.86057.. Area Under Curve: 0.94791Epoch: 7/50.. Training Loss: 0.35096.. Valid Loss: 0.27714.. Valid Accuracy: 0.89760.. Area Under Curve: 0.94918Epoch: 8/50.. Training Loss: 0.36902.. Valid Loss: 0.28113.. Valid Accuracy: 0.89978.. Area Under Curve: 0.94979Epoch: 9/50.. Training Loss: 0.35148.. Valid Loss: 0.27296.. Valid Accuracy: 0.90632.. Area Under Curve: 0.95223Epoch: 10/50.. Training Loss: 0.35789.. Valid Loss: 0.26953.. Valid Accuracy: 0.89542.. Area Under Curve: 0.95172Epoch: 11/50.. Training Loss: 0.36694.. Valid Loss: 0.28762.. Valid Accuracy: 0.86275.. Area Under Curve: 0.95213Epoch: 12/50.. Training Loss: 0.39102.. Valid Loss: 0.26983.. Valid Accuracy: 0.89760.. Area Under Curve: 0.95384Epoch: 13/50.. Training Loss: 0.38918.. Valid Loss: 0.27797.. Valid Accuracy: 0.88453.. Area Under Curve: 0.95359Epoch: 14/50.. Training Loss: 0.38915.. Valid Loss: 0.26591.. Valid Accuracy: 0.89325.. Area Under Curve: 0.95589Epoch: 15/50.. Training Loss: 0.37149.. Valid Loss: 0.26141.. Valid Accuracy: 0.89542.. Area Under Curve: 0.95728Epoch: 16/50.. Training Loss: 0.37789.. Valid Loss: 0.26677.. Valid Accuracy: 0.89542.. Area Under Curve: 0.95542Epoch: 17/50.. Training Loss: 0.35278.. Valid Loss: 0.26262.. Valid Accuracy: 0.89760.. Area Under Curve: 0.95646Epoch: 18/50.. Training Loss: 0.38484.. Valid Loss: 0.26765.. Valid Accuracy: 0.88889.. Area Under Curve: 0.95656Epoch: 19/50.. Training Loss: 0.38384.. Valid Loss: 0.26423.. Valid Accuracy: 0.89978.. Area Under Curve: 0.95709Epoch: 20/50.. Training Loss: 0.37842.. Valid Loss: 0.26729.. Valid Accuracy: 0.89978.. Area Under Curve: 0.95528Epoch: 21/50.. Training Loss: 0.35959.. Valid Loss: 0.26464.. Valid Accuracy: 0.89107.. Area Under Curve: 0.95663Epoch: 22/50.. Training Loss: 0.35680.. Valid Loss: 0.29077.. Valid Accuracy: 0.87800.. Area Under Curve: 0.95760Epoch: 23/50.. Training Loss: 0.35464.. Valid Loss: 0.25870.. Valid Accuracy: 0.90196.. Area Under Curve: 0.95734Epoch: 24/50.. Training Loss: 0.36316.. Valid Loss: 0.30004.. Valid Accuracy: 0.88235.. Area Under Curve: 0.95589Epoch: 25/50.. Training Loss: 0.36476.. Valid Loss: 0.25689.. Valid Accuracy: 0.90196.. Area Under Curve: 0.95923. . .참고 자료Invasive Hydrangeas : http://jenmenke.com/who-knew-hydrangeas-were-invasive/    CutMix Code(clovaai) : https://github.com/clovaai/CutMix-PyTorch  CutMix 설명 - 이유한님  : https://github.com/kaggler-tv/codes/blob/master/cutmix/cutmix.ipynb]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> CNN </tag>
        
          <tag> Pytorch </tag>
        
          <tag> Cut Mix </tag>
        
          <tag> ResNet </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[VinBigData Chest X-ray(Object Detection)]]></title>
      <url>/detection/2021/04/17/vinbigdata-train/</url>
      <content type="text"><![CDATA[대회 소개Chest X-ray 흉부 기형 탐지하기VinBigData Chest X-ray Abnormalities DetectionAutomatically localize and classify thoracic abnormalities from chest radiographs Kaggle Link : https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/overview패키지 불러오기import numpy as npimport pandas as pd import cv2import torchimport torchvision# 데이터 로더 관련from torch.utils.data import DataLoader, Datasetfrom torch.utils.data.sampler import SequentialSampler, RandomSamplerimport albumentations as Afrom albumentations.pytorch.transforms import ToTensorV2from datetime import datetimeimport timeimport osimport sysimport warningsfrom random import randintimport randomimport matplotlib.pyplot as pltwarnings.filterwarnings("ignore")kaggle에서 EfficientDet을 사용하기 위한 준비 과정  VinBigData lib을 검색하여 현재 디렉토리에 추가해준다.(Add data)  아래 코드를 실행하여 wheel 설치를 진행하고, 경로를 추가해주면 effdet 패키지를 사용할 수 있다.!pip install ../input/vbg-lib/timm-0.1.26-py3-none-any.whl!tar xfz ../input/vbg-lib/pkgs.tgz# for pytorch1.6cmd = "sed -i -e 's/ \/ / \/\/ /' timm-efficientdet-pytorch/effdet/bench.py"!$cmdProcessing /kaggle/input/vbg-lib/timm-0.1.26-py3-none-any.whlRequirement already satisfied: torch&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from timm==0.1.26) (1.7.0)Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.1.26) (0.8.1)Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.0-&gt;timm==0.1.26) (0.18.2)Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.0-&gt;timm==0.1.26) (3.7.4.3)Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.0-&gt;timm==0.1.26) (0.6)Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.0-&gt;timm==0.1.26) (1.19.5)Requirement already satisfied: pillow&gt;=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm==0.1.26) (7.2.0)Installing collected packages: timmSuccessfully installed timm-0.1.26sys.path.insert(0, "timm-efficientdet-pytorch")sys.path.insert(0, "omegaconf")from effdet import get_efficientdet_config, EfficientDet, DetBenchTrainfrom effdet.efficientdet import HeadNet데이터 경로 설정args = {    'TRAIN_DATA' : '../input/vinbigdata-original-image-dataset/vinbigdata/train',}데이터 불러오기  Bounding Box가 검출이 되지 않은 이미지는 제외해보자.train = pd.read_csv('/kaggle/input/vinbigdata-original-image-dataset/vinbigdata/train.csv')train = train.loc[train['class_id']!=14]train.head()                  image_id      class_name      class_id      rad_id      x_min      y_min      x_max      y_max      width      height                  2      9a5094b2563a1ef3ff50dc5c7ff71345      Cardiomegaly      3      R10      691.0      1375.0      1653.0      1831.0      2080      2336              3      051132a778e61a86eb147c7c6f564dfe      Aortic enlargement      0      R10      1264.0      743.0      1611.0      1019.0      2304      2880              5      1c32170b4af4ce1a3030eb8167753b06      Pleural thickening      11      R9      627.0      357.0      947.0      433.0      2540      3072              6      0c7a38f293d5f5e4846aa4ca6db4daf1      ILD      5      R17      1347.0      245.0      2188.0      2169.0      2285      2555              7      47ed17dcb2cbeec15182ed335a8b5a9e      Nodule/Mass      8      R9      557.0      2352.0      675.0      2484.0      2568      3353      이미지 확인하기  한 이미지에서 여러 boundingbox가 겹쳐 보이는 이유는, multiple radiologists가 labeling을 했기 때문이다.  Data Description의 설명을 확인하자.   Note that a key part of this competition is working with ground truth from multiple radiologists.imgs = []img_ids = train['image_id'].valuesclass_ids = train['class_id'].unique()# map label_id to specify colorlabel2color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}scale = 5fig , axes = plt.subplots(2,4)fig.set_size_inches(15,8)for index in range(8):    img_id = random.choice(img_ids)    img_path = f'{args["TRAIN_DATA"]}/{img_id}.jpg'    img = cv2.imread(img_path)    img = cv2.resize(img, None, fx=1/scale, fy=1/scale)    boxes = train.loc[train['image_id'] == img_id, ['x_min', 'y_min', 'x_max', 'y_max']].values/scale    labels = train.loc[train['image_id'] == img_id, ['class_id']].values.squeeze()        for label_id, box in zip(labels, boxes):        color = label2color[label_id]        img = cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 3)            img = cv2.resize(img, (500,500))        axes[index//4 , index%4].imshow(img)    axes[index//4,  index%4].axis('off')데이터 전처리데이터 로더 만들기  데이터 로더 만들 시 주의할 점은, 어떤 Detection 모델을 사용하느냐에 따라 boxes 좌표 순서가 바뀔 수 있다는 점이다.  EfficientDet 모델은 y_min, x_min, y_max, x_max 순서로 좌표가 들어가므로 이에 맞춰서 전처리를 해주어야 한다.  Kaggle에서 dicom 파일의 이미지들을 jpg 포맷으로 바꿔놓은 데이터가 올라와 있으므로, 이 데이터를 사용해보자.class VinBigDataset(Dataset):    def __init__(self, marking, image_ids, transforms=None, test=False):        super().__init__()        self.marking = marking        self.image_ids = image_ids        self.transforms = transforms        self.test = test            def __len__(self):        return self.image_ids.shape[0]        def load_image_and_boxes(self, index):        image_id = self.image_ids[index]        image = cv2.imread(f"{args['TRAIN_DATA']}/{image_id}.jpg", cv2.IMREAD_COLOR)        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)        image = image / 255.0                records = self.marking[self.marking['image_id']==image_id]        boxes = records[['x_min','y_min','x_max','y_max']].values        labels = records['class_id'].values                return image, boxes, labels            def __getitem__(self, index):                image_id = self.image_ids[index]                image, boxes, labels = self.load_image_and_boxes(index)                target = {'boxes':boxes, 'labels': torch.tensor(labels)}                if self.transforms:            sample = {'image' : image, 'bboxes' : boxes, 'labels' : labels}            sample = self.transforms(**sample)            image = sample['image']                        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(sample['bboxes'])))).squeeze(1)                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:, [1,0,3,2]]                return image, target, image_idTrain Valid 나누기  Train, Valid 데이터는 8:2로 나눠보자.  데이터 로더에 들어갈 형식에 맞게 이미지 id들을 나눠준다.random_seed= 42image_names = np.random.permutation(train['image_id'].unique())val_len = int(len(image_names)*0.2)images_train = image_names[:-val_len]images_valid = image_names[-val_len:]print(len(images_train), len(images_valid))3516 878transforms 함수 만들기  이미지들의 크기가 모두 다르기 때문에, Resize 함수로 전처리를 해준다.  데이터 로더는 나중에 정의할 것이기 때문에, 여기서는 train_set, valid_set만 정의해주자.def get_train_transforms():    return A.Compose([A.Resize(512,512),                      A.Flip(0.5),                      ToTensorV2()],                      bbox_params=A.BboxParams(format='pascal_voc',                                               label_fields=['labels']))def get_valid_transforms():    return A.Compose([A.Resize(512,512),                      ToTensorV2()],                      bbox_params=A.BboxParams(format='pascal_voc',                                               label_fields=['labels']))train_set = VinBigDataset(train, images_train,transforms=get_train_transforms(), test=False)valid_set = VinBigDataset(train, images_valid, transforms=get_valid_transforms(), test=True)모델 훈련을 위한 함수 정의Fitter 클래스 정의  모델 학습 파이프라인을 설정해주자.fit 함수  전체 Epoch 동안의 학습 과정을 관리하는 함수이다.train_one_epoch 함수  하나의 Epoch 내의 모델 학습 과정을 관리하는 함수이다.validation 함수  하나의 Epoch 내의 valid 데이터 예측 및 평가 과정을 관리하는 함수이다.save 함수  모델 학습 과정에서, 모델의 파라미터 저장 등을 관리하는 함수이다.load 함수  저장된 모델 파라미터 등을 불러오는 함수이다.logger 함수  학습이 진행되는 동안의 log를 기록하는 함수이다.  AdamW, model.named_parameters 관련 참고 사이트model.named_parameters() : https://comlini8-8.tistory.com/50 AdamW : https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.htmlclass Fitter:    def __init__(self, model, device, config):        self.model = model        self.device = device        self.config = config        self.epoch = 0        self.base_dir = f"./{config.folder}"                if not os.path.exists(self.base_dir):            os.makedirs(self.base_dir)                self.log_path = f"{self.base_dir}/log.txt"        self.best_summary_loss = 10**5        self.param_optimizer = list(self.model.named_parameters())        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = config.lr)        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)        self.log(f"device is {self.device}")    def fit(self, train_loader, valid_loader):        for e in range(self.config.n_epochs):            if self.config.verbose:                lr = self.optimizer.param_groups[0]['lr']                timestamp = datetime.utcnow().isoformat()                self.log(f"시간 : {timestamp}, Learning Rate : {lr}")                        t = time.time()            summary_loss = self.train_one_epoch(train_loader)            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}') # loss 평균               self.save(f"{self.base_dir}/last-checkpoint.bin")                        t = time.time()            summary_loss = self.validation(valid_loader)            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')                        if summary_loss.avg &lt; self.best_summary_loss:                self.best_summary_loss = summary_loss.avg                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')                            try:                    os.remove(f)                except:                    pass                                f = f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin'            if self.config.validation_scheduler:                self.scheduler.step(metrics=summary_loss.avg)                        self.epoch+=1                    def validation(self, val_loader):        self.model.eval()        summary_loss = AverageMeter()        t = time.time()        for step, (images, targets, image_ids) in enumerate(val_loader):            if self.config.verbose:                if step % self.config.verbose_step == 0:                    print(f"Val Step : {step}/{len(val_loader)}, " + \                          f"summary_loss : {summary_loss.avg:.5f}, " + \                          f"time : {(time.time() - t):.5f}", end="\r"                         )            with torch.no_grad():                images = torch.stack(images)                batch_size = images.shape[0]                images = images.to(self.device).float()                boxes = [x['boxes'].to(self.device).float() for x in targets]                labels = [x['labels'].to(self.device).float() for x in targets]                                loss, _, _ = self.model(images, boxes, labels)                                summary_loss.update(loss.detach().item(), batch_size)        return summary_loss                        def train_one_epoch(self, train_loader):        self.model.train()        summary_loss = AverageMeter()        t = time.time()        for step, (images, targets, image_ids) in enumerate(train_loader):            if self.config.verbose:                if step % self.config.verbose_step == 0:                    print(f"Train Step : {step}/{len(train_loader)}, " + \                          f"summary_loss : {summary_loss.avg:.5f}, " + \                          f"time : {(time.time() - t):.5f}", end="\r"                         )            images = torch.stack(images)            images = images.to(self.device).float()            batch_size = images.shape[0]            boxes = [x['boxes'].to(self.device).float() for x in targets]            labels = [x['labels'].to(self.device).float() for x in targets]                        self.optimizer.zero_grad()            loss, _, _ = self.model(images, boxes, labels)            loss.backward()            summary_loss.update(loss.detach().item(), batch_size)            self.optimizer.step()                        if self.config.step_scheduler:                self.scheduler.step()                return summary_loss                        def save(self, path):        self.model.eval()        torch.save({            'model_state_dict' : self.model.state_dict(),            'optimizer_state_dict' : self.optimizer.state_dict(),            'scheduler_state_dict' : self.scheduler.state_dict(),            'best_summary_loss' : self.best_summary_loss,            'epoch' : self.epoch        }, path)            def load(self, path):        checkpoint = torch.load(path)        self.model.load_state_dict(checkpoint['model_state_dict'])        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])        self.best_summary_loss = checkpoint['best_summary_loss']        self.epoch = checkpoint['epoch'] + 1            def log(self, message):        if self.config.verbose:            print(message)        with open(self.log_path, 'a+') as logger:            logger.write(f'{message}\n')AverageMeter 클래스 정의  Loss 계산을 위한 클래스를 정의한다.class AverageMeter(object):    """Computes and stores the average and current value"""    def __init__(self):        self.reset()    def reset(self):        self.val = 0        self.avg = 0        self.sum = 0        self.count = 0    def update(self, val, n=1):        self.val = val        self.sum += val * n        self.count += n        self.avg = self.sum / self.countTrainGlobalConfig 클래스 정의(하이퍼 파라미터 설정)  Fitter 클래스에서 사용될 하이퍼파라미터를 설정한다.class TrainGlobalConfig:    num_workers = 2    batch_size = 4     n_epochs = 20    lr = 0.0002    folder = 'effdet5-models'    verbose = True    verbose_step = 1    step_scheduler = False    validation_scheduler = True    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau    scheduler_params = dict(        mode='min',        factor=0.5,        patience=5,        verbose=False,         threshold=0.0001,        threshold_mode='abs',        cooldown=0,         min_lr=1e-8,        eps=1e-08    )데이터로더 정의 및 학습 함수(Fit) 정의  DataLoader의 파라미터별 용도 참고 : https://subinium.github.io/pytorch-dataloader/          shuffle : 데이터를 섞어서 불러오는 옵션      sampler : shuffle과 유사한 옵션이나,index를 조정하는 옵션. 이 옵션을 사용하려면 shuffle옵션은 False(기본값)이어야 한다.                  RandomSampler : 랜덤으로 불러온다. Replacement 여부 선택 가능, 개수 선택 가능          SequentialSampler : 항상 같은 순서로 불러온다.                    pin_memory : 메모리 소모를 줄이는 옵션. Tensor를 CUDA 고정 메모리에 올린다.      drop_last : Batch의 길이가 다른 경우에 따라 loss를 구하기 어려운 경우가 생기고, batch의 크기에 따른 의존도 높은 함수를 사용할 때 걱정이 되는 경우 True 옵션을 주어 마지막 batch를 사용하지 않을 수 있다.      collate_fn : map-style 데이터셋에서 sample list를 batch 단위로 바꾸기 위해 필요한 옵션. Zero-padding이나 Variable Size 데이터 등 데이터 사이즈를 맞추기 위해 많이 사용한다.      def run_training():    device = torch.device('cuda:0')    net.to(device)    train_loader = torch.utils.data.DataLoader(        train_set,        batch_size=TrainGlobalConfig.batch_size,        sampler=RandomSampler(train_set),        pin_memory=False,        drop_last=True,        num_workers=TrainGlobalConfig.num_workers,        collate_fn=collate_fn,    )    val_loader = torch.utils.data.DataLoader(        valid_set,         batch_size=TrainGlobalConfig.batch_size,        num_workers=TrainGlobalConfig.num_workers,        shuffle=False,        sampler=SequentialSampler(valid_set),        pin_memory=False,        collate_fn=collate_fn,    )        fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)    fitter.fit(train_loader, val_loader)def collate_fn(batch):    return tuple(zip(*batch))모델 정의  모델은 EfficientDet을 사용해본다.  pretrained를 True로 설정하고, 모델의 config.num_classes를 사용 목적에 맞게 꼭 바꿔주자. 그렇지 않으면 모델 학습시 input 사이즈 에러가 발생한다.def get_net():    config = get_efficientdet_config("tf_efficientdet_d0")    net = EfficientDet(config, pretrained_backbone=True)    config.image_size = 512    config.num_classes = 14    net.class_net = HeadNet(config, num_outputs=config.num_classes)    return DetBenchTrain(net, config)모델 훈련  시드를 고정시킨 후, 학습을 진행한다.random_seed= 42torch.manual_seed(random_seed)torch.cuda.manual_seed(random_seed)torch.backends.cudnn.deterministic = Truetorch.backends.cudnn.benchmark = Falsenp.random.seed(random_seed)random.seed(random_seed)net = get_net()run_training()device is cuda:0시간 : 2021-04-17T06:39:02.714171, Learning Rate : 0.0002[RESULT]: Train. Epoch: 0, summary_loss: 248.13443, time: 522.19967[RESULT]: Val. Epoch: 0, summary_loss: 7.92773, time: 104.95752시간 : 2021-04-17T06:49:30.211356, Learning Rate : 0.0002[RESULT]: Train. Epoch: 1, summary_loss: 3.93020, time: 518.21235[RESULT]: Val. Epoch: 1, summary_loss: 2.32165, time: 102.05856시간 : 2021-04-17T06:59:50.871942, Learning Rate : 0.0002[RESULT]: Train. Epoch: 2, summary_loss: 1.79468, time: 519.58026[RESULT]: Val. Epoch: 2, summary_loss: 1.56013, time: 101.75270시간 : 2021-04-17T07:10:12.591429, Learning Rate : 0.0002[RESULT]: Train. Epoch: 3, summary_loss: 1.34318, time: 520.87777[RESULT]: Val. Epoch: 3, summary_loss: 1.29430, time: 102.15580시간 : 2021-04-17T07:20:36.011119, Learning Rate : 0.0002[RESULT]: Train. Epoch: 4, summary_loss: 1.12282, time: 524.12953[RESULT]: Val. Epoch: 4, summary_loss: 1.12894, time: 103.07392시간 : 2021-04-17T07:31:03.716515, Learning Rate : 0.0002[RESULT]: Train. Epoch: 5, summary_loss: 0.98686, time: 525.41065[RESULT]: Val. Epoch: 5, summary_loss: 1.01692, time: 103.37271시간 : 2021-04-17T07:41:32.885071, Learning Rate : 0.0002[RESULT]: Train. Epoch: 6, summary_loss: 0.89752, time: 524.67249Val Step : 9/220, summary_loss : 0.91504, time : 4.61471...결론  EfficientDet b0를 사용한 베이스라인 코드이다.  여러 방사선 전문의들이 레이블링하였기 때문에 이러한 overlapping boxes에 대한 전처리가 필요할 것으로 보인다.      또한, CutMix 기법, Weighted Boxes Fusion 등을 적용해볼 수 있다. 모델 역시 YOLOv5 등을 사용해볼 수 있다.    코드 참고 : https://www.kaggle.com/morizin/14-class-efficientdet-detection-trainWeighted Boxes Fusion Example      겹치는 박스(동일 레이블)를 처리할 수 있다.        Weighted Boxes Fusion 내용 : https://towardsdatascience.com/  ]]></content>
      <categories>
        
          <category> Detection </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Pytorch </tag>
        
          <tag> Detection </tag>
        
          <tag> EfficientDet </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Weighted Boxes Fusion(Detection)]]></title>
      <url>/detection/2021/03/29/wheatdetection-wbf-inference/</url>
      <content type="text"><![CDATA[Weighted Boxes Fusion 방법 소개밀이삭 영역 탐지 대회(Global Wheat Detection)Weighted Boxes Fusion을 사용하여 Detection 문제에서 여러 모델의 예측 결과를 앙상블할 수 있습니다.WBF(Weighted Boxes Fusion)WBF는 Object detection 문제에서, 여러 바운딩 박스들을 앙상블하는 방법입니다. 모델은 이미지 내에 어디에(4개의 좌표값), 어떤 Class가(ex 사람/강아지/고양이), 얼마만큼의 확률(0~1)로 존재하는지 예측하게 됩니다. 만약 여러 모델에서 각기 다른 예측값들을 효과적으로 앙상블할 수 있다면, 모델의 성능을 더욱 높일 수 있을 것입니다.기존 NMS(Non Maximum Suppression)과 Soft-NMS extenstion의 방법도 존재합니다. NMS나 Soft-NMS는 예측값의 일부를 제거하는 방식이라면, WBF는 예측된 모든 바운딩 박스들의 정보를 사용하여 결합하는 방식입니다. 아래 이미지를 보면, 빨간색 박스가 모델의 예측값이고 파란색 박스가 정답값입니다. NMS는 부정확한 박스를 모두 제외하고 하나의 박스만 남겼다면, WBF는 예측된 3개의 박스의 정보를 모두 사용하여 최종 예측값을 뽑아냈습니다.WBF의 알고리즘을 간단히 요약하자면, 두 박스의 IoU를 계산하여 iou_thr 임계값을 넘으면 융합이 진행되는데, 융합 시 예측된 각 박스의 score에 따라 결과가 조정되는 방식입니다. 예측 박스들을 융합하는 알고리즘은 아래 참고 블로그에서 더욱 자세하게 알 수 있습니다.WBF 참고 블로그 : https://lv99.tistory.com/74패키지 불러오기import numpy as npimport pandas as pdimport torchfrom torch.utils.data import Dataset, DataLoaderimport torchvisionfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictorimport albumentations as Afrom albumentations.pytorch.transforms import ToTensorV2from glob import globimport cv2import matplotlib.pyplot as pltimport os# Weighted Boxes Fusion 라이브러리!pip install --no-deps '../input/weightedboxesfusion/' &gt; /dev/nullfrom ensemble_boxes import * 하이퍼파라미터 설정args = {    "TEST_DIR" : "/kaggle/input/global-wheat-detection/sample_submission.csv",    "TEST_IMG_DIR" : '../input/global-wheat-detection/test/',    "DEVICE" : torch.device('cuda' if torch.cuda.is_available() else 'cpu')}데이터 전처리데이터셋 만들기(불러오기)  sample_submission파일을 불러옵니다.test = pd.read_csv(args["TEST_DIR"])test                  image_id      PredictionString                  0      aac893a91      1.0 0 0 50 50              1      51f1be19e      1.0 0 0 50 50              2      f5a1f0358      1.0 0 0 50 50              3      796707dd7      1.0 0 0 50 50              4      51b3e36ab      1.0 0 0 50 50              5      348a992bb      1.0 0 0 50 50              6      cc3532ff6      1.0 0 0 50 50              7      2fd875eaa      1.0 0 0 50 50              8      cb8d261a3      1.0 0 0 50 50              9      53f253011      1.0 0 0 50 50      데이터 로더 만들기class WheatTestDataLoader(Dataset):    def __init__(self, dataframe, image_dir, transforms=None):        super().__init__()        self.df = dataframe        self.image_dir = image_dir        self.transforms = transforms        self.image_ids = dataframe['image_id'].unique()            def __getitem__(self, index):                 # Dataset의 index와 연동되어 이미지 각각에 대해 접근한다.        image_id = self.image_ids[index]         image = cv2.imread(self.image_dir + image_id + '.jpg', cv2.IMREAD_COLOR)                # 메모리 소모량을 줄이기 위해 데이터타입을 float32로 바꾼다.        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)                 # keras에서는 이미지의 픽셀 값들을 0~1 사이로 자동적으로 표준화를 해주지만, 여기선 직접 설정해줘야 한다.        image = image / 255.0                if self.transforms :            sample = {'image' : image}                        # ** : transforms 함수 안에 'image'라는 이름에 접근한다는 의미            sample = self.transforms(**sample)                        image = sample['image']                return image, image_id            def __len__(self):        return len(self.image_ids)# transforms def get_test_transforms():    return A.Compose([            A.Resize(height=512, width=512),            ToTensorV2()])# collate_fndef collate_fn(batch):    return tuple(zip(*batch))testset = WheatTestDataLoader(test, args["TEST_IMG_DIR"], transforms=get_test_transforms())test_loader = DataLoader(testset,                         batch_size=1,                         shuffle=False,                          num_workers=4,                         collate_fn=collate_fn)모델 불러와서 Weighted Boxes Fusion 적용하기모델 가중치 로드하기  5번의 교차검증으로 학습된 5개의 모델을 불러옵니다. 불러오기 전, 학습했을 때의 모델 구조로 바꿔줍니다.  추가하고자 하는 모델 가중치가 gpu 환경에서 학습되었기 때문에, 우리의 모델 역시 gpu 환경으로 맞춰줍니다. 만약 cpu로 예측을 하고 싶으면, map_location 옵션을 cpu로 설정합니다.  또한 데이터를 예측 시에는 model.eval()옵션을 줍니다. 이 옵션을 사용하면 모델 내부의 모든 layer가 evaluation 모드가 됩니다. Evaluation 모드에서는 batchnorm, Dropout과 같은 기능들이 사용되지 않습니다.def get_model(path):    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained_backbone=False)    model.roi_heads.box_predictor = FastRCNNPredictor(1024, 2)    model.load_state_dict(torch.load(path, map_location=args['DEVICE']))    model = model.to(args["DEVICE"])    model.eval()    return modelmodels = [get_model('../input/model/fold0_model.tar'),          get_model('../input/model/fold1_model.tar'),          get_model('../input/model/fold2_model.tar'),          get_model('../input/model/fold3_model.tar'),          get_model('../input/model/fold4_model.tar')]5개 모델의 예측 결과 저장하는 함수 정의  make_ensemble_predictions 함수는 5개의 모델의 예측값을 모두 저장하여 리스트 값으로 반환해주는 함수입니다.# 앙상블 def make_ensemble_predictions(images):    images = [x.to(args["DEVICE"]) for x in images]     result = []    for model in models:         outputs = model(images)        result.append(outputs)    return resultWeighted Boxes Fusion 함수 정의  WBF 적용할 때는, bounding box 좌표값들이 0~1 사이의 값을 가져야 합니다. 아래 코드에서 좌표값들을 (image_size-1)로 나누는 이유가 바로 그것입니다.  weighted_boxes_fusion 함수에서, iou_thr과 skip_box_thr 하이퍼파라미터를 이해해야 합니다.          iou_thr : IoU(Intersection over Union) 임계값입니다. 두 박스의 IoU를 계산하여, 임계값을 넘겼을 경우, config_type='avg'(디폴트 값) 방식으로 두 박스가 융합됩니다.      skip_box_thr class에 대한 예측 확률값(scores)이 skip_box_thr 임계값을 넘길 때만 가져옵니다.      # WBF 적용def run_wbf(predictions, image_index, image_size=512, iou_thr=0.55, skip_box_thr=0.7):        # (image_size-1)로 나누는 이유 :    # WBF 적용 시 bounding box로 들어올 때, 이미지를 512로 리사이즈 했으므로 0~511사이의 값들을 갖는다. 이 값들을 0~1사이로 바뀌어야 한다.    boxes = [x[image_index]['boxes'].data.cpu().numpy() / (image_size-1) for x in predictions]        scores =  [x[image_index]['scores'].data.cpu().numpy() for x in predictions] # scores에 접근, gpu 연산 필요 없음!        labels = [np.ones(x[image_index]['scores'].shape[0]) for x in predictions] # bb가 발견된 경우, 무조건 벼(class=1)        boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, iou_thr=iou_thr, skip_box_thr=skip_box_thr, conf_type='avg',)    # iou_thr : IoU(Intersection over Union) 임계값이다. 두 박스의 IoU를 계산하여, 임계값을 넘겼을 경우, config_type='avg'(디폴트 값) 방식으로 두 박스가 융합된다.    # skip_box_thr : class에 대한 값이 skip_box_thr 임계값을 넘길 때만 가져온다.        # 다시 512 사이즈로 바꿔준다.    boxes = boxes * (image_size-1)        return boxes, scores, labelsWeighted Boxes Fusion 적용 결과 확인1. Weighted Boxes Fusion 적용하지 않았을 때  5개의 모델이 예측한 바운딩 박스를 모두 plot 해봤습니다. 모델 별로 색을 다르게 하여 구분할 수 있게 했습니다.  아래 결과 이미지를 보면, 5개 모델이 예측한 바운딩 박스들을 볼 수 있습니다.# WBF 적용되지 않은 결과fig, axes = plt.subplots(2, 5)fig.set_size_inches(15, 8)color = [(0,0,255),(255,0,255),(0,255,255),(0,255,0),(255,0,0)]for i, (images, image_ids) in enumerate(test_loader):    predictions = make_ensemble_predictions(images)    img = cv2.imread(f"/kaggle/input/global-wheat-detection/test/{image_ids[0]}.jpg", cv2.IMREAD_COLOR)    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    for folds in range(len(predictions)):        for index in range(len(predictions[folds][0]['boxes'])):                        # bbox 값의 범위를 이미지 사이즈(1024)에 맞춰준다.            bbox = (predictions[folds][0]['boxes'][index]*2)            bbox = bbox.data.cpu().numpy().astype(np.int32).clip(min=0, max=1023)            xmin, ymin, xmax, ymax = int(float(bbox[0])), int(float(bbox[1])), int(float(bbox[2])), int(float(bbox[3]))            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color[folds], 3)    axes[i//5, i%5].imshow(img)    axes[i//5, i%5].set_title(image_ids[0])    axes[i//5, i%5].axis('off')2. Weighted Boxes Fusion 적용했을 때  WBF를 적용한 결과입니다.# WBF 적용된 결과fig, axes = plt.subplots(2, 5)fig.set_size_inches(15, 8)for i, (images, image_ids) in enumerate(test_loader):    predictions = make_ensemble_predictions(images)    sample = images[0].permute(1,2,0).cpu().numpy()    boxes, scores, labels = run_wbf(predictions, image_index=0)    boxes = boxes.astype(np.int32).clip(min=0, max=511)    for box in boxes:        cv2.rectangle(sample,                      (box[0], box[1]),                      (box[2], box[3]),                      (255, 0, 0), 2)    axes[i//5, i%5].imshow(sample)    axes[i//5, i%5].set_title(image_ids[0])    axes[i//5, i%5].axis('off')3. Weighted Boxes Fusion 적용 전과 후  왼쪽 이미지부터 원본, WBF 적용 전, WBF 적용 후의 이미지입니다.fig, axes = plt.subplots(1, 3)fig.set_size_inches(15, 10)images, image_ids = next(iter(test_loader))predictions = make_ensemble_predictions(images)color = [(0,0,255),(255,0,255),(0,255,255),(255,255,0),(255,0,0)]img = cv2.imread(f"/kaggle/input/global-wheat-detection/test/{image_ids[0]}.jpg", cv2.IMREAD_COLOR)img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)img_origin = img.copy()img_wbf = img.copy()for folds in range(len(predictions)):    for index in range(len(predictions[folds][0]['boxes'])):        # bbox 값의 범위를 이미지 사이즈(1024)에 맞춰준다.        bbox = (predictions[folds][0]['boxes'][index]*2)        bbox = bbox.data.cpu().numpy().astype(np.int32).clip(min=0, max=1023)        xmin, ymin, xmax, ymax = int(float(bbox[0])), int(float(bbox[1])), int(float(bbox[2])), int(float(bbox[3]))        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color[folds], 3)boxes, scores, labels = run_wbf(predictions, image_index=0)boxes = (boxes*2).astype(np.int32).clip(min=0, max=1023)for box in boxes:    cv2.rectangle(img_wbf,(box[0], box[1]),(box[2], box[3]), (0, 255, 0), 3)axes[0].imshow(img_origin)axes[0].set_title("Origin Image")axes[0].axis('off')axes[1].imshow(img)axes[1].set_title("Before WBF(5 models prediction)")axes[1].axis('off')axes[2].imshow(img_wbf)axes[2].set_title("WBF")axes[2].axis('off')(-0.5, 1023.5, 1023.5, -0.5)중간 이미지는 5개의 모델에 대한 예측값들 모두 그려진 상태입니다. 모델이 각각 다른 가중치를 갖고 있기에 예측값도 조금씩 다릅니다. 이러한 예측값들을 모두 고려하여 WBF를 적용한 결과, 가장 오른쪽의 이미지와 같은 결과를 갖게 됩니다. 각 예측값의 score 값, 박스 간 IoU 값들을 기준으로 융합된 결과입니다.제출하기  최근 kaggle에서는 예측값이 담긴 csv만 제출하는 것이 아니라, 부정행위를 방지하기 위해 notebook 전체를 제출하게끔 하고 있습니다.  notebook으로 제출하기                            오른쪽 콘솔에서 Internet 을 끈다.                                      Save Version -&gt; Advanced Settings -&gt; Always save output                                      submission 파일 이름은 반드시 submission.csv로 설정하기                                      Save하여 제출                    Submission 양식으로 바꿔주는 함수 정의하기  대회마다 submission 제출 양식이 다릅니다. 이 대회에서는 아래와 같은 제출 양식을 요구하고 있습니다. 즉 image_id별로 [class_score, xmin, ymin, width, height] 의 양식으로 들어가야 한다는 것입니다.test.head()                  image_id      PredictionString                  0      aac893a91      1.0 0 0 50 50              1      51f1be19e      1.0 0 0 50 50              2      f5a1f0358      1.0 0 0 50 50              3      796707dd7      1.0 0 0 50 50              4      51b3e36ab      1.0 0 0 50 50      def format_prediction_string(boxes, scores):    pred_strings = []    for j in zip(scores, boxes):        pred_strings.append("{0:.4f} {1} {2} {3} {4}".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))    return " ".join(pred_strings)최종 예측  fasterrcnn_resnet50_fpn의 output은 자동적으로 labels, scores, boxes 값들이 dictionary 형태로 저장됩니다.  또한 output의 boxes은 [xmin, ymin, xmax, ymax]로 나오므로, [xmin, ymin, w, h]로 처리해줘야 합니다.# 예측 코드results = []for images, image_ids in test_loader:    predictions = make_ensemble_predictions(images)    for i, image in enumerate(images):        boxes, scores, labels = run_wbf(predictions, image_index=i) # WBF 적용        boxes = (boxes*2).astype(np.int32).clip(min=0, max=1023) # (1024,1024)로 이미지 사이즈 맞춰줌        image_id = image_ids[i]        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]                result = {            'image_id': image_id,            'PredictionString': format_prediction_string(boxes, scores)        }        results.append(result)test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])test_df.to_csv('submission.csv', index=False)test_df.head()                  image_id      PredictionString                  0      aac893a91      0.9948 58 0 115 162 0.9921 618 914 74 109 0.98...              1      51f1be19e      0.9933 605 87 156 166 0.9898 278 470 133 121 0...              2      f5a1f0358      0.9972 886 645 86 141 0.9964 544 272 107 113 0...              3      796707dd7      0.9935 710 823 107 101 0.9922 942 73 81 100 0....              4      51b3e36ab      0.9986 836 450 186 145 0.9982 234 644 93 155 0...      참고 자료  Weighted Boxes Fusion Ensemble Code : https://www.kaggle.com/shonenkov/wbf-approach-for-ensemble  Weighted Boxes Fusion Github : https://github.com/ZFTurbo/Weighted-Boxes-Fusion  Weighted Boxes Fusion 정리 : https://lv99.tistory.com/74]]></content>
      <categories>
        
          <category> Detection </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Pytorch </tag>
        
          <tag> Detection </tag>
        
          <tag> Faster RCNN </tag>
        
          <tag> StratifyKFold </tag>
        
          <tag> Ensemble </tag>
        
          <tag> Weighted Boxes Fusion </tag>
        
          <tag> Non Maximum Suppression </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Global Wheat Detection(Object Detection)]]></title>
      <url>/detection/2021/03/14/wheat-detection-train/</url>
      <content type="text"><![CDATA[대회 소개밀이삭 영역 탐지하기Global Wheat DetectionCan you help identify wheat heads using image analysis?You are attempting to predict bounding boxes around each wheat head in images that have them. If there are no wheat heads, you must predict no bounding boxes.캐글 주소 : https://www.kaggle.com/c/global-wheat-detection/overview  데이터셋에 대한 논문 : https://arxiv.org/abs/2005.02162패키지 불러오기import numpy as np import pandas as pd import osfrom PIL import Imageimport cv2import matplotlib.pyplot as pltimport torchimport torch.nn as nnfrom torch.utils.data import Dataset, DataLoaderimport torchvisionfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictorfrom sklearn.model_selection import StratifiedKFoldfrom glob import globfrom albumentations.pytorch.transforms import ToTensorV2import albumentations as A하이퍼파라미터 설정args = {    "TRAIN" : '/kaggle/input/global-wheat-detection/train/',    "DEVICE" : torch.device('cuda' if torch.cuda.is_available() else 'cpu'),    "BATCH_SIZE" : 8,    "NUM_EPOCHS" : 20,    "NUM_FOLDS" : 5}이미지 확인하기img = Image.open('/kaggle/input/global-wheat-detection/train/944c60a15.jpg')print(img.size)img.resize((256,256))(1024, 1024)  이미지의 크기가 (1024,1024)로 매우 크다.  int32, float32 로 이미지 데이터 타입을 바꿔서 메모리 소모량을 최소화하자.  float32 보다 float16으로 하면 안되는가          모델 학습 시 gradient update할 때, 소수점 아래 미세한 차이가 결과적으로 큰 차이를 가져온다.      float16은 소수점 4번째까지만 저장하게 되고, 그 아래의 값들은 모두 버리게 되어 정보가 손실된다. 모델 성능에까지 영향을 주게 된다.      따라서 이미지의 데이터 타입은 보통 float32으로 설정해준다.      print('origin  :', (0.9)**8, '\nfloat16 :', np.float16((0.9)**8), '\nfloat32 :', np.float32((0.9)**8), '\nfloat64 :', np.float64((0.9)**8))origin  : 0.4304672100000001 float16 : 0.4304 float32 : 0.43046722 float64 : 0.4304672100000001데이터 전처리  Detection 대회에서 항상 기억해야할 전처리          데이터셋 만들기(불러오기)      Data Loader만들기      데이터셋 만들기(불러오기)  train 데이터프레임을 보면, bbox의 칼럼값이 str으로 들어가 있다.  각 값을 x, y, w, h칼럼에 각각 넣어주는 처리를 해주자.  아래 코드에서 np.stack() 함수를 이용하면 쉽게 데이터 프레임의 칼럼으로 집어 넣을 수 있다.  dtypes을 찍어봐서 x, y, w, h 값들이 숫자형태로 들어갔는지 확인하자.          Detection 문제에서 사용할 수 있는 모델(efficientdet, fastrcnn 등)에 따라 bounding box의 전처리가 조금씩 달라질 수 있다.      지금은 fastrcnn모델을 사용할 것이므로 이에 맞는 전처리를 해줘야 한다.      train = pd.read_csv('/kaggle/input/global-wheat-detection/train.csv')train[['x','y','w','h']] = np.stack(train['bbox'].apply(lambda x : np.fromstring(x[1:-1], sep=',')))train.head()                  image_id      width      height      bbox      source      x      y      w      h                  0      b6ab77fd7      1024      1024      [834.0, 222.0, 56.0, 36.0]      usask_1      834.0      222.0      56.0      36.0              1      b6ab77fd7      1024      1024      [226.0, 548.0, 130.0, 58.0]      usask_1      226.0      548.0      130.0      58.0              2      b6ab77fd7      1024      1024      [377.0, 504.0, 74.0, 160.0]      usask_1      377.0      504.0      74.0      160.0              3      b6ab77fd7      1024      1024      [834.0, 95.0, 109.0, 107.0]      usask_1      834.0      95.0      109.0      107.0              4      b6ab77fd7      1024      1024      [26.0, 144.0, 124.0, 117.0]      usask_1      26.0      144.0      124.0      117.0      train.dtypesimage_id     objectwidth         int64height        int64bbox         objectsource       objectx           float64y           float64w           float64h           float64dtype: object데이터셋 살펴보기print(f"total length : {len(train)}\nunique length : {len(train['image_id'].unique())}")total length : 147793unique length : 3373  train셋에서 총 행의 개수가 147793이지만, unique한 개수는 3373개이다. 즉, 한 이미지에 여러 bounding box 값들이 있다는 의미이다.  Detection 모델에 데이터를 넣어줄 때 관례적으로 bounding box의 [xmin,ymin, xmax,ymax] 입력받는다.(왼쪽 위, 오른쪽 아래 점)  현재 train 데이터프레임의 bbox 칼럼에서는 [xmin, ymin, width, height]값으로 들어가 있으므로 좌표 역시 전처리 해줘야 한다. 모델 예측한 ouput값은 [xmin,ymin, xmax,ymax]로 나오므로, 역시 제출 시 처리를 해줘야 한다는 것을 까먹지 말자.이미지에 Bounding Box 그려보기1. 이미지에 하나의 bounding box 그려보기index=0img = cv2.imread(args["TRAIN"] + train['image_id'][index] + '.jpg', cv2.IMREAD_COLOR)img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)xmin, ymin, w, h = int(train['x'][index]), int(train['y'][index]), int(train['w'][index]), int(train['h'][index])xmax, ymax = xmin+w, ymin+hcv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255,0,0), 3)plt.imshow(img)&lt;matplotlib.image.AxesImage at 0x7f0d019dfc10&gt;2. 한 이미지에 해당하는 모든 bounding box 그려보기fig, axes = plt.subplots(1, 5)fig.set_size_inches(20, 15)for i in range(0,5):    img_idx = i    image_id = train['image_id'].unique()[img_idx]    temp_df = train[train['image_id']==image_id].reset_index()    img = cv2.imread(f"{args['TRAIN']}{image_id}.jpg", cv2.IMREAD_COLOR)    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    for index in range(0,len(temp_df)):        xmin, ymin, w, h = int(temp_df['x'][index]), int(temp_df['y'][index]), int(temp_df['w'][index]), int(temp_df['h'][index])        xmax, ymax = xmin+w, ymin+h        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255,0,0), 3)    axes[i].imshow(img)    axes[i].axis('off')교차 검증을 위해 Train, Valid 데이터셋 나누기  현재 이미지마다 바운딩박스의 개수가 모두 다르다.  먼저 바운딩박스 개수에 대한 정보를 추가하자. 또한, 현재 데이터에는 source 칼럼에 벼 종류에 대한 정보가 있다. 참고로 벼 종류의 유니크한 개수는 7이다. 이러한 벼 종류 역시 골고루 나눠지게 해야 한다.  바운딩박스의 개수 및 벼 종류에 따라 구간을 나눠서 카테고리를 나눠보자.  예를 들어 0-15개의 바운딩 박스를 갖는 이미지는 a 클래스, 16-30개의 바운딩박스를 갖는 이미지는 b 클래스 …1. image_id 데이터 가져오기df_folds = train[['image_id']].copy()df_folds                  image_id                  0      b6ab77fd7              1      b6ab77fd7              2      b6ab77fd7              3      b6ab77fd7              4      b6ab77fd7              ...      ...              147788      5e0747034              147789      5e0747034              147790      5e0747034              147791      5e0747034              147792      5e0747034      147793 rows × 1 columns2. image_id 별로 bounding box 개수 count하기df_folds.loc[:,'bbox_count'] = 1 df_folds = df_folds.groupby('image_id').count()df_folds                  bbox_count              image_id                        00333207f      55              005b0d8bb      20              006a994f7      25              00764ad5d      41              00b5fefed      25              ...      ...              ffb445410      57              ffbf75e5b      52              ffbfe7cc0      34              ffc870198      41              ffdf83e42      39      3373 rows × 1 columns3. source 정보 추가하기# 벼(source) 역시 교차검증 나눌 때 클래스별로 나누기print(f"Unique Number of Source : {train['source'].nunique()}")df_folds.loc[:, 'source'] = train[['image_id','source']].groupby('image_id').min()["source"]df_foldsUnique Number of Source : 7                  bbox_count      source              image_id                              00333207f      55      arvalis_1              005b0d8bb      20      usask_1              006a994f7      25      inrae_1              00764ad5d      41      inrae_1              00b5fefed      25      arvalis_3              ...      ...      ...              ffb445410      57      rres_1              ffbf75e5b      52      arvalis_1              ffbfe7cc0      34      arvalis_1              ffc870198      41      usask_1              ffdf83e42      39      arvalis_1      3373 rows × 2 columns4. bbox_count, source 정보를 합친 stratify_group 칼럼 만들기  bounding box의 개수의 범위 정보와 source 정보가 담겨져 있는 stratify_group 칼럼 생성한다.  stratify_group은 source 종류와 바운딩박스 개수를 15로 나누었을 때의 나머지에 대한 정보가 들어간다.df_folds.loc[:,'stratify_group'] = np.char.add(df_folds['source'].values.astype(str),                                                df_folds['bbox_count'].apply(lambda x : f'_{x//15}').values)df_folds                  bbox_count      source      stratify_group              image_id                                    00333207f      55      arvalis_1      arvalis_1_3              005b0d8bb      20      usask_1      usask_1_1              006a994f7      25      inrae_1      inrae_1_1              00764ad5d      41      inrae_1      inrae_1_2              00b5fefed      25      arvalis_3      arvalis_3_1              ...      ...      ...      ...              ffb445410      57      rres_1      rres_1_3              ffbf75e5b      52      arvalis_1      arvalis_1_3              ffbfe7cc0      34      arvalis_1      arvalis_1_2              ffc870198      41      usask_1      usask_1_2              ffdf83e42      39      arvalis_1      arvalis_1_2      3373 rows × 3 columns5. stratify_group의 개수 파악하기# bbox 개수 및 source 별 개수df_folds['stratify_group'].value_counts()arvalis_1_2    495ethz_1_4       364arvalis_1_3    346ethz_1_5       214rres_1_3       172arvalis_2_1    171arvalis_3_1    160arvalis_3_0    154rres_1_2       145ethz_1_3       141inrae_1_1      127arvalis_1_1    115arvalis_3_3    104arvalis_3_2     94usask_1_1       93usask_1_2       81rres_1_4        75arvalis_1_4     70arvalis_3_4     40rres_1_1        39inrae_1_0       31arvalis_2_0     26arvalis_1_5     23ethz_1_6        18inrae_1_2       18usask_1_3       15usask_1_0       11ethz_1_2         8arvalis_3_5      7arvalis_2_2      7arvalis_1_6      5ethz_1_7         2arvalis_1_0      1rres_1_5         1Name: stratify_group, dtype: int646. fold 칼럼을 추가하여 데이터 나누기  먼저 fold의 기본값을 0으로 설정하고, StratifiedKFold을 이용하여 stratify_group 기준으로 0~4의 값으로 나눠준다.df_folds.loc[:, 'fold']=0df_folds                  bbox_count      source      stratify_group      fold              image_id                                          00333207f      55      arvalis_1      arvalis_1_3      0              005b0d8bb      20      usask_1      usask_1_1      0              006a994f7      25      inrae_1      inrae_1_1      0              00764ad5d      41      inrae_1      inrae_1_2      0              00b5fefed      25      arvalis_3      arvalis_3_1      0              ...      ...      ...      ...      ...              ffb445410      57      rres_1      rres_1_3      0              ffbf75e5b      52      arvalis_1      arvalis_1_3      0              ffbfe7cc0      34      arvalis_1      arvalis_1_2      0              ffc870198      41      usask_1      usask_1_2      0              ffdf83e42      39      arvalis_1      arvalis_1_2      0      3373 rows × 4 columnsfrom sklearn.model_selection import StratifiedKFoldskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)# 그룹 나누기for i, (train_index, valid_index) in enumerate(skf.split(df_folds,df_folds['stratify_group'])):    df_folds.loc[df_folds.iloc[valid_index].index,'fold'] = i df_folds = df_folds.reset_index()df_folds                  image_id      bbox_count      source      stratify_group      fold                  0      00333207f      55      arvalis_1      arvalis_1_3      1              1      005b0d8bb      20      usask_1      usask_1_1      3              2      006a994f7      25      inrae_1      inrae_1_1      1              3      00764ad5d      41      inrae_1      inrae_1_2      0              4      00b5fefed      25      arvalis_3      arvalis_3_1      3              ...      ...      ...      ...      ...      ...              3368      ffb445410      57      rres_1      rres_1_3      1              3369      ffbf75e5b      52      arvalis_1      arvalis_1_3      1              3370      ffbfe7cc0      34      arvalis_1      arvalis_1_2      3              3371      ffc870198      41      usask_1      usask_1_2      4              3372      ffdf83e42      39      arvalis_1      arvalis_1_2      4      3373 rows × 5 columns7. fold별로 stratify_group의 개수 파악하기  fold 별로 잘 나누어졌는지 확인한다.for i in range(5):    print("*"*30,'\n')    print(f"FOLD : {i}\n")    print(df_folds[df_folds["fold"]==i]['stratify_group'].value_counts(),'\n')****************************** FOLD : 0arvalis_1_2    99ethz_1_4       72arvalis_1_3    70ethz_1_5       43rres_1_3       35arvalis_2_1    34arvalis_3_1    32arvalis_3_0    31rres_1_2       29ethz_1_3       28inrae_1_1      26arvalis_1_1    23arvalis_3_3    21usask_1_1      18arvalis_3_2    18usask_1_2      16rres_1_4       15arvalis_1_4    14arvalis_3_4     8rres_1_1        8inrae_1_0       7arvalis_2_0     6ethz_1_6        4arvalis_1_5     4inrae_1_2       3usask_1_3       3usask_1_0       2ethz_1_2        2arvalis_3_5     1arvalis_1_0     1arvalis_1_6     1arvalis_2_2     1Name: stratify_group, dtype: int64 ****************************** FOLD : 1arvalis_1_2    99ethz_1_4       73arvalis_1_3    69ethz_1_5       43arvalis_2_1    35rres_1_3       34arvalis_3_1    32arvalis_3_0    31rres_1_2       29ethz_1_3       28inrae_1_1      25arvalis_1_1    23arvalis_3_3    20usask_1_1      19arvalis_3_2    19usask_1_2      16rres_1_4       15arvalis_1_4    14arvalis_3_4     8rres_1_1        8inrae_1_0       6arvalis_2_0     5arvalis_1_5     5ethz_1_6        4inrae_1_2       4usask_1_3       3usask_1_0       2arvalis_3_5     2ethz_1_2        1arvalis_1_6     1arvalis_2_2     1rres_1_5        1Name: stratify_group, dtype: int64 ****************************** FOLD : 2arvalis_1_2    99ethz_1_4       73arvalis_1_3    69ethz_1_5       42rres_1_3       34arvalis_2_1    34arvalis_3_1    32arvalis_3_0    31rres_1_2       29ethz_1_3       28inrae_1_1      25arvalis_1_1    23arvalis_3_3    21usask_1_1      19arvalis_3_2    19usask_1_2      16rres_1_4       15arvalis_1_4    14arvalis_3_4     8rres_1_1        8inrae_1_0       6arvalis_2_0     5arvalis_1_5     5ethz_1_6        4inrae_1_2       4usask_1_0       3usask_1_3       3arvalis_3_5     2arvalis_2_2     2ethz_1_2        1arvalis_1_6     1Name: stratify_group, dtype: int64 ****************************** FOLD : 3arvalis_1_2    99ethz_1_4       73arvalis_1_3    69ethz_1_5       43rres_1_3       34arvalis_2_1    34arvalis_3_1    32arvalis_3_0    30rres_1_2       29ethz_1_3       28inrae_1_1      25arvalis_1_1    23arvalis_3_3    21usask_1_1      19arvalis_3_2    19usask_1_2      17rres_1_4       15arvalis_1_4    14arvalis_3_4     8rres_1_1        7inrae_1_0       6arvalis_2_0     5arvalis_1_5     5inrae_1_2       4ethz_1_6        3usask_1_3       3arvalis_2_2     2usask_1_0       2ethz_1_2        2arvalis_3_5     1arvalis_1_6     1ethz_1_7        1Name: stratify_group, dtype: int64 ****************************** FOLD : 4arvalis_1_2    99ethz_1_4       73arvalis_1_3    69ethz_1_5       43rres_1_3       35arvalis_2_1    34arvalis_3_1    32arvalis_3_0    31rres_1_2       29ethz_1_3       29inrae_1_1      26arvalis_1_1    23arvalis_3_3    21arvalis_3_2    19usask_1_1      18usask_1_2      16rres_1_4       15arvalis_1_4    14arvalis_3_4     8rres_1_1        8inrae_1_0       6arvalis_2_0     5arvalis_1_5     4ethz_1_6        3usask_1_3       3inrae_1_2       3usask_1_0       2ethz_1_2        2arvalis_3_5     1ethz_1_7        1arvalis_1_6     1arvalis_2_2     1Name: stratify_group, dtype: int64 데이터 로더 만들기class WheatDataset(Dataset):    def __init__(self, dataframe, image_dir, transforms=None):        super().__init__()        self.df = dataframe        self.image_dir = image_dir        self.transforms = transforms        self.image_ids = dataframe['image_id'].unique()            def __getitem__(self, index):        image_id = self.image_ids[index]                # bounding box        records = self.df[self.df['image_id'] == image_id]        boxes = records[['x', 'y', 'w', 'h']].values # values -&gt; dataframe to array                # w,h to xmax, ymax        boxes[:,2] = boxes[:,2] + boxes[:,0]        boxes[:,3] = boxes[:,3] + boxes[:,1]                # label        # 다중 분류가 아니기 때문에(wheat or background), 모델 학습이 돌아갈 수 있게끔 모양만 만들어 두자.        # 한 이미지에 대한 모든 bounding box의 개수를 가져온다.        labels = torch.ones(len(records), dtype = torch.int64)                # image        img = cv2.imread(self.image_dir + image_id + '.jpg', cv2.IMREAD_COLOR)        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)        img = img / 255.0                # 데이터를 dictionary 형태로 보관하자.        # faster rcnn model에 들어갈 때, target의 key값들은 아래와 일치하도록 해야 한다. 안그러면 KeyError 발생한다.        target = {'labels' : labels, 'boxes' : boxes}                # transforms        # img 뿐만 아니라 bounding box, label 모두 tensor 형태로 바꿔야 한다.        if self.transforms:            # image, bboxes라는 key값 자체가 있기 때문에 반드시 이름을 이와 같게 해야 한다.            sample = {'image' : img, 'bboxes' : boxes, 'labels' : labels}                        sample = self.transforms(**sample)            img = sample['image']                        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(sample['bboxes'])))).squeeze(1)                return img, target, image_id            def __len__(self):        return len(self.image_ids)Bounding Box를 Tensor로 만들 때 주의해야할 점  데이터로더의 __getitem__의 마지막 부분을 보면, boxes값들을 tensor로 만들어주는 코드가 있다.  밑의 예시를 보며 각각의 코드들이 어떤 역할을 하는지 이해할 필요가 있다.x = {'boxes' : [[1,2,3], [4,5,6], [7,8,9]]}x['boxes'][[1, 2, 3], [4, 5, 6], [7, 8, 9]]print(f"Using unpacking operator : {list(zip(*x['boxes']))}\nNot using unpacking operator : {list(zip(x['boxes']))}")Using unpacking operator : [(1, 4, 7), (2, 5, 8), (3, 6, 9)]Not using unpacking operator : [([1, 2, 3],), ([4, 5, 6],), ([7, 8, 9],)]  unpacking operator를 사용하면 값들의 순서가 바뀐 것을 볼 수 있다. 이렇게 바뀐 순서를 다시 원래대로 돌려주기 위해 permute함수가 사용된 것이다.using_permute = torch.stack(tuple(map(torch.tensor, zip(*x['boxes'])))).permute(1,0)print(f"Using permute : \n{using_permute}\nShape : {using_permute.shape}")Using permute : tensor([[1, 2, 3],        [4, 5, 6],        [7, 8, 9]])Shape : torch.Size([3, 3])  코드를 더 간단하게 만들기 위해 squeeze를 사용해도 괜찮다.using_squeeze = torch.stack(tuple(map(torch.tensor, zip(x['boxes'])))).squeeze(1)print(f'Using squeeze : \n{using_squeeze}\nShape : {using_squeeze.shape}')Using squeeze : tensor([[1, 2, 3],        [4, 5, 6],        [7, 8, 9]])Shape : torch.Size([3, 3])transforms 함수 만들기  format = pascal_voc:어느 데이터셋에 대한 bbox 내용을 가져올 것인지  label_fields : 우리가 만든 labels  albumentations 참고 : https://albumentations.ai/docs/examples/example_bboxes/def get_train_transform():    return A.Compose([        A.Flip(0.5),        ToTensorV2()    ], bbox_params = A.BboxParams(format = 'pascal_voc', label_fields = ['labels'])    )def get_valid_transform():    return A.Compose([        ToTensorV2()    ], bbox_params = A.BboxParams(format = 'pascal_voc', label_fields = ['labels'])    )"Stack" 관련 에러 날 경우RuntimeError: stack expects each tensor to be equal size, but got [49, 4] at entry 0 and [26, 4] at entry 1  이러한 에러가 날 시, DataLoader 설정 시 collate_fn 옵션을 넣어줘야 한다.  특정 상황에서 batch별로 데이터가 잘 안묶여서 생기는 에러이므로, 이러한 오류가 나면 아래와 같은 함수를 설정하여 옵션에 넣어주자.def collate_fn(batch):    return tuple(zip(*batch))train_dataset = WheatDataset(train, args["TRAIN"], transforms = get_train_transform())train_loader = DataLoader(train_dataset, batch_size = args["BATCH_SIZE"], shuffle = True, collate_fn = collate_fn, num_workers = 4)학습하기모델 불러오기  모델은 fasterrcnn_resnet50_fpn을 사용한다.          fpn : feature pyramid network로, 기존 모델 구조에서 보다 발전된 형태이다. 참고 : https://eehoeskrap.tistory.com/300      Backbone은 resnet50을 사용한다.        모델의 구조를 보면, Dropout, BatchNormalization 등의 레이어 층을 볼 수 있다.          Dropout : 모델이 train 데이터셋을 통해 중요한 노드(요소)들을 집중적으로 학습하다 보니, test셋과 같은 새로운 데이터가 왔을 때 예측을 잘 하지 못하는 경우가 발생한다. train셋에 과적합 되었기 때문이다. 이를 개선하기 위해 Dropout옵션을 정해주는 것이다. 또한 매번 다른 노드들로 학습을 하다 보니, 앙상블의 효과도 얻을 수 있다.                  만약 Dropout(0.3)이면, 0.3만큼의 임의의 노드를 사용하지 않는다는 의미이다.          이 때 0이 되지 않은 0.7에 해당하는 값은 (1/0.7) 만큼 scale이 된다. 따라서 (1/0.7 = 1.4286…)이 되는 것이다.                    BatchNorm : 배치로 들어오는 데이터에 규제를 넣어주는 것이다. 학습 속도 및 학습 성능에 영향을 준다.      참고 블로그 : https://gaussian37.github.io/dl-pytorch-snippets/      model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)modelFasterRCNN(  (transform): GeneralizedRCNNTransform(      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])      Resize(min_size=(800,), max_size=1333, mode='bilinear')  )  (backbone): BackboneWithFPN(    (body): IntermediateLayerGetter(      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)      (bn1): FrozenBatchNorm2d(64)      (relu): ReLU(inplace=True)      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)      (layer1): Sequential(        (0): Bottleneck(          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn1): FrozenBatchNorm2d(64)          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn2): FrozenBatchNorm2d(64)          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn3): FrozenBatchNorm2d(256)          (relu): ReLU(inplace=True)          (downsample): Sequential(            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (1): FrozenBatchNorm2d(256)          )...        (fpn): FeaturePyramidNetwork(          (inner_blocks): ModuleList(            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))            (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))            (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))          )          (layer_blocks): ModuleList(            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))          )          (extra_blocks): LastLevelMaxPool()        )      )      (rpn): RegionProposalNetwork(        (anchor_generator): AnchorGenerator()        (head): RPNHead(          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))          (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))          (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))        )      )      (roi_heads): RoIHeads(        (box_roi_pool): MultiScaleRoIAlign()        (box_head): TwoMLPHead(          (fc6): Linear(in_features=12544, out_features=1024, bias=True)          (fc7): Linear(in_features=1024, out_features=1024, bias=True)        )        (box_predictor): FastRCNNPredictor(          (cls_score): Linear(in_features=1024, out_features=91, bias=True)          (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)        )      )    )모델 out_features 바꿔주기  모델 구조를 보면, 마지막 box_predictor를 보면 cls_score의 out_features가 91이다.  현재 우리는 wheat / background 라는 두 개의 class를 분류하는 문제를 갖고 있으므로, out_features=2로 바꿔줘야 한다.  bbox_pred 역시 class 개수에 따라 바꿔준다. 로 자동적으로 바뀐다.          torch.nn.Linear로 바꿀 시, cls_score, bbox_pred 모두 바꿔줘야 하고, torchvision.models.detection.faster_rcnn.FastRCNNPredictor로 접근하여 바꿀 시, bbox_pred는 cls_score*4 로 자동적으로 바뀐다.       방법 1 model.roi_heads.box_predictor = FastRCNNPredictor(1024,2)model.roi_heads.box_predictorFastRCNNPredictor(  (cls_score): Linear(in_features=1024, out_features=2, bias=True)  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)) 방법 2 model.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)model.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)model.roi_heads.box_predictorFastRCNNPredictor(  (cls_score): Linear(in_features=1024, out_features=2, bias=True)  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True))# model to GPUmodel.to(args["DEVICE"])FasterRCNN(  (transform): GeneralizedRCNNTransform(      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])      Resize(min_size=(800,), max_size=1333, mode='bilinear')  )  (backbone): BackboneWithFPN(    (body): IntermediateLayerGetter(      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)      (bn1): FrozenBatchNorm2d(64)      (relu): ReLU(inplace=True)      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)      (layer1): Sequential(        (0): Bottleneck(          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn1): FrozenBatchNorm2d(64)          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn2): FrozenBatchNorm2d(64)          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn3): FrozenBatchNorm2d(256)          (relu): ReLU(inplace=True)          (downsample): Sequential(            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (1): FrozenBatchNorm2d(256)          )...        (fpn): FeaturePyramidNetwork(          (inner_blocks): ModuleList(            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))            (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))            (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))          )          (layer_blocks): ModuleList(            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))          )          (extra_blocks): LastLevelMaxPool()        )      )      (rpn): RegionProposalNetwork(        (anchor_generator): AnchorGenerator()        (head): RPNHead(          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))          (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))          (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))        )      )      (roi_heads): RoIHeads(        (box_roi_pool): MultiScaleRoIAlign()        (box_head): TwoMLPHead(          (fc6): Linear(in_features=12544, out_features=1024, bias=True)          (fc7): Linear(in_features=1024, out_features=1024, bias=True)        )        (box_predictor): FastRCNNPredictor(          (cls_score): Linear(in_features=1024, out_features=2, bias=True)          (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)        )      )    )업데이트할 Parameters 설정  model의 parameters 중, requires_grad=True, 즉 freeze가 안된 파라미터들을 가져와서 학습하겠다는 의미이다.  Epoch를 돌며 경사하강법으로 학습할 파라미터이다.params = [x for x in model.parameters() if x.requires_grad]params[-5:][Parameter containing: tensor([ 0.0086,  0.0309,  0.0581,  ..., -0.0181,  0.0325,  0.0189],        device='cuda:0', requires_grad=True), Parameter containing: tensor([[-7.6312e-03,  2.0107e-02,  1.4423e-02,  ...,  6.0976e-05,           3.0176e-02,  2.4542e-02],         [ 1.1975e-02,  3.7853e-03,  1.3403e-02,  ..., -2.7104e-02,          -2.0492e-02, -1.9000e-02]], device='cuda:0', requires_grad=True), Parameter containing: tensor([0.0195, 0.0138], device='cuda:0', requires_grad=True), Parameter containing: tensor([[-1.7756e-02,  1.4171e-03,  1.5141e-02,  ...,  2.7508e-02,          -1.1840e-02, -1.3901e-02],         [-1.7905e-02, -1.7732e-02, -1.7152e-02,  ...,  7.7827e-03,           1.2814e-02, -1.0298e-02],         [-2.6198e-02,  2.3477e-05, -2.5067e-03,  ..., -2.8354e-02,          -6.5023e-03,  1.1452e-02],         ...,         [-2.0409e-02,  1.9236e-02,  6.2685e-03,  ...,  2.4985e-02,          -2.8892e-02, -2.0194e-03],         [-5.7999e-03, -1.4193e-02,  8.6269e-03,  ..., -2.1297e-02,           1.7533e-03, -1.9947e-02],         [-2.9891e-03, -2.0081e-02,  2.4276e-03,  ...,  1.5759e-02,          -1.5163e-02,  2.7659e-02]], device='cuda:0', requires_grad=True), Parameter containing: tensor([-0.0081,  0.0307,  0.0157, -0.0003, -0.0152, -0.0005,  0.0208, -0.0266],        device='cuda:0', requires_grad=True)]Optimizer 설정  AdamW 등 Detection 문제에서 평균적으로 더 좋은 성능을 보이는 옵티마이저가 있지만, 지금은 Adam을 사용해보자.optimizer = torch.optim.Adam(params = params, lr=0.001)Averager 클래스 설정  Averager는 loss 값을 계산할 때 사용한다.  Faster Rcnn 모델에서 내부적으로 손실값을 계산할때 사용하는 class로, 이미 구현이 되어있다.  모델마다 이러한 Averager들이 조금씩 다를 수 있으므로, 다른 모델을 사용할 때 검색해서 한번 찾아보자.class Averager:    def __init__(self):        self.current_total = 0.0        self.iterations = 0.0    def send(self, value):        self.current_total += value        self.iterations += 1    @property    def value(self):        if self.iterations == 0:            return 0        else:            return 1.0 * self.current_total / self.iterations # 평균값    def reset(self): # 초기화        self.current_total = 0.0        self.iterations = 0.0        loss_hist = Averager() Validation 함수  pytorch의 faster-rcnn 모델을 사용할 시, model.eval()은 loss가 아닌 예측 바운딩박스 좌표값들을 돌려준다. 우선은 with torch.no_grad()만 적용하여 validation loss를 계산해보자.참고 : https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn# validation functiondef validation(model, valid_loader, args):    valid_loss = 0    for images, targets, image_ids in valid_loader:        if torch.cuda.is_available():            images = list(x.to(args["DEVICE"]) for x in images)            targets = [{k:v.to(args["DEVICE"]) for k,v in t.items()} for t in targets]        val_loss_dict = model(images, targets)        val_losses = sum(x for x in val_loss_dict.values())        valid_loss += val_losses.item()    return valid_loss / len(valid_loader)모델 학습하기  모델은 5번의 교차검증으로 훈련한다.단일 모델 훈련 코드def train_model(model, train_loader, valid_loader, optimizer, args):    steps = 0    total_step = len(train_loader)    iterations = 1    for epoch in range(args["NUM_EPOCHS"]):        loss_hist.reset()        for images, targets, image_ids in train_loader:            # Data to GPU            images = list(x.to(args["DEVICE"]) for x in images)            targets = [{k:v.to(args["DEVICE"]) for k,v in t.items()} for t in targets]            steps += 1            # Label classification, BoundingBox Regression의 Loss 값이 Ditcionary 형태로 나온다.            loss_dict = model(images, targets)            # Dict 형태의 loss_dict에서 .values()로 key:values에서 values를 가져온다.            losses = sum(x for x in loss_dict.values())            # losses를 print해보면 tensor()로 감싸져 있다. .item()으로 숫자만 가져오자.            loss_value = losses.item()            # loss_hist에 loss_value를 저장한다.            loss_hist.send(loss_value)            # optimizer 업데이트 시 항상 나오는 삼총사            # 1. 기울기 0으로 초기화            optimizer.zero_grad()            # 2. 역전파            losses.backward()            # 3. 업데이트            optimizer.step()            # 10번마다 loss값 출력            if iterations % 100 == 0:                print(f"{iterations} iterations ... Loss : {loss_value}")            iterations +=1                        if steps % total_step == 0:                with torch.no_grad():                    valid_loss = validation(model, valid_loader, args)                                 print("Epoch: {}/{}.. ".format(epoch+1,args['NUM_EPOCHS']),                     "Training Loss: {:.6f}..".format(loss_hist.value),                     "Valid Loss: {:.6f}..".format(valid_loss))                                step = 0    return교차검증 훈련  5번의 교차 검증을 진행한다. 아래 출력창을 보면 로스값이 떨어지는 것을 확인할 수 있다.train_fold = pd.merge(train, df_folds, on='image_id', how='left')def train_folds(dataframe, args):    for fold in range(args["NUM_FOLDS"]):        # Model &amp; Optimizer        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)        model.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)        model.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)        model.to(args["DEVICE"])        params = [x for x in model.parameters() if x.requires_grad]        optimizer = torch.optim.Adam(params = params, lr=0.001)        train_ = train_fold[train_fold["fold"]!=fold].reset_index(drop=True)        valid_ = train_fold[train_fold["fold"]==fold].reset_index(drop=True)        # Train Loader &amp; Valid Loader        train_dataset = WheatDataset(train_, args["TRAIN"], transforms = get_train_transform())        train_loader = DataLoader(train_dataset,                                  batch_size = args["BATCH_SIZE"],                                   shuffle = True,                                  collate_fn = collate_fn,                                  num_workers = 4)        valid_dataset = WheatDataset(valid_, args["TRAIN"], transforms = get_valid_transform())        valid_loader = DataLoader(valid_dataset,                                  batch_size = 1,                                   shuffle = False,                                  collate_fn = collate_fn,                                  num_workers = 4)        print(f"\n{fold+1}/{args['NUM_FOLDS']} Cross Validation Training Starts ...\n")        train_model(model, train_loader, valid_loader, optimizer, args)        print(f"\n{fold+1}/{args['NUM_FOLDS']} Cross Validation Training Ends ...\n")        torch.save(model.state_dict(), f"fold{fold}_model.pth")    returntrain_folds(train_fold, args)1/5 Cross Validation Training Starts ...100 iterations ... Loss : 1.7992744816770365200 iterations ... Loss : 1.3215401355936756300 iterations ... Loss : 1.2728456529162266Epoch: 1/20..  Training Loss: 1.705802.. Valid Loss: 1.148290..400 iterations ... Loss : 1.1416268536681002500 iterations ... Loss : 0.9909895147586031600 iterations ... Loss : 1.0571585424735501Epoch: 2/20..  Training Loss: 1.031144.. Valid Loss: 1.045934..700 iterations ... Loss : 1.0805089997351918800 iterations ... Loss : 0.9486090712623582900 iterations ... Loss : 0.81593776950126061000 iterations ... Loss : 0.9475474243279667Epoch: 3/20..  Training Loss: 0.931789.. Valid Loss: 0.931303..1100 iterations ... Loss : 0.9034360086947431200 iterations ... Loss : 0.87690056987687071300 iterations ... Loss : 0.7356736096027937Epoch: 4/20..  Training Loss: 0.882577.. Valid Loss: 0.892810..1400 iterations ... Loss : 0.8412896578547771500 iterations ... Loss : 0.8285454769860281600 iterations ... Loss : 0.8009961004305629모델 저장하기  모델을 저장하는 방법은 여러 가지이다.                            모든 epoch마다 모델을 저정하는 방법                                      EarlyStopping을 사용하여 최소 loss 값을 보이는 모델만 저장하는 방법                      사용할 방법을 코드로 구현해서 적용하면 된다.마무리  Object Detection - 이진 분류 대회에 대한 기본적인 Base line 코드를 공부해봤다.  교차검증을 위해 Train, Valid 셋을 나눌 때, stratify_group을 만들었는데, 이는 최대한 데이터 분포를 고르게 하여 특정 fold에서 모델이 과적합 되지 않게 하기 위함이었다.  모델 성능을 더 높이기 위한 여러 방법들을(하이퍼 파라미터 조정, 데이터 전처리, 모델 바꿔서 학습해보기 등) 더 공부해보고 적용해보자.      저장한 모델로 test 셋 예측 및 제출하기까지의 방법은 Weighted Boxes Fusion 글을 참고하면 된다.    Test데이터 추론에는 Pre-trained faster-rcnn 모델을 5번의 교차검증으로 훈련한 후, weighted boxes fusion방법을 적용했다.  리더보드 점수는 private : 0.5870, public : 0.6735          참고 : 리더보드상 1위 private : 0.6897, 1위 public : 0.7746      참고 자료  FasterRCNN 학습 코드 참고 : https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train  albumentations 참고 : https://albumentations.ai/docs/examples/example_bboxes/  Dropout, BatchNorm 참고 : https://gaussian37.github.io/dl-pytorch-snippets/  StratifyKFold 참고 : https://www.kaggle.com/shonenkov/wbf-approach-for-ensemble  Pytorch FasterRCNN Validation 참고 : https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn]]></content>
      <categories>
        
          <category> Detection </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Pytorch </tag>
        
          <tag> Detection </tag>
        
          <tag> Faster RCNN </tag>
        
          <tag> StratifyKFold </tag>
        
          <tag> Ensemble </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[TGS Salt Identification Challenge(Segmentation)]]></title>
      <url>/segmentation/2021/02/15/tgs_salt/</url>
      <content type="text"><![CDATA[대회 소개지표면의 소금 영역 검출하기TGS Salt Identification ChallengeSegment salt deposits beneath the Earth’s surfacehttps://www.kaggle.com/c/tgs-salt-identification-challenge/overviewDescriptionSeveral areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface.But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.To create the most accurate seismic images and 3D renderings, TGS (the world’s leading geoscience data company) is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.패키기 불러오기import numpy as np import pandas as pd import zipfileimport globfrom PIL import Imageimport matplotlib.pyplot as pltimport seaborn as snsimport cv2import randomfrom tqdm import tqdm_notebookfrom tensorflow.keras.preprocessing.image import load_img, img_to_arrayfrom skimage.transform import resizefrom keras.layers import Conv2D, MaxPooling2D, Input, concatenate, Conv2DTransposefrom keras import *from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateauimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))/kaggle/input/tgs-salt-identification-challenge/depths.csv/kaggle/input/tgs-salt-identification-challenge/sample_submission.csv/kaggle/input/tgs-salt-identification-challenge/train.zip/kaggle/input/tgs-salt-identification-challenge/competition_data.zip/kaggle/input/tgs-salt-identification-challenge/test.zip/kaggle/input/tgs-salt-identification-challenge/train.csv/kaggle/input/tgs-salt-identification-challenge/flamingo.zip데이터 불러오기with zipfile.ZipFile('../input/tgs-salt-identification-challenge/train.zip', 'r') as z :     z.extractall('train')    with zipfile.ZipFile('../input/tgs-salt-identification-challenge/test.zip', 'r') as z :     z.extractall('test')train = pd.read_csv('../input/tgs-salt-identification-challenge/train.csv')depth = pd.read_csv('../input/tgs-salt-identification-challenge/depths.csv')train                  id      rle_mask                  0      575d24d81d      NaN              1      a266a2a9df      5051 5151              2      75efad62c1      9 93 109 94 210 94 310 95 411 95 511 96 612 96...              3      34e51dba6a      48 54 149 54 251 53 353 52 455 51 557 50 659 4...              4      4875705fb0      1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...              ...      ...      ...              3995      9cbd5ddba4      NaN              3996      caa039b231      2398 7 2499 11 2600 16 2700 22 2801 26 2901 29...              3997      1306fcee4c      NaN              3998      48d81e93d9      2828 1 2927 3 3026 5 3126 6 3225 8 3324 10 342...              3999      edf1e6ac00      NaN      4000 rows × 2 columnsimages= glob.glob('./train/images/*')masks= glob.glob('./train/masks/*')Image.open(images[2]).resize((256,256))이미지(원본 및 마스크) 확인하기images_df = pd.DataFrame({'images' : images,                         'masks' : masks})images_df                  images      masks                  0      ./train/images/46f402dcd5.png      ./train/masks/46f402dcd5.png              1      ./train/images/a56e87840f.png      ./train/masks/a56e87840f.png              2      ./train/images/2af3c055b2.png      ./train/masks/2af3c055b2.png              3      ./train/images/7c61d788af.png      ./train/masks/7c61d788af.png              4      ./train/images/ef654c1b73.png      ./train/masks/ef654c1b73.png              ...      ...      ...              3995      ./train/images/14152a6731.png      ./train/masks/14152a6731.png              3996      ./train/images/418b7878a8.png      ./train/masks/418b7878a8.png              3997      ./train/images/106e4043ba.png      ./train/masks/106e4043ba.png              3998      ./train/images/bce104494c.png      ./train/masks/bce104494c.png              3999      ./train/images/27a240a570.png      ./train/masks/27a240a570.png      4000 rows × 2 columnsfig , axes = plt.subplots(2,5)fig.set_size_inches(15,8)for index in range(0,5):    idx = random.randrange(len(images_df))    img = Image.fromarray(cv2.imread(images_df.iloc[idx,0], cv2.IMREAD_COLOR)).resize((256,256))    mask = Image.fromarray(cv2.imread(images_df.iloc[idx,1], cv2.IMREAD_COLOR)).resize((256,256))    axes[index//5 , index%5].imshow(img)    axes[index//5 , index%5].set_title(f'Image {index}')    axes[index//5,  index%5].axis('off')        axes[index//5+1 , index%5].imshow(mask)    axes[index//5+1 , index%5].set_title(f'Mask {index}')    axes[index//5+1,  index%5].axis('off')    데이터 전처리  segmentation 대회의 정답값은 예전 classification 대회처럼 클라스 값이 아니라 이미지이다.  Input image에서 소금인 영역을 찾아내야 한다.      따라서 예전처럼 ImageDataGenerator로 데이터를 처리하기 까다롭기 때문에 직접 데이터셋을 customizing 해보자.    segmentation 에서는 UNet 모델을 가장 많이 사용한다.          모델을 직접 쌓아보면 알겠지만 이미지의 사이즈가 2의 거듭제곱 형태이면 편하다.      U 형태로 feature extraction이 이루어지고, 밑으로 갈수록 이미지의 크기는 작아지지만 채널 수가 늘어나는 특징이 있다.      이진분류, 다중분류 등에 사용된다.      Train Dataset 전처리하기      대부분 ImageDataGenerator로 이미지 전처리가 가능하지만, 몇몇 대회에선 이미지 형태에 따라 전처리를 직접 해야하는 경우가 있다. 어떤한 데이터 형태든 다룰 수 있도록 직접 전처리 하는 방법들을 익혀두자.        먼저 데이터를 담을 빈 그릇을 만든다.          빈 그릇을 만들 때 너비, 높이 뿐만 아니라 채널을 잊지 말고 설정해주자.      이미지의 RGB 픽셀 값들을 보면 모두 같음을 알수 있다. 학습 속도 향상을 위해 하나의 채널만 사용하자.      image 데이터 타입은 가장 효율적으로 정보를 담을 수 있는 dtype = np.uint8으로 설정한다.                  양수만 표현 가능하다.          2^8 개수 만큼 표현 가능하다.(0 ~ 255)                    y 데이터 타입은 dtype = np.bool로 설정한다.      # All the values are samenp.array(Image.open(images[0]))[:,:,0]np.array(Image.open(images[0]))[:,:,1]np.array(Image.open(images[0]))[:,:,2]array([[206, 193, 193, ..., 121, 134, 134],       [222, 207, 208, ..., 116, 137, 106],       [221, 201, 199, ..., 125, 146,  89],       ...,       [106,  61,  35, ...,  68,  88,  90],       [ 86,  70,  61, ...,  85, 112,  91],       [ 85,  94,  89, ..., 110, 125,  95]], dtype=uint8)빈 그릇 만들기train = np.zeros((4000,128,128,1), dtype = np.uint8)y = np.zeros((4000,128,128,1), dtype = np.bool)이미지 데이터 담기for i,j in tqdm_notebook(enumerate(images), total=4000):    image = load_img(j)    image = img_to_array(image)[:,:,0] # 한 채널값만 사용하기    image = resize(image,(128,128,1))  # (101,101,1) -&gt; (128,128,1)    train[i] = imageMask 데이터 담기  Mask 데이터는 True, False 형태로 픽셀값이 채워져야 한다.for i,j in tqdm_notebook(enumerate(masks), total = 4000):    mask = load_img(j)    mask = img_to_array(mask)[:,:,0]    y[i] = resize(mask, (128,128,1))Train, Valid 나누기  segmentation은 회귀 문제의 느낌이 있다.  이미지마다 소금이 들어있는 정도가 다를 수 있으므로, 회귀에서 y의 비율을 맞추려면 어떻게 해야할까.          소금의 비율 구간을 나눈다.      구간별로 추출하면 안정적인 평가 셋이 될 것이다.      지금은 우선 train_test_split으로 일괄적으로 나누자.      from sklearn.model_selection import train_test_splitX_train, X_valid, y_train, y_valid = train_test_split(train,y, test_size = 0.1, random_state = 42) U-net 모델 구축하기  Classification 문제에선 주로 model = Sequential() 을 사용한 비교적 단순한 모델 구조를 쌓았다.      함수를 이용하여 층들을 병렬로 쌓을 수 있는 방법을 적용하자. Functional API 라고도 부른다.    U-net 모델 구조의 특징          출력값의 크기는 Input의 크기와 같다.                  이미지 분류 문제에서는 이미지의 크기가 작아지다가 Dense층을 거쳐 확률값으로 나가는 형태인데, Segmentation 문제에서는 input과 output이 똑같은 크기를 갖는다.                    각각의 픽셀값이 [0,1] 사이로 출력 되어야 한다.      가장 단순한 U-net 구조  UNet에서는 padding 옵션 넣어 이미지의 크기가 줄어드는 것을 방지한다. 기본값인 valid이면 이미지 크기가 작아진다.  0~1 사이로 값이 나가야하므로 출력층에서 sigmoid를 적용한다.  모델을 선언할 때 input, output을 명시해준다.inp = Input((128,128,1))cnn1 = Conv2D(16, (3,3), activation = 'relu', padding='same')(inp)out = Conv2D(1, (1,1), activation = 'sigmoid')(cnn1)model = Model(inp, out) model.summary()Model: "functional_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         [(None, 128, 128, 1)]     0         _________________________________________________________________conv2d (Conv2D)              (None, 128, 128, 16)      160       _________________________________________________________________conv2d_1 (Conv2D)            (None, 128, 128, 1)       17        =================================================================Total params: 177Trainable params: 177Non-trainable params: 0_________________________________________________________________U-net 구조 응용하기      Conv2DTranspose() 층은 줄어든 이미지를 다시 키우기 위한 연산 작업이다. 비선형 학습이 이루어지지 않기 때문에 activation함수를 설정해주지 않는다.    concatenate 를 통해 사이즈가 같은 층끼리 합쳐준다. 기본적으로 axis=-1로 설정되어 있어 마지막 축을 기준으로 합쳐진다.  아직은 U-net 구조를 깊게 쌓지 않아서 모델의 성능이 높게 나오진 않는다. 다른 하이퍼파라미터 최적화를 끝낸 후, 모델을 더 깊게 쌓아보자.inp = Input((128,128,1))cnn0 = Conv2D(8, (3,3), activation = 'relu', padding='same')(inp)cnn0 = Conv2D(8, (3,3), activation = 'relu', padding='same')(cnn0)pool0 = MaxPooling2D()(cnn0) cnn1 = Conv2D(16, (3,3), activation = 'relu', padding='same')(pool0)cnn1 = Conv2D(16, (3,3), activation = 'relu', padding='same')(cnn1)pool1 = MaxPooling2D()(cnn1) cnn2 = Conv2D(32, (3,3), activation = 'relu', padding='same')(pool1)cnn2 = Conv2D(32, (3,3), activation = 'relu', padding='same')(cnn2)pool2 = MaxPooling2D()(cnn2)cnn3 = Conv2D(64, (3,3), activation = 'relu', padding='same')(pool2)cnn3 = Conv2D(64, (3,3), activation = 'relu', padding='same')(cnn3)pool3 = MaxPooling2D()(cnn3)cnn_m = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool3)# 이미지 크기 키우기ct3 = Conv2DTranspose(64, (3,3), padding='same', strides = (2,2))(cnn_m)cc3 = concatenate([ct3, cnn3])cc3_up = Conv2D(64, (3,3), activation = 'relu', padding='same')(cc3)cc3_up = Conv2D(64, (3,3), activation = 'relu', padding='same')(cc3_up)ct2 = Conv2DTranspose(32, (3,3),padding='same', strides = (2,2))(cc3_up) cc2 = concatenate([ct2, cnn2])cc2_up = Conv2D(32, (3,3),activation = 'relu', padding='same')(cc2) cc2_up = Conv2D(32, (3,3),activation = 'relu', padding='same')(cc2_up)ct1 = Conv2DTranspose(16, (3,3), padding='same', strides = (2,2))(cc2_up)cc1 = concatenate([ct1, cnn1]) cc1_up = Conv2D(16, (3,3), activation='relu', padding='same')(cc1) cc1_up = Conv2D(16, (3,3), activation='relu', padding='same')(cc1_up)ct0 = Conv2DTranspose(8, (3,3), padding='same', strides = (2,2))(cc1_up)cc0 = concatenate([ct0, cnn0]) cc0_up = Conv2D(8, (3,3), activation='relu', padding='same')(cc0) cc0_up = Conv2D(8, (3,3), activation='relu', padding='same')(cc0_up)out = Conv2D(1, (1,1), activation = 'sigmoid')(cc0_up) model = Model(inp, out)model.summary()# val_loss = 0.1921 Model: "functional_5"__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_3 (InputLayer)            [(None, 128, 128, 1) 0                                            __________________________________________________________________________________________________conv2d_20 (Conv2D)              (None, 128, 128, 8)  80          input_3[0][0]                    __________________________________________________________________________________________________conv2d_21 (Conv2D)              (None, 128, 128, 8)  584         conv2d_20[0][0]                  __________________________________________________________________________________________________max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 8)    0           conv2d_21[0][0]                  __________________________________________________________________________________________________conv2d_22 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_4[0][0]            __________________________________________________________________________________________________conv2d_23 (Conv2D)              (None, 64, 64, 16)   2320        conv2d_22[0][0]                  __________________________________________________________________________________________________max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 16)   0           conv2d_23[0][0]                  __________________________________________________________________________________________________conv2d_24 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_5[0][0]            __________________________________________________________________________________________________conv2d_25 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_24[0][0]                  __________________________________________________________________________________________________max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 32)   0           conv2d_25[0][0]                  __________________________________________________________________________________________________conv2d_26 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_6[0][0]            __________________________________________________________________________________________________conv2d_27 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_26[0][0]                  __________________________________________________________________________________________________max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_27[0][0]                  __________________________________________________________________________________________________conv2d_28 (Conv2D)              (None, 8, 8, 128)    73856       max_pooling2d_7[0][0]            __________________________________________________________________________________________________conv2d_transpose_4 (Conv2DTrans (None, 16, 16, 64)   73792       conv2d_28[0][0]                  __________________________________________________________________________________________________concatenate_4 (Concatenate)     (None, 16, 16, 128)  0           conv2d_transpose_4[0][0]                                                                          conv2d_27[0][0]                  __________________________________________________________________________________________________conv2d_29 (Conv2D)              (None, 16, 16, 64)   73792       concatenate_4[0][0]              __________________________________________________________________________________________________conv2d_30 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_29[0][0]                  __________________________________________________________________________________________________conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 32)   18464       conv2d_30[0][0]                  __________________________________________________________________________________________________concatenate_5 (Concatenate)     (None, 32, 32, 64)   0           conv2d_transpose_5[0][0]                                                                          conv2d_25[0][0]                  __________________________________________________________________________________________________conv2d_31 (Conv2D)              (None, 32, 32, 32)   18464       concatenate_5[0][0]              __________________________________________________________________________________________________conv2d_32 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_31[0][0]                  __________________________________________________________________________________________________conv2d_transpose_6 (Conv2DTrans (None, 64, 64, 16)   4624        conv2d_32[0][0]                  __________________________________________________________________________________________________concatenate_6 (Concatenate)     (None, 64, 64, 32)   0           conv2d_transpose_6[0][0]                                                                          conv2d_23[0][0]                  __________________________________________________________________________________________________conv2d_33 (Conv2D)              (None, 64, 64, 16)   4624        concatenate_6[0][0]              __________________________________________________________________________________________________conv2d_34 (Conv2D)              (None, 64, 64, 16)   2320        conv2d_33[0][0]                  __________________________________________________________________________________________________conv2d_transpose_7 (Conv2DTrans (None, 128, 128, 8)  1160        conv2d_34[0][0]                  __________________________________________________________________________________________________concatenate_7 (Concatenate)     (None, 128, 128, 16) 0           conv2d_transpose_7[0][0]                                                                          conv2d_21[0][0]                  __________________________________________________________________________________________________conv2d_35 (Conv2D)              (None, 128, 128, 8)  1160        concatenate_7[0][0]              __________________________________________________________________________________________________conv2d_36 (Conv2D)              (None, 128, 128, 8)  584         conv2d_35[0][0]                  __________________________________________________________________________________________________conv2d_37 (Conv2D)              (None, 128, 128, 1)  9           conv2d_36[0][0]                  ==================================================================================================Total params: 392,489Trainable params: 392,489Non-trainable params: 0__________________________________________________________________________________________________checkpoint = ModelCheckpoint('best.h5',save_best_only=True,verbose = True)early_stop = EarlyStopping(patience=10, verbose=True)reduce_lr = ReduceLROnPlateau(patience=6, verbose=True)model.compile(metrics = ['acc'], optimizer = 'adam', loss = 'binary_crossentropy')model.fit(X_train,y_train, validation_data = (X_valid,y_valid), epochs = 100, callbacks=[checkpoint, early_stop, reduce_lr])model.load_weights('best.h5')Epoch 1/5112/113 [============================&gt;.] - ETA: 0s - loss: 0.1644 - acc: 0.9396Epoch 00001: val_loss improved from 0.23510 to 0.22475, saving model to best.h5113/113 [==============================] - 4s 31ms/step - loss: 0.1641 - acc: 0.9398 - val_loss: 0.2247 - val_acc: 0.9083Epoch 2/5112/113 [============================&gt;.] - ETA: 0s - loss: 0.1596 - acc: 0.9423Epoch 00002: val_loss did not improve from 0.22475113/113 [==============================] - 3s 27ms/step - loss: 0.1594 - acc: 0.9424 - val_loss: 0.2567 - val_acc: 0.9115Epoch 3/5111/113 [============================&gt;.] - ETA: 0s - loss: 0.1501 - acc: 0.9450Epoch 00003: val_loss did not improve from 0.22475113/113 [==============================] - 3s 27ms/step - loss: 0.1493 - acc: 0.9452 - val_loss: 0.2593 - val_acc: 0.9141Epoch 4/5112/113 [============================&gt;.] - ETA: 0s - loss: 0.1490 - acc: 0.9443Epoch 00004: val_loss did not improve from 0.22475113/113 [==============================] - 3s 27ms/step - loss: 0.1491 - acc: 0.9443 - val_loss: 0.2426 - val_acc: 0.9143Epoch 5/5113/113 [==============================] - ETA: 0s - loss: 0.1384 - acc: 0.9486Epoch 00005: val_loss did not improve from 0.22475113/113 [==============================] - 3s 27ms/step - loss: 0.1384 - acc: 0.9486 - val_loss: 0.2346 - val_acc: 0.9168Validation Data를 Mean IOU로 평가하기  대회 점수를 올리기 위해 다음의 과정을 거치게 된다.          (1) 현재의 모델을 최적화하기      (2) 모델의 구조를 복잡하게 하기            먼저 주어진 모델을 최적화한 다음 모델 구조를 복잡하게 해야 최소한의 노력, 시간으로 효율적으로 모델 성능을 높일 수 있다.    Test 데이터셋을 예측하기 전, 이 대회의 평가 지표인 Mean IOU로 모델의 성능을 최적화 해보자.Validation Dataset 크기 조정하기result_valid = model.predict(X_valid, verbose=True)print(result_valid.shape,y_valid.shape)13/13 [==============================] - 0s 6ms/step(400, 128, 128, 1) (400, 128, 128, 1)  Mean IOU 채점 방식 역시 101x101 기준으로 이루어지기 때문에, 이와 같은 조건으로 설정해야 한다.  result_valid 및 y_valid의 크기를 바꿔주자.result_valid = np.squeeze(result_valid, axis = -1)y_valid = np.squeeze(y_valid, axis = -1)print(result_valid.shape, y_valid.shape)(400, 128, 128) (400, 128, 128)result_valid = np.array([resize(x,(101,101)) for x in result_valid])y_valid = np.array([resize(x,(101,101)) for x in y_valid])print(result_valid.shape, y_valid.shape)(400, 101, 101) (400, 101, 101)Mean IOU 계산하기      validation dataset을 이용해 어떤 threshold 값이 높은 mean IOU 값을 갖는지 확인하자. 적절한 threshold를 찾아 Test Dataset 예측에 적용한다.    threshold 값을 높게 잡으면 보수적으로 예측하게 되어 실제 환자를 건강하다고 예측하는 경우가 발생한다.      따라서 의료계에서는 보통 threshold을 낮게 잡는다.    np.linspace(0,1,num) 함수를 이용하여 0~1 사이를 num개의 간격으로 나누자.# mean IOU 함수 정의def iou_metric(y_true_in, y_pred_in, print_table=False):    labels = y_true_in # 실제 정답값    y_pred = y_pred_in # 예측값        true_objects = 2 # 소금 / not 소금      pred_objects = 2    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0] # 정답값과 비교    # Compute areas (needed for finding the union between all objects)    area_true = np.histogram(labels, bins = true_objects)[0]    area_pred = np.histogram(y_pred, bins = pred_objects)[0]    area_true = np.expand_dims(area_true, -1)    area_pred = np.expand_dims(area_pred, 0)    # Compute union    union = area_true + area_pred - intersection    # Exclude background from the analysis    intersection = intersection[1:,1:]    union = union[1:,1:]    union[union == 0] = 1e-9    # Compute the intersection over union    iou = intersection / union    # Precision helper function    def precision_at(threshold, iou):        matches = iou &gt; threshold        true_positives = np.sum(matches, axis=1) == 1   # Correct objects        false_positives = np.sum(matches, axis=0) == 0  # Missed objects        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)        return tp, fp, fn    # Loop over IoU thresholds    prec = []    if print_table:        print("Thresh\tTP\tFP\tFN\tPrec.")    for t in np.arange(0.5, 1.0, 0.05):        tp, fp, fn = precision_at(t, iou)        if (tp + fp + fn) &gt; 0:            p = tp / (tp + fp + fn)        else:            p = 0        if print_table:            print("{:1.3f}\t{}\t{}\t{}\t{:1.3f}".format(t, tp, fp, fn, p))        prec.append(p)        if print_table:        print("AP\t-\t-\t-\t{:1.3f}".format(np.mean(prec)))    return np.mean(prec)def iou_metric_batch(y_true_in, y_pred_in): # 0 또는 1로 들어감    batch_size = y_true_in.shape[0]    metric = []    for batch in range(batch_size):        value = iou_metric(y_true_in[batch], y_pred_in[batch])        metric.append(value)    return np.mean(metric)thresholds = np.linspace(0,1,50)thresholdsarray([0.        , 0.02040816, 0.04081633, 0.06122449, 0.08163265,       0.10204082, 0.12244898, 0.14285714, 0.16326531, 0.18367347,       0.20408163, 0.2244898 , 0.24489796, 0.26530612, 0.28571429,       0.30612245, 0.32653061, 0.34693878, 0.36734694, 0.3877551 ,       0.40816327, 0.42857143, 0.44897959, 0.46938776, 0.48979592,       0.51020408, 0.53061224, 0.55102041, 0.57142857, 0.59183673,       0.6122449 , 0.63265306, 0.65306122, 0.67346939, 0.69387755,       0.71428571, 0.73469388, 0.75510204, 0.7755102 , 0.79591837,       0.81632653, 0.83673469, 0.85714286, 0.87755102, 0.89795918,       0.91836735, 0.93877551, 0.95918367, 0.97959184, 1.        ])ious = np.array([iou_metric_batch(y_valid,np.round(result_valid &gt; x)) for x in tqdm_notebook(thresholds)])iousarray([0.515  , 0.40925, 0.414  , 0.421  , 0.43075, 0.41725, 0.4325 ,       0.44375, 0.47325, 0.48925, 0.50425, 0.5195 , 0.5385 , 0.54375,       0.55425, 0.56025, 0.58175, 0.596  , 0.60225, 0.62125, 0.62525,       0.6425 , 0.644  , 0.64875, 0.65225, 0.6535 , 0.6555 , 0.66   ,       0.667  , 0.66575, 0.66425, 0.67575, 0.679  , 0.68125, 0.67875,       0.67575, 0.67825, 0.67675, 0.6765 , 0.67675, 0.6725 , 0.6655 ,       0.6605 , 0.662  , 0.65125, 0.64975, 0.62825, 0.5985 , 0.567  ,       0.52125])최적의 threshold 값 찾기  threshold 별로 계산된 IOU 값을 시각화해보자.  최적의 threshold 값을 threshold_best에 저장하여 test data 예측에 사용한다.  실제 제출 점수와 최적의 IOU 값이 비슷해야 모델이 안정적으로 학습된 것이다.threshold_best_index = np.argmax(ious)threshold_best_index33iou_best = ious[threshold_best_index]iou_best0.68125threshold_best = thresholds[threshold_best_index]threshold_best0.673469387755102plt.plot(thresholds, ious)plt.plot(threshold_best, iou_best, 'xr', label = 'best threshold')plt.legend()plt.xlabel('threshold')plt.ylabel('iou')Text(0, 0.5, 'iou')Test data 예측하기  Train data를 구축했던 것처럼 Test data 역시 같은 방식으로 전처리 해주자.  model.predict() 후 원래 이미지 크기인 101x101로 resize 해줘야 한다.  또한 np.squeeze()를 사용하여 마지막 차원을 제거해준다. 이미지의 차원도 다시 맞춰줘야 한다.test_image = glob.glob('./test/*/*')test = np.zeros((len(test_image),128,128,1),dtype = np.uint8 )for i,j in tqdm_notebook(enumerate(test_image), total=len(test_image)):    img = load_img(j)    img = img_to_array(img)[:,:,0]    img = resize(img, (128,128,1))    test[i] = imgresult = model.predict(test)print(result.shape)result(18000, 128, 128, 1)array([[[[2.60078728e-01],         [3.29250753e-01],         [5.30278683e-01],         ...,         [1.12744577e-01],         [3.22315812e-01],         [8.85543376e-02]],        [[4.51777399e-01],         [6.17203057e-01],         [6.75710499e-01],         ...,        [[1.50929749e-01],         [2.73675825e-02],         [6.20411597e-02],         ...,         [1.17766663e-01],         [2.22633988e-01],         [1.56619608e-01]]]], dtype=float32)from tqdm import tnrangeresult_list = []for i in tnrange(len(result)):    resized_img = resize(np.squeeze(result[i]),(101,101))     result_list.append(resized_img)Submission 완료하기      segmentation 대회에서는 보통 각 픽셀별로 확률값 자체를 제출하지 않는다. 지금 이 대회만 보더라도 이미지 하나에 101x101 = 10201개의 픽셀이 존자하고, 총 18000개의 테스트 이미지로 183618000 개의 확률값이 나오게 된다. 이 값들을 그대로 재출 시 엄청난 양의 연산이 채점에 필요하게 될 것이므로, kaggle 측에서는 보통 대회와는 조금 다른 제출 방식을 요구하고 있다.        run-length encoding 로 픽셀 값들을 바꿔준 뒤 제출을 해야 자신의 점수를 제대로 확인할 수 있다.          (1) 기본적으로 0.5보다 확률값이 높으면 1, 아니면 0으로 바꿔주자.                  0.5 역시 하이퍼파라미터로, 최적의 값을 찾아줘야 한다.                    (2) 픽셀 값들을 run-length encoding로 바꿔준다. 1의 시작점과 시작점부터 run-length의 값이 한 쌍이 된다.                  예를 들어, 000111001111 의 픽셀 값은 3 3 8 4로 표현된다.                     Run-Length Encoding 함수  run-length encoding 함수를 정의하고, 추출된 예측값을 dictionary 형태로 저장한다.  dictionary로 저장시, key값은 test image의 주소, value 값은 run-length 인코딩된 값이다.def rle_encode(im):    '''    im: numpy array, 1 - mask(소금), 0 - background(소금X)    Returns run length as string formated    '''    pixels = im.flatten(order = 'F', )                  # 101 X 101 Flatten in column-major (Fortran- style) order    pixels = np.concatenate([[0], pixels, [0]])         # 양 끝 zero padding    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1    runs[1::2] -= runs[::2]                             # [1::2] : index 1 부터 끝까지 두 칸 간격으로        return ' '.join(str(x) for x in runs)               # 띄어쓰기 기준으로 나눔result_dict = {x : rle_encode(np.round(result_list[i] &gt; threshold_best))  for i,x in enumerate(tqdm_notebook(test_image))}print(result_dict){'./test/images/ca0030c4ed.png': '46 7 103 1 106 1 149 6 204 1 207 1 249 6 324 6 352 1 424 2 3314 3 3416 1 4225 6 5793 4 5892 9 5992 11 6094 13 6196 8 6259 1 6298 1 6300 3 6505 7 6606 6', ...'./test/images/d809054e30.png': '6315 1 6416 1 6517 1 10099 1 10200 1', './test/images/4f7b28dcba.png': '8055 1 8155 3 8257 1'}최종 제출  기존의 제출 형식(칼럼 이름, 각 칼럼에 들어가는 값)을 확인하자.  Dictionary 형태로 만들어둔 result 값을 제출 형식에 맞게 변형한다.sub = pd.read_csv('/kaggle/input/tgs-salt-identification-challenge/sample_submission.csv')sub                  id      rle_mask                  0      155410d6fa      1 1              1      78b32781d1      1 1              2      63db2a476a      1 1              3      17bfcdb967      1 1              4      7ea0fd3c88      1 1              ...      ...      ...              17995      c78063e0a6      1 1              17996      bfcdcd2720      1 1              17997      07c3553ef7      1 1              17998      9c2e45bf79      1 1              17999      41d0f0703c      1 1      18000 rows × 2 columnssub_df = pd.DataFrame.from_dict(result_dict,orient = 'index') # if the keys should be rows, pass 'index'sub_df = sub_df.reset_index()sub_df = sub_df.rename(columns={'index': 'id',0:'rle_mask'}) # rename column namessub_df['id']  = sub_df['id'].apply(lambda x : x.split('/')[-1].split('.')[0])sub_df                  id      rle_mask                  0      ca0030c4ed      46 7 103 1 106 1 149 6 204 1 207 1 249 6 324 6...              1      e8bec3476b      103 1 1432 1 2539 2 2738 5 2747 2 2843 8 2945 ...              2      4d5bd7f272      1 1 3 91 102 95 203 93 304 91 405 91 506 89 60...              3      f6b8b2edef      1 1 3 97 102 909 1012 100 1113 100 1214 100 13...              4      b4916183a5      75 17 176 17 277 17 378 17 478 19 579 20 674 1...              ...      ...      ...              17995      aa0a7a440e      17 8 27 37 67 9 106 1 114 49 167 4 194 2 201 2...              17996      95eb72bdc1      1 1 3 28 102 29 203 4 304 2 405 3 506 3 607 3 ...              17997      a58f7bf642      78 3 88 3 95 1 187 4 288 3 389 1 2724 2 2827 2...              17998      d809054e30      6315 1 6416 1 6517 1 10099 1 10200 1              17999      4f7b28dcba      8055 1 8155 3 8257 1      18000 rows × 2 columnssub_df.to_csv('sub_segmentation.csv',index=False)]]></content>
      <categories>
        
          <category> Segmentation </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
          <tag> Segmentation </tag>
        
          <tag> U-net </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[National Data Science Bowl(Classification)]]></title>
      <url>/classification/2021/01/24/bowl/</url>
      <content type="text"><![CDATA[대회 소개플랑크톤 이미지로 121 개의 플랑크톤 종 구분하는 대회National Data Science BowlPredict ocean health, one plankton at a timehttps://www.kaggle.com/c/datasciencebowlDescriptionPlankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health.The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the health of the world’s oceans, one plankton at a time.import numpy as np import pandas as pd import zipfileimport globfrom PIL import Imageimport osimport matplotlib.pyplot as pltimport seaborn as snsimport randomimport cv2import tensorflow as tffrom tensorflow.keras import *from tensorflow.keras.layers import *from tensorflow.keras.applications import EfficientNetB1from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStoppingfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldimport matplotlib.pyplot as pltimport seaborn as snsfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))/kaggle/input/datasciencebowl/plankton_identification.pdf/kaggle/input/datasciencebowl/train.zip/kaggle/input/datasciencebowl/sampleSubmission.csv.zip/kaggle/input/datasciencebowl/test.zip데이터 불러오기  train, test 데이터가 zip으로 압축되어 있으므로 먼저 압축을 풀어주자.with zipfile.ZipFile('/kaggle/input/datasciencebowl/train.zip','r') as z:    z.extractall('train')    with zipfile.ZipFile('/kaggle/input/datasciencebowl/test.zip','r') as z:    z.extractall('test')Train Dataset  Train 데이터프레임을 만들어 주기train = glob.glob('train/*/*/*')train_df = pd.DataFrame({'path' : train})train_df['image'] = train_df['path'].apply(lambda x : x.split('/')[-1])train_df['label'] = train_df['path'].apply(lambda x : x.split('/')[-2])train_df                  path      image      label                  0      train/train/pteropod_theco_dev_seq/36451.jpg      36451.jpg      pteropod_theco_dev_seq              1      train/train/pteropod_theco_dev_seq/44793.jpg      44793.jpg      pteropod_theco_dev_seq              2      train/train/pteropod_theco_dev_seq/157712.jpg      157712.jpg      pteropod_theco_dev_seq              3      train/train/pteropod_theco_dev_seq/4992.jpg      4992.jpg      pteropod_theco_dev_seq              4      train/train/pteropod_theco_dev_seq/144126.jpg      144126.jpg      pteropod_theco_dev_seq              ...      ...      ...      ...              30331      train/train/hydromedusae_shapeB/27912.jpg      27912.jpg      hydromedusae_shapeB              30332      train/train/hydromedusae_shapeB/49210.jpg      49210.jpg      hydromedusae_shapeB              30333      train/train/hydromedusae_shapeB/114615.jpg      114615.jpg      hydromedusae_shapeB              30334      train/train/hydromedusae_shapeB/81391.jpg      81391.jpg      hydromedusae_shapeB              30335      train/train/hydromedusae_shapeB/95843.jpg      95843.jpg      hydromedusae_shapeB      30336 rows × 3 columnsTest Dataset  Test 데이터 프레임을 만들어 주기test = glob.glob('./test/*/*')test_df = pd.DataFrame({'path' : test})test_df['image'] = test_df['path'].apply(lambda x : x.split('/')[-1])test_df                  path      image                  0      ./test/test/105188.jpg      105188.jpg              1      ./test/test/27651.jpg      27651.jpg              2      ./test/test/11940.jpg      11940.jpg              3      ./test/test/87538.jpg      87538.jpg              4      ./test/test/95396.jpg      95396.jpg              ...      ...      ...              130395      ./test/test/34122.jpg      34122.jpg              130396      ./test/test/88776.jpg      88776.jpg              130397      ./test/test/78514.jpg      78514.jpg              130398      ./test/test/1830.jpg      1830.jpg              130399      ./test/test/8917.jpg      8917.jpg      130400 rows × 2 columns데이터 확인하기  이미지 모양 및 크기 확인  class 개수 및 분포 확인 이미지 확인하기 fig , axes = plt.subplots(4,3)fig.set_size_inches(15,10)for index in range(0,12):    idx = random.randrange(len(train_df))    img = Image.fromarray(cv2.imread(train_df.iloc[idx,0], cv2.IMREAD_COLOR)).resize((256,256))    axes[index//3 , index%3].imshow(img)    axes[index//3 , index%3].set_title(train_df['label'][idx])    axes[index//3,  index%3].axis('off')샘플 이미지 사이즈 확인하기sample_image = Image.open(train[1000])print("Image size: ",sample_image.size)sample_image.resize((128,128))Image size:  (131, 251)Train Dataset 이미지들의 평균 너비 및 높이w = []h = []idx=0for i in train:    img = Image.open(i)    w.append(img.size[0])    h.append(img.size[1])    img.close()    idx+=1    print("Train Images' mean width :",np.mean(w),"\nTrain Images' mean height :",np.mean(h))Train Images' mean width : 73.50728507383967 Train Images' mean height : 66.66182093881856Label의 분포 print("The unique label numbers :",train_df['label'].nunique())a,b = plt.subplots(1,1,figsize=(20,12))plot = sns.countplot(train_df['label'])plt.setp(plot.get_xticklabels(), rotation=90)plt.show()The unique label numbers : 121  각 label 별로 데이터의 분포가 다르다.Test Dataset 전처리idg_test = ImageDataGenerator()test_generator = idg_test.flow_from_dataframe(test_df,                                                 x_col = 'path',                                                y_col = None,                                                 class_mode = None,                                                shuffle=False,                                                batch_size = 64,                                                target_size = (150,150),)Found 130400 validated image filenames.교차 검증으로 모델 학습하기k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)result=0index=0for train_index, valid_index in k_fold.split(train_df, train_df['label']):        X_train = train_df.iloc[train_index]    X_val = train_df.iloc[valid_index]        idg_train = ImageDataGenerator(horizontal_flip=True)    idg_val = ImageDataGenerator()        train_generator = idg_train.flow_from_dataframe(X_train,                                                 x_col = 'path',                                                y_col = 'label',                                                 batch_size = 64,                                                target_size = (150,150))    val_generator = idg_val.flow_from_dataframe(X_val,                                                 x_col = 'path',                                                y_col = 'label',                                                 batch_size = 64,                                                target_size = (150,150))            model = Sequential()    model.add(EfficientNetB1(include_top = False,weights ='imagenet', pooling = 'avg'))    model.add(Dense(121, activation = 'softmax'))    model.compile(optimizer = tf.keras.optimizers.Adam() , loss = 'categorical_crossentropy', metrics = ['acc'],)    es = EarlyStopping(patience = 10, verbose = True)    ckpt = ModelCheckpoint('best.h5', save_best_only = True, verbose =True, monitor = 'val_loss')    rl = ReduceLROnPlateau(monitor = 'val_loss',patience = 5, verbose = True)    model.fit(train_generator,               validation_data = val_generator,              callbacks = [ckpt, rl, es],               epochs =25)    model.load_weights('best.h5')        result += model.predict(test_generator, verbose=True) / 5        index+=1    print(f"\n\n{index} 번째 교차 검증 완료 ... ... ...\n\n")Epoch 12/25380/380 [==============================] - ETA: 0s - loss: 0.0739 - acc: 0.9796Epoch 00012: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0739 - acc: 0.9796 - val_loss: 0.9785 - val_acc: 0.7618Epoch 13/25380/380 [==============================] - ETA: 0s - loss: 0.0572 - acc: 0.9852Epoch 00013: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0572 - acc: 0.9852 - val_loss: 1.0230 - val_acc: 0.7636Epoch 14/25380/380 [==============================] - ETA: 0s - loss: 0.0483 - acc: 0.9866Epoch 00014: val_loss did not improve from 0.93774380/380 [==============================] - 112s 295ms/step - loss: 0.0483 - acc: 0.9866 - val_loss: 1.0412 - val_acc: 0.7620Epoch 15/25380/380 [==============================] - ETA: 0s - loss: 0.0404 - acc: 0.9894Epoch 00015: val_loss did not improve from 0.93774Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.380/380 [==============================] - 112s 295ms/step - loss: 0.0404 - acc: 0.9894 - val_loss: 1.0788 - val_acc: 0.7603Epoch 16/25380/380 [==============================] - ETA: 0s - loss: 0.0322 - acc: 0.9927Epoch 00016: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0322 - acc: 0.9927 - val_loss: 1.0791 - val_acc: 0.7623Epoch 17/25380/380 [==============================] - ETA: 0s - loss: 0.0290 - acc: 0.9941Epoch 00017: val_loss did not improve from 0.93774380/380 [==============================] - 112s 296ms/step - loss: 0.0290 - acc: 0.9941 - val_loss: 1.0779 - val_acc: 0.7625Epoch 18/25380/380 [==============================] - ETA: 0s - loss: 0.0285 - acc: 0.9938Epoch 00018: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0285 - acc: 0.9938 - val_loss: 1.0824 - val_acc: 0.7640Epoch 19/25380/380 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9948Epoch 00019: val_loss did not improve from 0.93774380/380 [==============================] - 113s 297ms/step - loss: 0.0260 - acc: 0.9948 - val_loss: 1.0838 - val_acc: 0.7651Epoch 20/25380/380 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9952Epoch 00020: val_loss did not improve from 0.93774Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.380/380 [==============================] - 112s 296ms/step - loss: 0.0253 - acc: 0.9952 - val_loss: 1.0899 - val_acc: 0.7640Epoch 00020: early stopping2038/2038 [==============================] - 107s 53ms/step5 번째 교차 검증 완료 ... ... ...제출  sampleSubmission.csv를 열어서 column의 순서를 확인해보면, 알파벳 순서로 들어가 있지 않다.  result 칼럼의 순서와도 맞지 않으니, 이런 겨웅 직접 제출 파일을 만드는 것이 더 쉽다.with zipfile.ZipFile('/kaggle/input/datasciencebowl/sampleSubmission.csv.zip','r') as z:    z.extractall('sub')sub = pd.read_csv("sub/sampleSubmission.csv")sub                  image      acantharia_protist_big_center      acantharia_protist_halo      acantharia_protist      amphipods      appendicularian_fritillaridae      appendicularian_s_shape      appendicularian_slight_curve      appendicularian_straight      artifacts_edge      ...      trichodesmium_tuft      trochophore_larvae      tunicate_doliolid_nurse      tunicate_doliolid      tunicate_partial      tunicate_salp_chains      tunicate_salp      unknown_blobs_and_smudges      unknown_sticks      unknown_unclassified                  0      1.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              1      10.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              2      100.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              3      1000.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              4      10000.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              130395      99994.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130396      99995.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130397      99996.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130398      99997.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264              130399      99999.jpg      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      ...      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      0.008264      130400 rows × 122 columnscol_names = list(sub.columns[1:])col_names.sort()col_names['acantharia_protist', 'acantharia_protist_big_center', 'acantharia_protist_halo', 'amphipods', 'appendicularian_fritillaridae', 'appendicularian_s_shape', 'appendicularian_slight_curve', 'appendicularian_straight', 'artifacts', 'artifacts_edge', 'chaetognath_non_sagitta', 'chaetognath_other', 'chaetognath_sagitta', 'chordate_type1', 'copepod_calanoid', 'copepod_calanoid_eggs', 'copepod_calanoid_eucalanus', 'copepod_calanoid_flatheads', 'copepod_calanoid_frillyAntennae', 'copepod_calanoid_large', 'copepod_calanoid_large_side_antennatucked', 'copepod_calanoid_octomoms', 'copepod_calanoid_small_longantennae', 'copepod_cyclopoid_copilia',    ...  'pteropod_butterfly', 'pteropod_theco_dev_seq', 'pteropod_triangle', 'radiolarian_chain', 'radiolarian_colony', 'shrimp-like_other', 'shrimp_caridean', 'shrimp_sergestidae', 'shrimp_zoea', 'siphonophore_calycophoran_abylidae', 'siphonophore_calycophoran_rocketship_adult', 'siphonophore_calycophoran_rocketship_young', 'siphonophore_calycophoran_sphaeronectes', 'siphonophore_calycophoran_sphaeronectes_stem', 'siphonophore_calycophoran_sphaeronectes_young', 'siphonophore_other_parts', 'siphonophore_partial', 'siphonophore_physonect', 'siphonophore_physonect_young', 'stomatopod', 'tornaria_acorn_worm_larvae', 'trichodesmium_bowtie', 'trichodesmium_multiple', 'trichodesmium_puff', 'trichodesmium_tuft', 'trochophore_larvae', 'tunicate_doliolid', 'tunicate_doliolid_nurse', 'tunicate_partial', 'tunicate_salp', 'tunicate_salp_chains', 'unknown_blobs_and_smudges', 'unknown_sticks', 'unknown_unclassified']my_sub = pd.concat([test_df,pd.DataFrame(result, columns=col_names)], axis=1).drop(['path'], axis=1)my_sub                  image      acantharia_protist      acantharia_protist_big_center      acantharia_protist_halo      amphipods      appendicularian_fritillaridae      appendicularian_s_shape      appendicularian_slight_curve      appendicularian_straight      artifacts      ...      trichodesmium_tuft      trochophore_larvae      tunicate_doliolid      tunicate_doliolid_nurse      tunicate_partial      tunicate_salp      tunicate_salp_chains      unknown_blobs_and_smudges      unknown_sticks      unknown_unclassified                  0      105188.jpg      3.407894e-05      5.234519e-07      4.727002e-07      2.523708e-05      6.414000e-07      1.429394e-04      7.389794e-05      2.510147e-05      8.997078e-08      ...      2.137576e-05      2.115729e-06      1.604188e-06      8.190606e-06      1.233781e-06      2.818242e-06      1.334955e-06      0.000023      7.377646e-05      0.000072              1      27651.jpg      2.613361e-06      2.237140e-07      1.978252e-06      6.420553e-07      1.818773e-06      1.416835e-05      8.689414e-06      2.637850e-07      2.187041e-08      ...      2.810447e-07      9.286011e-07      8.374320e-06      2.925759e-06      2.006822e-07      2.315101e-07      7.389501e-07      0.000014      2.626070e-06      0.000423              2      11940.jpg      6.104551e-06      2.521627e-06      2.513692e-07      4.636616e-05      2.085026e-06      1.489576e-05      8.225787e-05      1.137621e-05      3.021312e-07      ...      1.476521e-05      7.298316e-07      1.122896e-06      1.197497e-04      4.583527e-07      1.900000e-05      4.955790e-07      0.000482      3.988420e-05      0.002802              3      87538.jpg      1.197881e-05      4.275779e-06      1.933437e-05      3.641628e-03      1.369998e-03      2.135313e-04      4.967969e-04      7.344080e-04      8.830065e-06      ...      1.323840e-01      8.188009e-06      6.234657e-04      2.646256e-04      5.426758e-05      1.694718e-03      1.481177e-05      0.000878      4.522099e-02      0.106003              4      95396.jpg      1.079931e-07      1.477846e-06      4.025902e-07      9.353130e-05      6.802826e-07      3.575570e-06      4.541085e-07      1.869251e-07      2.157424e-07      ...      3.714577e-06      4.525396e-07      1.575890e-07      7.061017e-07      1.216337e-06      4.131224e-07      3.218001e-07      0.000011      8.910469e-07      0.000015              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              130395      34122.jpg      9.404398e-06      4.410570e-06      1.469415e-05      4.892956e-04      4.235670e-04      7.620276e-04      7.704680e-05      4.103683e-05      1.476081e-05      ...      1.900832e-04      1.661727e-05      9.112542e-02      4.120418e-04      3.442946e-04      3.249257e-04      1.036034e-05      0.033025      1.140596e-05      0.008211              130396      88776.jpg      1.188530e-04      3.108746e-06      7.981415e-06      2.125595e-07      5.005016e-08      5.221577e-07      6.974371e-07      1.526524e-06      1.525192e-05      ...      7.819511e-01      6.797829e-07      2.360160e-07      1.453742e-07      5.104931e-06      3.599948e-06      5.606108e-07      0.000010      1.053775e-04      0.000017              130397      78514.jpg      3.513707e-06      1.072021e-07      6.263767e-07      2.130197e-07      8.390234e-07      3.240724e-05      1.973183e-04      1.500386e-04      1.360382e-08      ...      1.262665e-07      2.861952e-08      2.094702e-04      1.114861e-06      8.004471e-07      1.075251e-05      1.032177e-07      0.000038      1.695668e-06      0.000293              130398      1830.jpg      2.687246e-08      6.054729e-10      1.001864e-07      2.510690e-08      9.267259e-09      2.659469e-08      6.407049e-08      6.902572e-08      9.994848e-01      ...      1.325866e-07      2.992527e-09      6.681008e-08      9.846344e-09      9.125281e-08      4.204970e-09      2.354498e-08      0.000091      7.961059e-08      0.000001              130399      8917.jpg      1.647273e-07      1.519561e-07      9.120576e-08      1.517367e-03      1.538393e-07      2.816295e-07      6.235562e-07      5.191119e-06      2.437675e-06      ...      7.615223e-07      3.302962e-06      1.213374e-02      8.384104e-01      9.281476e-05      2.865229e-03      1.414542e-05      0.000016      7.298431e-06      0.003105      130400 rows × 122 columnsmy_sub.to_csv('bowl_sub.csv',index=False)  Evaluation : Log Loss 0.65988순위 : 1,049 중 35등]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[State Farm Distracted Driver Detection(Classification)]]></title>
      <url>/classification/2021/01/14/statefarm/</url>
      <content type="text"><![CDATA[대회 소개운전 중인 운전자의 이미지로 운전자의 행동 분류하는 대회State Farm Distracted Driver DetectionCan computer vision spot distracted drivers?https://www.kaggle.com/c/state-farm-distracted-driver-detection/overviewDescriptionWe’ve all been there: a light turns green and the car in front of you doesn’t budge. Or, a previously unremarkable vehicle suddenly slows and starts swerving from side-to-side.When you pass the offending driver, what do you expect to see? You certainly aren’t surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year.State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver’s behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?What to doThe 10 classes to predict are:c0: normal drivingc1: texting - rightc2: talking on the phone - rightc3: texting - leftc4: talking on the phone - leftc5: operating the radioc6: drinkingc7: reaching behindc8: hair and makeupc9: talking to passengerImport Libraryimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import osimport numpy as np import pandas as pdimport osimport globfrom PIL import Imagefrom tensorflow.keras import *from tensorflow.keras.layers import *from tensorflow.keras.applications import EfficientNetB1from tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau from sklearn.model_selection import StratifiedKFoldpd.options.display.max_colwidth =999이미지 리스트 불러오기total = pd.read_csv('/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv')total                  subject      classname      img                  0      p002      c0      img_44733.jpg              1      p002      c0      img_72999.jpg              2      p002      c0      img_25094.jpg              3      p002      c0      img_69092.jpg              4      p002      c0      img_92629.jpg              ...      ...      ...      ...              22419      p081      c9      img_56936.jpg              22420      p081      c9      img_46218.jpg              22421      p081      c9      img_25946.jpg              22422      p081      c9      img_67850.jpg              22423      p081      c9      img_9684.jpg      22424 rows × 3 columnsTrain Dataset 만들기train = glob.glob('../input/state-farm-distracted-driver-detection/imgs/train/*/*')train_df = pd.DataFrame({'path' : train})train_df['img'] = train_df['path'].apply(lambda x : x.split('/')[-1])train_df['classname'] = train_df['path'].apply(lambda x : x.split('/')[-2])train_df                  path      img      classname                  0      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_68208.jpg      img_68208.jpg      c5              1      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_77583.jpg      img_77583.jpg      c5              2      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_49189.jpg      img_49189.jpg      c5              3      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_6690.jpg      img_6690.jpg      c5              4      ../input/state-farm-distracted-driver-detection/imgs/train/c5/img_95740.jpg      img_95740.jpg      c5              ...      ...      ...      ...              22419      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_6087.jpg      img_6087.jpg      c0              22420      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_36959.jpg      img_36959.jpg      c0              22421      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_19429.jpg      img_19429.jpg      c0              22422      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_99342.jpg      img_99342.jpg      c0              22423      ../input/state-farm-distracted-driver-detection/imgs/train/c0/img_48589.jpg      img_48589.jpg      c0      22424 rows × 3 columnstrain_df['classname'].nunique() # 클래스 개수10데이터 확인Image.open(train[10])Test Dataset 만들기test = glob.glob('../input/state-farm-distracted-driver-detection/imgs/test/*')test_df = pd.DataFrame({'path' : test})test_df['img'] = test_df['path'].apply(lambda x : x.split('/')[-1])test_df                  path      img                  0      ../input/state-farm-distracted-driver-detection/imgs/test/img_96590.jpg      img_96590.jpg              1      ../input/state-farm-distracted-driver-detection/imgs/test/img_32366.jpg      img_32366.jpg              2      ../input/state-farm-distracted-driver-detection/imgs/test/img_99675.jpg      img_99675.jpg              3      ../input/state-farm-distracted-driver-detection/imgs/test/img_85937.jpg      img_85937.jpg              4      ../input/state-farm-distracted-driver-detection/imgs/test/img_73903.jpg      img_73903.jpg              ...      ...      ...              79721      ../input/state-farm-distracted-driver-detection/imgs/test/img_109.jpg      img_109.jpg              79722      ../input/state-farm-distracted-driver-detection/imgs/test/img_53257.jpg      img_53257.jpg              79723      ../input/state-farm-distracted-driver-detection/imgs/test/img_90376.jpg      img_90376.jpg              79724      ../input/state-farm-distracted-driver-detection/imgs/test/img_28000.jpg      img_28000.jpg              79725      ../input/state-farm-distracted-driver-detection/imgs/test/img_93083.jpg      img_93083.jpg      79726 rows × 2 columnsidg_test = ImageDataGenerator()test_generator = idg_test.flow_from_dataframe(test_df,                                             x_col = 'path',                                             y_col = None,                                             class_mode =None,                                             shuffle = False,                                             target_size = (256,256))Found 79726 validated image filenames.K-fold Cross Validation(교차검증)으로 모델 학습하기kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)모든 데이터셋을 학습시켜 보다 안정적인 모델 성능 뽑아내기  이 대회에서 교차검증 없이 모델을 학습하면 특정 subject(운전자)에 과적합되는 문제가 발생한다.  모델이 classname을 예측할 때 subject에 크게 영향을 받을 수 있다는 것이다.  train_test_split에서 stratify옵션을 주어 학습을 해도, 데이터의 일부분만을 추출하는 것이기 때문에 Test 데이터의 분포가 Valid 데이터셋의 분포와 다를 수 있다.  결국 일정 부분의 학습 데이터를 잃는 것이기에, 전체 데이터셋을 모두 훈련에 사용하고 싶을 때 교차 검증을 진행한다.  k-fold Cross Validation 특징          전체 데이터를 훈련 및 평가할 수 있음.      앙상블의 효과를 얻을 수 있다. k-fold 학습마다 독립적인 모델을 학습한다라고 생각하면 쉽다.      같은 작업을 여러번 반복하는 것이기에 시간은 오래 걸린다.      result = 0index=0for train_index, valid_index in kfold.split(train_df, train_df['classname']):    X_train = train_df.iloc[train_index]    X_val = train_df.iloc[valid_index]            idg_train = ImageDataGenerator()    idg_val = ImageDataGenerator()        train_generator = idg_train.flow_from_dataframe(X_train,                                                    x_col = 'path',                                                   y_col = 'classname',                                                   batch_size = 16,                                                   target_size = (256,256))            val_generator = idg_val.flow_from_dataframe(X_val,                                                x_col = 'path',                                               y_col = 'classname',                                               batch_size = 16,                                               target_size = (256,256))                    early_stop = EarlyStopping(patience=5,                              verbose = True)    model_ckpt = ModelCheckpoint(filepath= 'best.h5',                                 monitor = 'val_loss',                                save_best_only = True,                                verbose = True)    rl = ReduceLROnPlateau(verbose=True,patience=4,)      model = Sequential()    model.add(EfficientNetB1(include_top = False, weights = 'imagenet',pooling = 'avg'))    model.add(Dense(10, activation = 'softmax'))    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])        model.fit(train_generator,             validation_data = val_generator,             epochs=  20,             callbacks = [early_stop, model_ckpt, rl])        model.load_weights('best.h5')            result += model.predict(test_generator, verbose = True) /5    index +=1    print('fin : ',index)제출  sample_submission.csv 에서 각 이미지의 순서와 result의 이미지 순서가 다르기 때문에 새로 Dataframe을 만드는 것이 편하다.result_df = pd.DataFrame(result, columns=['c0', 'c1', 'c2', 'c3','c4', 'c5', 'c6', 'c7','c8', 'c9'])result_df                                                          c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      4.898581e-03      6.054179e-03      5.146383e-03      5.325366e-04      7.062429e-04      0.012463      0.642532      2.142958e-02      3.042254e-01      0.002012              1      4.047657e-08      4.258116e-08      7.385115e-08      7.353977e-08      5.030099e-08      0.999987      0.000007      2.678771e-08      2.975033e-07      0.000006              2      4.653947e-01      9.502587e-03      3.015550e-02      7.141132e-03      8.581814e-03      0.001376      0.228859      1.078819e-03      2.433791e-01      0.004531              3      8.255903e-05      1.916165e-05      1.645608e-05      2.139845e-03      2.219971e-04      0.990963      0.000140      5.377778e-06      3.694886e-05      0.006375              4      3.502117e-07      1.061771e-04      4.572770e-05      2.403757e-06      1.187768e-05      0.000053      0.000408      9.991777e-01      1.797632e-04      0.000014              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      7.563246e-03      1.897852e-02      2.062093e-01      1.308421e-03      5.482526e-03      0.002233      0.118231      1.006306e-01      5.321654e-01      0.007198              79722      2.018641e-03      3.363243e-04      4.372783e-04      4.747344e-01      4.001005e-01      0.004480      0.000733      3.581994e-04      8.031728e-04      0.115998              79723      8.465707e-02      5.784687e-01      3.608754e-03      3.662695e-02      1.371328e-02      0.011587      0.017968      7.717087e-02      3.644365e-02      0.139756              79724      6.226664e-08      4.983243e-08      7.004503e-09      2.172765e-08      6.979324e-08      0.999966      0.000004      6.643240e-09      2.172691e-06      0.000028              79725      5.966864e-02      2.801355e-02      7.123795e-03      6.038536e-04      2.516853e-03      0.004967      0.027987      1.176820e-03      8.555837e-01      0.012359      79726 rows × 10 columnssub = pd.read_csv('../input/state-farm-distracted-driver-detection/sample_submission.csv')sub                  img      c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      img_1.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              1      img_10.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              2      img_100.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              3      img_1000.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              4      img_100000.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      img_99994.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79722      img_99995.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79723      img_99996.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79724      img_99998.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1              79725      img_99999.jpg      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1      79726 rows × 11 columnsmy_sub = pd.concat([test_df, result_df], axis = 1).drop(['path'], axis =1)my_sub                  img      c0      c1      c2      c3      c4      c5      c6      c7      c8      c9                  0      img_96590.jpg      4.898581e-03      6.054179e-03      5.146383e-03      5.325366e-04      7.062429e-04      0.012463      0.642532      2.142958e-02      3.042254e-01      0.002012              1      img_32366.jpg      4.047657e-08      4.258116e-08      7.385115e-08      7.353977e-08      5.030099e-08      0.999987      0.000007      2.678771e-08      2.975033e-07      0.000006              2      img_99675.jpg      4.653947e-01      9.502587e-03      3.015550e-02      7.141132e-03      8.581814e-03      0.001376      0.228859      1.078819e-03      2.433791e-01      0.004531              3      img_85937.jpg      8.255903e-05      1.916165e-05      1.645608e-05      2.139845e-03      2.219971e-04      0.990963      0.000140      5.377778e-06      3.694886e-05      0.006375              4      img_73903.jpg      3.502117e-07      1.061771e-04      4.572770e-05      2.403757e-06      1.187768e-05      0.000053      0.000408      9.991777e-01      1.797632e-04      0.000014              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              79721      img_109.jpg      7.563246e-03      1.897852e-02      2.062093e-01      1.308421e-03      5.482526e-03      0.002233      0.118231      1.006306e-01      5.321654e-01      0.007198              79722      img_53257.jpg      2.018641e-03      3.363243e-04      4.372783e-04      4.747344e-01      4.001005e-01      0.004480      0.000733      3.581994e-04      8.031728e-04      0.115998              79723      img_90376.jpg      8.465707e-02      5.784687e-01      3.608754e-03      3.662695e-02      1.371328e-02      0.011587      0.017968      7.717087e-02      3.644365e-02      0.139756              79724      img_28000.jpg      6.226664e-08      4.983243e-08      7.004503e-09      2.172765e-08      6.979324e-08      0.999966      0.000004      6.643240e-09      2.172691e-06      0.000028              79725      img_93083.jpg      5.966864e-02      2.801355e-02      7.123795e-03      6.038536e-04      2.516853e-03      0.004967      0.027987      1.176820e-03      8.555837e-01      0.012359      79726 rows × 11 columnsmy_sub.to_csv('sub.csv', index=False) # 0.18165  Final score which evaluated with multi-class logarithmic loss is 0.18165.]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> CNN </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Plant Seedlings Classification(Classification)]]></title>
      <url>/classification/2021/01/06/plantseedlings/</url>
      <content type="text"><![CDATA[대회 소개묘목과 잡초 이미지를 분류하는 대회Plant Seedlings ClassificationDetermine the species of a seedling from an imagehttps://www.kaggle.com/c/plant-seedlings-classificationDescriptionCan you differentiate a weed from a crop seedling?The ability to do so effectively can mean better crop yields and better stewardship of the environment.The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.We’re hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.Baseline 코드import numpy as npimport pandas as pd import osimport globfrom PIL import Imagefrom sklearn.model_selection import train_test_splitfrom keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras import * from tensorflow.keras.layers import * from tensorflow.keras.applications.efficientnet import EfficientNetB1 # 거의 B1 모델 사용 / B2로 갈수록 무거워짐from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint데이터 불러오기path = glob.glob("/kaggle/input/plant-seedlings-classification/train/*/*")train = pd.DataFrame({"path" : path})train['label'] = train['path'].apply(lambda x : x.split("/")[-2])train                  path      label                  0      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              1      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              2      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              3      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              4      /kaggle/input/plant-seedlings-classification/t...      Scentless Mayweed              ...      ...      ...              4745      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4746      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4747      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4748      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse              4749      /kaggle/input/plant-seedlings-classification/t...      Shepherds Purse      4750 rows × 2 columns데이터 확인Image.open(path[100])Train, valid dataset 만들기  Train, Valid Dataset 나누기x_train, x_valid = train_test_split(train,                                     test_size = 0.2,                                     stratify = train['label'],                                    random_state=42)  ImgaeDataGenerator로 데이터 전처리하기idg = ImageDataGenerator()idg2 = ImageDataGenerator()train_generator = idg.flow_from_dataframe(x_train,                                         x_col = 'path',                                         y_col = 'label',                                         target_size = (256,256),                                          batch_size= 32) valid_generator = idg2.flow_from_dataframe(x_valid,                                         x_col = 'path',                                         y_col = 'label',                                         target_size = (256,256),                                         batch_size= 32)Found 3800 validated image filenames belonging to 12 classes.Found 950 validated image filenames belonging to 12 classes.Model 학습# # 모델 선언 # model = Sequential()# # 이미지 학습하는 층을 쌓기# model.add(Conv2D(16, (3,3), activation = 'relu', input_shape = (256,256,3))) # model.add(Conv2D(16, (3,3), activation = 'relu')) # model.add(MaxPooling2D()) # model.add(Conv2D(32, (3,3), activation = 'relu')) # model.add(Conv2D(32, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(64, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(128, (3,3), activation = 'relu')) # model.add(Conv2D(128, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(256, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(512, (3,3), activation = 'relu')) # model.add(MaxPooling2D())# model.add(Conv2D(1024, (1,1), activation = 'relu')) # # 차원 축소# model.add(GlobalAveragePooling2D()) # model.add(Dense(12, activation = 'softmax'))# 조기 종료 옵션es = EarlyStopping(patience=3,                    verbose=True)mc = ModelCheckpoint("best.h5",                     save_best_only = True,                     verbose=True                    )# transfer learningmodel = Sequential() # 항상 모델 선언 다시 하고 학습할 것.model.add(EfficientNetB1(include_top=False, weights = 'imagenet', pooling = 'avg')) model.add(Dense(12, activation='softmax')) # 최적화, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')model.fit(train_generator,         validation_data  = valid_generator,         epochs = 100,          callbacks = [es, mc],           ) model.load_weights('best.h5')  모델 Summarymodel.summary()Model: "sequential"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================efficientnetb1 (Functional)  (None, 1280)              6575239   _________________________________________________________________dense (Dense)                (None, 12)                15372     =================================================================Total params: 6,590,611Trainable params: 6,528,556Non-trainable params: 62,055_________________________________________________________________Test Dataset 만들기  Train, Valid dataset 만들듯이 ImageDataGenerator를 사용해 전처리하면 된다.test_path = glob.glob("/kaggle/input/plant-seedlings-classification/test/*")test = pd.DataFrame({"path" : test_path})test                  path                  0      /kaggle/input/plant-seedlings-classification/t...              1      /kaggle/input/plant-seedlings-classification/t...              2      /kaggle/input/plant-seedlings-classification/t...              3      /kaggle/input/plant-seedlings-classification/t...              4      /kaggle/input/plant-seedlings-classification/t...              ...      ...              789      /kaggle/input/plant-seedlings-classification/t...              790      /kaggle/input/plant-seedlings-classification/t...              791      /kaggle/input/plant-seedlings-classification/t...              792      /kaggle/input/plant-seedlings-classification/t...              793      /kaggle/input/plant-seedlings-classification/t...      794 rows × 1 columnstest_generator =idg.flow_from_dataframe(test,                                        x_col = 'path',                                        y_col = None,                                        class_mode=None,                                        shuffle = False,                                        target_size = (256,256)                                       )Found 794 validated image filenames.모델로 예측하기result = model.predict(test_generator, verbose = 1)result25/25 [==============================] - 9s 376ms/steparray([[3.1431966e-02, 1.3168590e-06, 1.8106741e-06, ..., 3.4729105e-06,        2.8560698e-05, 6.0173708e-05],       [1.1053331e-04, 1.5426338e-04, 2.5204610e-04, ..., 2.1230977e-05,        1.5883794e-05, 8.8891864e-01],       [1.6521607e-06, 6.2318693e-05, 2.8655048e-05, ..., 4.6870503e-01,        1.3315211e-04, 4.2114169e-03],       ...,       [2.1159883e-06, 2.1564020e-01, 7.7060443e-01, ..., 4.1908701e-03,        9.3793590e-03, 1.1505652e-05],       [6.5076878e-05, 3.2120446e-05, 3.2295244e-05, ..., 4.1470761e-05,        5.1161887e-06, 9.9024123e-01],       [1.2335427e-06, 6.4034253e-01, 4.4173703e-05, ..., 3.2450294e-05,        3.5616100e-01, 4.7782612e-07]], dtype=float32)제출  가장 높은 확률의 class(string)를 예측값으로 추출해야 한다.  train_generator.class_indices로 각 클래스에 해당하는 index를 가져온다.class_dict = train_generator.class_indicesclass_dict{'Black-grass': 0, 'Charlock': 1, 'Cleavers': 2, 'Common Chickweed': 3, 'Common wheat': 4, 'Fat Hen': 5, 'Loose Silky-bent': 6, 'Maize': 7, 'Scentless Mayweed': 8, 'Shepherds Purse': 9, 'Small-flowered Cranesbill': 10, 'Sugar beet': 11}class_list =[]for i in class_dict:    class_list.append(i)class_list ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']sub = pd.read_csv('/kaggle/input/plant-seedlings-classification/sample_submission.csv')sub['species']= [class_list[i] for i in np.argmax(result,1)]sub['file'] = test['path'].apply(lambda x : x.split("/")[-1])sub                  file      species                  0      fd87b36ae.png      Loose Silky-bent              1      0e8492cb1.png      Sugar beet              2      8d6acbe9b.png      Common Chickweed              3      54b3afd58.png      Cleavers              4      6049234e6.png      Fat Hen              ...      ...      ...              789      4c7838de4.png      Common Chickweed              790      fda39e16f.png      Loose Silky-bent              791      da4ed3a28.png      Charlock              792      a83820a2c.png      Sugar beet              793      e4a76885b.png      Maize      794 rows × 2 columnssub.to_csv('plant.csv',index=False)]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> CNN </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Dogs vs. Cats(Classification)]]></title>
      <url>/classification/2021/01/01/dogvscat/</url>
      <content type="text"><![CDATA[대회 소개개와 고양이 이미지를 분류하는 대회Dogs vs. Cats Redux: Kernels EditionDistinguish images of dogs from catshttps://www.kaggle.com/c/dogs-vs-cats-redux-kernels-editionDescriptionIn 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician’s cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty.Much has changed at Kaggle as well. Our online coding environment Kernels didn’t exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning’s future challenges.데이터 준비 및 확인이미지 분류 대회에서 …  아이디어 얻기 위해 데이터 하나 하나 뜯어보며 이미지 형태를 파악해야 한다.  머신러닝 대회에서 독립변수의 개수가 같아야 하는 것처럼 딥러닝에서도 Input 크기가 같아야 한다. 따라서 크기 조정 등의 최소한의 전처리가 필요하다.  이미지 크기 조정 시 이미 작은 이미지는 상관 없는데, 큰 이미지를 작은 이미지로 축소할 때 정보의 손실이 있을 수 있다.(나중에 더 깊게 다룰 예정)이미지 분류 베이스 라인 잡기  Data Preprocessing          glob 이용하여 데이터 불러오기      이미지 경로, 이미지 클래스를 칼럼으로 갖는 데이터 프레임 만들기      train_test_split으로 train, valid 데이터 만들기                  stratify 옵션으로 label 비율 맞춰서 데이터셋 나누기                    keras의 ImageDataGenerator 라이브러리를 사용하여 이미지 데이터 전처리하기        Modeling          모델 선언 및 학습 층 쌓기      최적화 함수, 손실 함수 정하기        train          모델로 전처리된 이미지 학습하기       데이터 확인 import numpy as np import pandas as pd import os!unzip '/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip'!unzip '/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip'from PIL import ImageImage.open('train/cat.100.jpg').resize((256,256))이미지 경로, 클래스를 칼럼으로 갖는 데이터프레임 만들기import globtrain = pd.DataFrame({"path" : glob.glob("train/*")})train['label'] = train['path'].apply(lambda x : x.split("/")[-1].split(".")[0])train                  path      label                  0      train/dog.10300.jpg      dog              1      train/cat.7540.jpg      cat              2      train/dog.5684.jpg      dog              3      train/cat.10367.jpg      cat              4      train/cat.537.jpg      cat              ...      ...      ...              24995      train/cat.2300.jpg      cat              24996      train/dog.7893.jpg      dog              24997      train/dog.8167.jpg      dog              24998      train/dog.6154.jpg      dog              24999      train/dog.4016.jpg      dog      25000 rows × 2 columnsTrain, Valid 셋 만들기from sklearn.model_selection import train_test_splitX_train, X_val = train_test_split(train,                                 test_size = 0.2,                                  stratify = train['label'],                                 random_state = 42)keras의 ImageDataGenerator 사용하여 데이터 전처리Data Augmentation  대회별로 도움되는 변형 옵션을 찾아야 한다.      데이터 변형을 한다고 해서 이미지 수가 두배가 되는게 아니다.    에폭당 50%확률로 변형이 적용되는데, 이는 ImageDataGenerator()에서 50%확률로 default 값이 지정되어 있기 때문이다.          한 epoch당 원본 이미지 50% + 변형 이미지 50%가 학습되는 것이므로, Data Augmentation을 할 때는 최소 여러 번의 epoch를 돌게 만들자.      어떤 변형 옵션이 도움이 되는지 확인할 때는 이미지 사이즈를 줄이고 배치 사이즈를 늘려 빨리 실험해보자.      평가할 때는 원본 이미지에 대해서 predict가 이루어져야 하기 때문에 idg_val = ImageDataGenerator()를 따로 지정해주는 것이다.      ImageDataGenerator()를 커스터마이징하여 값을 바꿀 수도 있다.      flow_from_dataframe 옵션  target_size          이미지 사이즈를 키우게 되면 정보 손실이 덜 하므로 그만큼 점수가 좋아진다.      원본 이미지의 사이즈가 클 경우 target_size를 이에 비슷하게 맞추는 것이 이상적이지만, 그만큼 학습 속도가 느려지게 된다. 또한 메모리 문제로 인해 target_size를 늘리면 batch_size를 줄여야 한다.      원본 이미지가 작은 경우 사이즈를 키웠다고 해서 더 성능이 좋아지는 것은 아니다.        batch_size          batch_size 역시 모델 성능(점수)에 영향을 준다.      from keras.preprocessing.image import ImageDataGeneratoridg_train = ImageDataGenerator(horizontal_flip=True)idg_valid = ImageDataGenerator()idg_test = ImageDataGenerator()train_generator = idg_train.flow_from_dataframe(X_train, # df 이름                                          x_col = 'path', # 이미지 path                                          y_col = 'label',# 클래스                                          batch_size = 32, # 기본값 32 -&gt; 처음엔 100으로 설정하여 학습 속도를 빠르게 하기도 함.                                          target_size = (256,256)) # 기본값 256                                          valid_generator = idg_valid.flow_from_dataframe(X_val,                                               x_col = 'path',                                               y_col = 'label',                                                batch_size = 32,                                                target_size = (256,256))Found 20000 validated image filenames belonging to 2 classes.Found 5000 validated image filenames belonging to 2 classes.모델링모델 선언 : Sequential()로 모델을 먼저 선언한다.  이미지 학습하는 층 쌓기: 기본적인 모델 구조를 설정한다. Conv2D함수 안에 n개의 feature map을 추출한다.          첫 layer에서는 input_shape을 설정해 주자. (256,256,3)      activation 함수를 적용하여 하나의 층이 비선형적인 학습을 할 수 있도록 한다.      하나의 Conv2D 층을 하나의 분류기라고 생각하자. 머신러닝에서 하나의 랜덤 포레스트 모델로 생각하면 쉽다.여러 개의 랜덤포레스트를 앙상블하면 학습에 도움이 되는 것처럼 여러 Conv2D층을 쌓으면 여러 특징들을 추출해낼 수 있기 때문에 학습에 도움이 된다.      Conv2D 층만 쌓게 되면 중요하지 않은 배경들도 똑같이 고려하게 되므로 학습이 어려워진다.합성곱층 통과 후 가장 특징적인 값만 가져오는   MaxPool2D층을 추가해주자.        차원 축소하기          Conv2D 채널을 지나서 나오는 피처맵들은 3차원 이미지 데이터들이다. Flatten()으로 3차원 데이터를 1차원으로 펼쳐준다.      Flatten() : 이미지 내 지역에 대한 고려 없이 1차원으로 펼처준다. 지역적인 정보가 날아갈 수 있다.      GlobalAveragePooling2D() : 지역을 고려하여 2D형태로 펼쳐준다. Flatten대신 많이 사용한다.        Output layer 설정하기 : 마지막 층은 학습하는 것이 아니라 각 클래스에 대한 확률값으로 값이 출력되어야 한다.          다중 분류시 softmax를 사용한다.        최적화함수, 손실함수 정의하기          모델의 정확도를 확인할 수 있도록 metrics를 설정한다.      최적화함수는 Adam, 손실함수는 categorical_crossentropy로 설정한다.      adam’s default Learning Rate =  0.001        모델 학습시키기          epochs : 한번의 epochs을 반복할 때마다 전체 데이터를 한번 학습하게 된다. 학습을 여러번 시킬수록 점수 향상에 도움이 되지만 과대적합 문제가 발생할 수 있다.      모델 summary : 모델 구조를 출력한다.from keras import * from keras.layers import * # 모델 선언 model = Sequential()# 이미지 학습하는 층을 쌓기model.add(Conv2D(16, (3,3), activation = 'relu', input_shape = (256,256,3)))model.add(MaxPool2D())model.add(Conv2D(32, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(64, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(128, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(256, (3,3), activation = 'relu'))model.add(MaxPool2D())model.add(Conv2D(512, (3,3), activation = 'relu'))model.add(MaxPool2D())# 차원 축소#model.add(Flatten())model.add(GlobalAveragePooling2D())# model.add(Dropout(0.5))# model.add(Dense(512, activation = 'relu'))# model.add(Dense(256, activation = 'relu'))model.add(Dense(2, activation = 'softmax'))# 최적화, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')model.fit(train_generator,         validation_data=valid_generator,        epochs= 10         )Epoch 1/10625/625 [==============================] - 93s 149ms/step - loss: 1.3097 - acc: 0.5896 - val_loss: 0.6291 - val_acc: 0.6382Epoch 2/10625/625 [==============================] - 93s 149ms/step - loss: 0.5652 - acc: 0.7053 - val_loss: 0.5327 - val_acc: 0.7402Epoch 3/10625/625 [==============================] - 95s 153ms/step - loss: 0.4760 - acc: 0.7775 - val_loss: 0.4140 - val_acc: 0.8166Epoch 4/10625/625 [==============================] - 93s 149ms/step - loss: 0.4070 - acc: 0.8147 - val_loss: 0.3493 - val_acc: 0.8484Epoch 5/10625/625 [==============================] - 93s 148ms/step - loss: 0.3471 - acc: 0.8486 - val_loss: 0.4356 - val_acc: 0.8134Epoch 6/10625/625 [==============================] - 93s 149ms/step - loss: 0.3118 - acc: 0.8680 - val_loss: 0.3170 - val_acc: 0.8708Epoch 7/10625/625 [==============================] - 93s 148ms/step - loss: 0.2792 - acc: 0.8821 - val_loss: 0.2858 - val_acc: 0.8822Epoch 8/10625/625 [==============================] - 94s 151ms/step - loss: 0.2515 - acc: 0.8939 - val_loss: 0.2577 - val_acc: 0.8920Epoch 9/10625/625 [==============================] - 93s 149ms/step - loss: 0.2362 - acc: 0.9018 - val_loss: 0.2665 - val_acc: 0.8920Epoch 10/10625/625 [==============================] - 94s 150ms/step - loss: 0.2168 - acc: 0.9097 - val_loss: 0.2565 - val_acc: 0.8932  모델 Summarymodel.summary()Model: "sequential_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_6 (Conv2D)            (None, 254, 254, 16)      448       _________________________________________________________________max_pooling2d_6 (MaxPooling2 (None, 127, 127, 16)      0         _________________________________________________________________conv2d_7 (Conv2D)            (None, 125, 125, 32)      4640      _________________________________________________________________max_pooling2d_7 (MaxPooling2 (None, 62, 62, 32)        0         _________________________________________________________________conv2d_8 (Conv2D)            (None, 60, 60, 64)        18496     _________________________________________________________________max_pooling2d_8 (MaxPooling2 (None, 30, 30, 64)        0         _________________________________________________________________conv2d_9 (Conv2D)            (None, 28, 28, 128)       73856     _________________________________________________________________max_pooling2d_9 (MaxPooling2 (None, 14, 14, 128)       0         _________________________________________________________________conv2d_10 (Conv2D)           (None, 12, 12, 256)       295168    _________________________________________________________________max_pooling2d_10 (MaxPooling (None, 6, 6, 256)         0         _________________________________________________________________conv2d_11 (Conv2D)           (None, 4, 4, 512)         1180160   _________________________________________________________________max_pooling2d_11 (MaxPooling (None, 2, 2, 512)         0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 1,573,794Trainable params: 1,573,794Non-trainable params: 0_________________________________________________________________전이학습(Transfer Learning)Imagenet 등의 빅데이터로 학습된 모델을 가져와 쓰기Pretrained Model : EfficientNet      작년에 나온 모델로, 이미지 분류 대회에서 압도적으로 쓰인다. B0 ~ B7까지 버전이 있고, B2 이상으로 갈수록 모델이 많이 무거워진다.    pretrained model 사용시 알아야할 3가지 필수 옵션          include_top : 마지막 출력층(top)을 포함할 것인지 여부 결정. False로 설정한 후 문제 상황에 맞게 분류기를 바꿀 수 있다.      weights : imagenet으로 설정 시, Imagenet을 통해 학습된 가중치를 사용하게 된다.                  학습된 가중치를 사용했을 때 장점                          대부분의 분류 문제에서 별다른 하이퍼파라미터 튜닝 없이 정확도가 8~90%가 나온다.              사진들끼리 공유하고 있는 특징(배경, 직선, 명암, 색체 등)이 있기 때문에 imagenet의 클래스에 없는 이미지에 대해서도 곧잘 분류한다.                                          pooling : avg 설정 시 분류층 직전에 Global average pooling을 적용한다.                  이미지 분류 문제에서는 대부분 classifier 전에 gap를 사용한다. 앞서 모델링파트에서 소개했듯이 지역적 정보를 가져오기 때문이다.          이미지 분류가 아닌 detection, segmentation, 음성 데이터 처리, 혹은 text mining 등의 경우 Flatten을 사용 시 모델의 성능이 잘 나오기도 한다. 딥러닝에서 정답은 없다. 많이 시도해봐야 한다.                      freeze          freeze시킴으로써 층을 선택적으로 학습을 시킬 수 있다.      데이터가 적으면 pretrained model의 feature extract 층은 얼리고 classifer 층만 학습시키기도 한다.      Callbacks 옵션여러가지 기법들로 모델 성능 높이기  EarlyStopping          조기 종료 옵션이다.                  patience : default로 val_loss를 monitor한다. 하이퍼파라미터로 설정한 양의 정수만큼 val_loss가 떨어지는 것을 참고 지켜보게 된다.                          만약 val_loss가 0.4 -&gt; 0.45 -&gt; 0.42로 높아졌을 때, 두 경우 모두에서 patience가 누적된다. 두 값 모두 0.4보다 높기 때문이다. 즉, 최적의 순간보다 val_loss가 n번 연속 좋아지지 않으면 멈춘다.                                            ModelCheckpoint          모델 최적의 가중치를 저장하는 옵션이다.      EarlyStopping을 사용했을 경우, 10번째가 최적일 시 13번째까지 학습하고 마지막 13번째의 가중치가 남아있게 된다. 체크포인트를 통해 10번째의 가중치를 가져올 수 있게 된다.      model.load_weights() 옵션으로 저장된 모델을 불러오는 것을 잊지 말자.        ReduceLROnPlateau          Learning rate가 컸을 때, 최적점 주변을 멤돌거나 지나쳐서 성능이 오히려 안좋아질 수 있다.      Learning rate(학습 보폭)를 조정하여 최적점에 도달하는데 도움을 줄 수 있다.      학습이 멈추기 전 작동해야하므로 항상 EarlyStopping의 patience 값 보단 작게 해야 한다.      factor 하이퍼파라미터의 기본값 : 0.1      min_lr 하이퍼파라미터의 기본값 : 0      from tensorflow.keras import * from tensorflow.keras.layers import * from tensorflow.keras.applications.efficientnet import EfficientNetB1 from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau# transfer learningmodel = Sequential() # 항상 모델 선언 다시 하고 학습할 것model.add(EfficientNetB1(include_top=False, weights = 'imagenet', pooling = 'avg')) # classifiermodel.add(Dense(2, activation='softmax')) # 최적화 함수, 손실 함수 정의하기model.compile(metrics = ['acc'], optimizer= 'adam', loss='categorical_crossentropy')# callbacks 설정하기es = EarlyStopping(patience=3,                     verbose=True)mc = ModelCheckpoint("best.h5", #h5:가중치 저장 확장자                    save_best_only = True,                     verbose=True)rlp = ReduceLROnPlateau(patience = 2,                       verbose=True)# 모델 학습하기model.fit(train_generator,         validation_data  = valid_generator,         epochs = 100,         callbacks = [es, mc, rlp],) model.load_weights('best.h5')Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h527025408/27018416 [==============================] - 0s 0us/stepEpoch 1/100625/625 [==============================] - ETA: 0s - loss: 0.0914 - acc: 0.9660Epoch 00001: val_loss improved from inf to 0.06357, saving model to best.h5625/625 [==============================] - 276s 442ms/step - loss: 0.0914 - acc: 0.9660 - val_loss: 0.0636 - val_acc: 0.9768Epoch 2/100625/625 [==============================] - ETA: 0s - loss: 0.0578 - acc: 0.9794Epoch 00002: val_loss did not improve from 0.06357625/625 [==============================] - 272s 435ms/step - loss: 0.0578 - acc: 0.9794 - val_loss: 0.1416 - val_acc: 0.9416Epoch 3/100625/625 [==============================] - ETA: 0s - loss: 0.0433 - acc: 0.9841Epoch 00003: val_loss did not improve from 0.06357Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.625/625 [==============================] - 273s 437ms/step - loss: 0.0433 - acc: 0.9841 - val_loss: 0.0679 - val_acc: 0.9750Epoch 4/100598/625 [===========================&gt;..] - ETA: 10s - loss: 0.0200 - acc: 0.9933제출하기  Test셋 예측 후 제출하기          Train셋과 마찬가지로 test_generator 만들어준다.      Test셋에는 레이블이 없으므로, y_col = None, class_mode = None 옵션 넣어줘야 한다.      학습 시에는 데이터가 섞여도 되지만, predict할 때는 index가 섞일 수 있으니 shuffle = False 옵션을 넣어준다.      test = pd.DataFrame({"path" : glob.glob("test/*")})test_generator = idg_test.flow_from_dataframe(test,                                               x_col = 'path',                                               y_col = None,                                              class_mode=None,                                              shuffle = False,                                              target_size = (256,256))result = model.predict(test_generator, verbose = 1)result   verbose = 1 : 모델 예측 진행 상황을 볼 수 있다.  result의 값을 출력해보면 클래스가 2개이므로 확률값 2개가 나온다.train_generator.class_indices   train_generator.class_indices 로 각 클래스의 변환값을 알 수 있다.  보통 0(False), 1(True)값이므로 이진 분류 대회에서는 1의 확률값을 제출한다.sub = pd.read_csv("/kaggle/input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv")sub['label'] = result[:,1] sub['id'] = test['path']sub['id'] = sub['id'].apply(lambda x : x.split("/")[1].split(".")[0])sub  lable이 id와 매핑되도록 바꿔줘야한다.현재 모델은 test['path']의 순서대로 예측값을 계산했는데, sub['id']는 index가 0부터이다.sub.to_csv("sub.csv",index=False) ]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Dacon Computer Vision(Classification)]]></title>
      <url>/classification/2020/12/23/ComputerVision/</url>
      <content type="text"><![CDATA[대회 소개딥러닝 모델을 이용하여 글자 속 숨겨진 숫자를 정확하게 분류해내는 대회대회 주소 : https://www.dacon.io/competitions/official/235626/overview/import tensorflow as tfimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalizationfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateaufrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import train_test_split데이터 불러오기train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)train.head()                  digit      letter      0      1      2      3      4      5      6      7      8      9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      ...      744      745      746      747      748      749      750      751      752      753      754      755      756      757      758      759      760      761      762      763      764      765      766      767      768      769      770      771      772      773      774      775      776      777      778      779      780      781      782      783              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1      5      L      1      1      1      4      3      0      0      4      4      3      0      4      3      3      3      4      4      0      0      1      1      3      4      0      4      2      0      4      0      1      3      1      0      4      1      1      3      1      ...      4      3      4      1      3      0      0      1      3      3      3      0      3      2      2      1      0      1      0      0      3      0      0      4      2      0      3      4      1      1      2      1      0      1      2      4      4      4      3      4              2      0      B      0      4      0      0      4      1      1      1      4      2      0      3      4      0      0      2      3      4      0      3      4      3      0      2      2      1      4      2      3      3      4      1      2      4      2      0      3      2      ...      4      2      3      0      0      0      0      4      3      2      2      4      2      1      1      1      3      3      1      2      4      4      4      2      2      4      4      0      4      2      0      3      0      1      4      1      4      2      1      2              3      4      L      1      1      2      2      1      1      1      0      2      1      3      2      2      2      4      1      1      4      1      0      1      3      4      2      2      2      4      1      1      2      0      3      0      2      3      4      0      1      ...      3      0      4      0      3      0      2      0      1      4      2      3      4      4      4      0      2      0      4      4      1      3      0      3      2      0      2      3      0      2      3      3      3      0      2      0      3      0      2      2              4      9      D      1      2      0      2      0      4      0      3      4      3      1      0      3      2      2      0      3      4      1      0      4      1      2      2      3      2      2      0      2      0      3      0      3      2      4      0      0      4      ...      0      3      0      1      4      1      3      1      2      1      1      1      2      2      2      4      3      4      3      0      4      1      2      4      1      4      0      1      0      4      3      3      2      0      1      4      0      0      1      1              5      6      A      3      0      2      4      0      3      0      4      2      4      2      1      4      1      1      4      4      0      2      3      4      4      3      3      3      3      4      1      0      3      0      3      0      0      0      1      1      2      ...      2      1      3      2      1      4      2      3      2      2      1      0      4      2      2      1      2      1      0      3      2      2      2      2      1      4      2      1      2      1      4      4      3      2      1      3      4      3      1      2      5 rows × 786 columnstest.head()                  letter      0      1      2      3      4      5      6      7      8      9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      ...      744      745      746      747      748      749      750      751      752      753      754      755      756      757      758      759      760      761      762      763      764      765      766      767      768      769      770      771      772      773      774      775      776      777      778      779      780      781      782      783              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2049      L      0      4      0      2      4      2      3      1      0      0      1      0      1      3      4      4      0      0      2      4      4      1      3      3      2      2      4      1      0      1      2      2      1      2      2      1      4      0      4      ...      1      3      1      1      3      3      4      1      3      1      2      4      1      2      0      3      1      2      4      0      2      1      2      4      1      1      3      2      1      0      2      0      4      2      2      4      3      4      1      4              2050      C      4      1      4      0      1      1      0      2      2      1      0      3      0      1      1      4      1      2      0      2      2      0      4      3      4      0      2      4      4      2      1      2      4      0      4      2      0      2      3      ...      3      4      2      6      2      2      0      1      2      4      1      1      3      3      2      3      4      2      2      4      3      1      3      3      3      1      3      4      4      2      0      3      2      4      2      4      2      2      1      2              2051      S      0      4      0      1      3      2      3      0      2      1      2      0      1      0      3      0      1      4      3      0      0      3      0      4      1      0      3      2      0      4      1      2      0      0      1      3      0      2      1      ...      0      4      4      3      4      1      4      2      3      4      1      2      0      2      2      3      3      1      1      4      1      2      4      0      0      0      0      2      3      2      1      3      2      0      3      2      3      0      1      4              2052      K      2      1      3      3      3      4      3      0      0      2      3      2      3      4      4      4      0      1      4      2      2      0      1      4      3      1      3      0      2      3      2      4      3      1      1      4      0      0      3      ...      0      4      1      1      2      3      2      3      3      0      0      1      3      3      0      2      0      0      2      3      2      2      3      1      1      2      4      0      1      2      3      0      3      2      4      1      0      4      4      4              2053      W      1      0      1      1      2      2      1      4      1      1      4      3      4      1      2      1      4      3      3      4      0      4      4      2      0      0      0      0      3      4      0      1      4      2      2      2      1      4      4      ...      4      1      3      2      1      2      1      4      4      1      2      3      2      4      2      1      4      3      4      3      0      1      0      1      1      2      1      1      0      2      4      3      1      4      0      2      1      2      3      4      5 rows × 785 columns  digit을 맞춰야 하는 대회입니다.데이터 살피기train.shape, test.shape, sub.shape ((2048, 786), (20480, 785), (20480, 1))  데이터 shape 확인하기print('digit :', train['digit'].unique(),'\n','num_digit :',train['digit'].nunique()) print('\n','letter : ', train['letter'].unique(),'\n','num_letter :', train['letter'].nunique()) digit : [5 0 4 9 6 8 1 3 2 7]  num_digit : 10 letter :  ['L' 'B' 'D' 'A' 'C' 'Q' 'M' 'F' 'J' 'H' 'N' 'X' 'I' 'R' 'V' 'Y' 'T' 'S' 'U' 'P' 'K' 'O' 'Z' 'G' 'E' 'W']  num_letter : 26  0~9 숫자 10개  알파벳 26개train.dtypes digit      int64letter    object0          int641          int642          int64           ...  779        int64780        int64781        int64782        int64783        int64Length: 786, dtype: object  숫자 맞춰야 하기 때문에 digit을 category형으로 바꿔야 합니다.train['digit'].value_counts() 2    2335    2256    2124    2073    2051    2029    1977    1940    1918    182Name: digit, dtype: int64  목적변수의 분포를 파악합니다.  클래스간 어느정도 균형을 이루고 있음을 알수 있습니다.데이터 시각화fig , axes = plt.subplots(2,5)fig.set_size_inches(20,8)for index in range(0,10):    img = np.array(train.iloc[index*3, 2:]).reshape(28, 28).astype(np.float)    axes[index//5 , index%5].imshow(img)    axes[index//5 , index%5].set_title('Letter : {}\nDigit : {}'.format(train['letter'][index*3+1],train['digit'][index*3+1]))    axes[index//5,  index%5].axis('off')  글자 속에 숫자가 숨겨져 있습니다.이미지 자세히 살펴보기train_img = train.iloc[:,2:].values.reshape(-1, 28, 28, 1).astype(np.float) # 이미지 데이터만 추출plt.subplot(1,2,2)data = np.where(train_img&gt;=140, train_img, 0)plt.imshow(data[3].reshape(28,28),cmap = 'gray')plt.title('Letter : {} , Digit : {}'.format(train['letter'][4], train['digit'][4]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)  D 속에 숨어있는 숫자 9  문자에 겹치는 숫자 부분만 표현된듯 합니다.이미지에 Conv2D 적용하기원본 이미지plt.imshow(np.array(train_img[3].reshape(28,28)))plt.title('letter: {} | digit: {}'.format( train['letter'][4],train['digit'][4]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)Conv2D 적용 후conv2d = Conv2D(64, (3,3), input_shape = (28,28,1))conv2d_activation  = Conv2D(64,(3,3), activation = 'relu', input_shape = (28,28,1))fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d(train_img)[3,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')activation 적용 후fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d_activation(train_img)[3,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')MaxPooling2D 적용 후fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(MaxPooling2D(2, 2)(conv2d(train_img))[3, :, :, i], cmap='gray')    axes[i//8, i%8].axis('off')  Letter 부분이 지워지지 않고 그대로 학습됐습니다.  Letter 부분 픽셀을 제거하고 학습하는 실험을 해봤습니다.Letter와 숫자가 겹치는 Pixel 값만 나타내보기fig, axes = plt.subplots(2, 10)fig.set_size_inches(20, 5)print('Letter : {}\nDigit : {}'.format(train['letter'][2], train['digit'][2]))for i in range(20):  data = np.where(train_img&gt;=i*5 +100, train_img, 0)  axes[i//10, i%10].imshow(data[1].reshape(28,28))  axes[i//10, i%10].set_title('pixel size : {}'.format(i*5 +100),fontsize = 12)  axes[i//10, i%10].axis('off')Letter : BDigit : 0  Pixel값 약 140정도  많은 정보가 손실돼서 학습시 도움이 되지 않았습니다.fig, axes = plt.subplots(2, 2)axes[0,0].imshow(np.array(train_img[3].reshape(28,28))) # 일정 픽셀 이상의 값만 추출해서 모델 적용axes[1,0].imshow(data[3].reshape(28,28))axes[0,0].set_title(train['letter'][4])axes[1,0].set_title(train['digit'][4])axes[0,0].axis('off')axes[1,0].axis('off')axes[0,1].imshow(np.array(train_img[4].reshape(28,28)))axes[1,1].imshow(data[4].reshape(28,28))axes[0,1].set_title(train['letter'][5])axes[1,1].set_title(train['digit'][5])axes[0,1].axis('off')axes[1,1].axis('off')(-0.5, 27.5, 27.5, -0.5)Pixel값이 140 이상인 경우 Conv2D 적용plt.imshow(np.array(data[4].reshape(28,28)))plt.title('letter: {} | digit: {}'.format( train['letter'][5],train['digit'][5]))plt.axis('off')(-0.5, 27.5, 27.5, -0.5)fig, axes = plt.subplots(8, 8)fig.set_size_inches(16, 16)for i in range(64):    axes[i//8, i%8].imshow(conv2d(data)[4,:,:,i], cmap='gray')    axes[i//8, i%8].axis('off')Data Augmentation  train셋이 2048개로 class 개수에 비해 적습니다.  데이터 증강이 필요할 것 같습니다.train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)X = train.drop(['digit','letter'], axis = 1) # imagey = train['digit'] # Label# 정규화X = X / 255.0X = X.values.reshape(-1,28,28,1)y = pd.get_dummies(y.values)train_img = train.iloc[:,2:].values.reshape(-1, 28, 28, 1).astype(np.float) # 이미지 데이터만 추출data = np.where(train_img&gt;=140, train_img, 0)ImageDataGenerator로 Data Augmentation 후 모델 학습(모델 직접 구축)def cnn_model():  # 파일 읽기  train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.  test = pd.read_csv('test.csv', index_col = 0)  sub = pd.read_csv('submission.csv',index_col = 0)  X = train.drop(['digit','letter'], axis = 1) # image  y = train['digit'] # Label  # 정규화  X = X / 255.0  X = (X.values).reshape(-1,28,28,1)  y = pd.get_dummies(y.values)  # train, valid 나누기  X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size = 0.1, random_state=2)  # 모델 생성  model = Sequential([                        Conv2D(512, (3,3), activation = 'relu', input_shape = (28,28,1)),                         #MaxPooling2D(),                        Conv2D(256, (3,3), activation = 'relu'),                         #MaxPooling2D(),                                              Conv2D(128, (3,3), activation = 'relu'),                           Conv2D(128, (3,3), activation = 'relu'),                         MaxPooling2D(),                                                Conv2D(64, (3,3), activation = 'relu'),                         Conv2D(64, (3,3), activation = 'relu'),                         Flatten(),                        Dropout(0.5),                        Dense(512, activation='relu'),                        Dense(10, activation = 'softmax')                      ])  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acc')  # Model Checkpoint  check_path = 'check.ckpt'  check_point = ModelCheckpoint(      filepath = check_path,      monitor = 'val_acc',      verbose = 1,      save_best_only = True,      save_weights_only = True)    # Data Augmentation  datagen = ImageDataGenerator(        featurewise_center=False,  # set input mean to 0 over the dataset        samplewise_center=False,  # set each sample mean to 0        featurewise_std_normalization=False,  # divide inputs by std of the dataset        samplewise_std_normalization=False,  # divide each input by its std        zca_whitening=False,  # apply ZCA whitening        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)        zoom_range = 0.1, # Randomly zoom image         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)        horizontal_flip=False,  # randomly flip images        vertical_flip=False)  # randomly flip images  datagen.fit(X_train)  batch_size = 16  history = model.fit_generator(datagen.flow(X_train,Y_train.values,                                              batch_size=16),                                              epochs = 100,                                              validation_data = (X_val,Y_val.values),                                              verbose = 1,                                               steps_per_epoch=X_train.shape[0] // batch_size  + 1,                                              validation_steps = X_val.shape[0] // batch_size +1,                                              callbacks = [check_point]                                              )                      model.load_weights(check_path)    return modelif __name__ == '__main__':    model = cnn_model()    #model.save("mymodel.h5")114/116 [============================&gt;.] - ETA: 0s - loss: 0.2794 - acc: 0.9045Epoch 00095: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2776 - acc: 0.9056 - val_loss: 0.3073 - val_acc: 0.8976Epoch 96/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2397 - acc: 0.9181Epoch 00096: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2440 - acc: 0.9164 - val_loss: 0.3861 - val_acc: 0.8683Epoch 97/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2676 - acc: 0.9042Epoch 00097: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2677 - acc: 0.9040 - val_loss: 0.2738 - val_acc: 0.8976Epoch 98/100114/116 [============================&gt;.] - ETA: 0s - loss: 0.2470 - acc: 0.9122Epoch 00098: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2465 - acc: 0.9121 - val_loss: 0.3565 - val_acc: 0.8780Epoch 99/100116/116 [==============================] - ETA: 0s - loss: 0.2602 - acc: 0.9126Epoch 00099: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2602 - acc: 0.9126 - val_loss: 0.4582 - val_acc: 0.8634Epoch 100/100113/116 [============================&gt;.] - ETA: 0s - loss: 0.2358 - acc: 0.9187Epoch 00100: val_acc did not improve from 0.90244116/116 [==============================] - 2s 15ms/step - loss: 0.2408 - acc: 0.9164 - val_loss: 0.5688 - val_acc: 0.8341최종 제출 코드 Train, Valid dataset 만들기# 데이터 불러오기train = pd.read_csv('train.csv', index_col = 0) # id column은 index column.test = pd.read_csv('test.csv', index_col = 0)sub = pd.read_csv('submission.csv',index_col = 0)# 기본 전처리x = np.array(train.iloc[:,2:]).reshape(-1,28,28,1).astype(np.float)  # reshape(batch_size, width, height, channel)y = pd.get_dummies(train['digit']).values  # target variable One Hot Encoding# train, valid 셋 나누기x_train, x_valid , y_train, y_valid = train_test_split(x, y, test_size = 0.1, random_state=1,                                                       stratify=y)x_train = x_train / 255.0x_valid = x_valid / 255.0 모델 구축 및 학습# model model = Sequential([                      Conv2D(256, (3,3), activation = 'relu', input_shape = (28,28,1)),                       MaxPooling2D(),                      Conv2D(256, (3,3), activation = 'relu'),                       #MaxPooling2D(),                                          Conv2D(128, (3,3), activation = 'relu'),                        Conv2D(128, (3,3), activation = 'relu'),                       #MaxPooling2D(),                      Flatten(),                      Dropout(0.5),                      Dense(512, activation='relu'),                      Dense(10, activation = 'softmax')                    ])# model compilemodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'acc')# model checkpointcheckpath = 'ck.ckpt'checkpoint = ModelCheckpoint(filepath = checkpath,                             monitor = 'val_loss',                             verbose =1,                             save_best_only = True,                             save_weights_only = True)# model fithistory = model.fit(x_train, y_train,                    validation_data = (x_valid, y_valid),                    batch_size = 32,                    epochs = 40,                    callbacks = [checkpoint])# model load weightsmodel.load_weights(checkpath)학습 결과 시각화Loss / Val_loss 시각화# val_loss vs loss 시각화plt.figure(figsize=(12, 9))plt.plot(np.arange(1, 41), history.history['loss'])plt.plot(np.arange(1, 41), history.history['val_loss'])plt.title('Loss / Val Loss', fontsize=20)plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend(['loss', 'val_loss'], fontsize=15)plt.show()Accuracy / Val_Accuracy 시각화plt.figure(figsize=(12, 9))plt.plot(np.arange(1, 41), history.history['acc'])plt.plot(np.arange(1, 41), history.history['val_acc'])plt.title('acc / val acc', fontsize=20)plt.xlabel('Epochs')plt.ylabel('acc')plt.legend(['acc', 'val_acc'], fontsize=15)plt.show()Test Dataset 전처리x_test = test.drop(['letter'], axis=1).valuesx_test = x_test.reshape(-1, 28, 28, 1)x_test = x_test/255Test Dataset 예측 및 제출sub['digit'] = np.argmax(model.predict(x_test), axis=1)sub.to_csv('submission.csv')sub.head()                  digit              id                        2049      6              2050      8              2051      2              2052      0              2053      3      예측 결과 시각화fig , axes = plt.subplots(2,5)fig.set_size_inches(20,8)for index in range(0,10):    img = np.array(x_test[index*3]).reshape(28, 28).astype(np.float)    axes[index//5 , index%5].imshow(img)    axes[index//5 , index%5].set_title('Letter : {}\nDigit : {}'.format(test['letter'].values[index*3],sub['digit'].values[index*3]))    axes[index//5,  index%5].axis('off')  Letter는 주어진 값이고, 모델은 Digit을 예측합니다.]]></content>
      <categories>
        
          <category> Classification </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Classification </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
